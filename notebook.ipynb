{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecb7f18",
   "metadata": {},
   "source": [
    "Recommended Top 10 Open Source Art Museums\n",
    "\n",
    "# Art Museum Country Data License Data Access Recommended Use\n",
    "\n",
    "1ï¸âƒ£ The Metropolitan Museum of Art (The Met) ğŸ‡ºğŸ‡¸ USA CC0 API\n",
    "\n",
    "/ CSV Largest, free, high-quality images\n",
    "\n",
    "2ï¸âƒ£ Rijksmuseum ğŸ‡³ğŸ‡± Netherlands CC0 API\n",
    "\n",
    "Excellent portraits with light and shadow and distinctive clothing features\n",
    "\n",
    "3ï¸âƒ£ Tate Britain ğŸ‡¬ğŸ‡§ UK CC-BY-NC GitHub CSV\n",
    "\n",
    "Pre-Raphaelite, Rossetti (key focus)\n",
    "\n",
    "4ï¸âƒ£ National Gallery (London) ğŸ‡¬ğŸ‡§ UK CC BY API\n",
    "Core of European classical portraiture\n",
    "\n",
    "5ï¸âƒ£ Art Institute of Chicago (AIC) ğŸ‡ºğŸ‡¸ USA CC0 API\n",
    "\n",
    "High-quality portraits, American Impressionism\n",
    "\n",
    "6ï¸âƒ£ Cleveland Museum of Art (CMA) ğŸ‡ºğŸ‡¸ USA CC0 API\n",
    "Baroque to Modern\n",
    "7ï¸âƒ£ National Gallery of Art (Washington, NGA) ğŸ‡ºğŸ‡¸ USA CC0 API\n",
    "Complete 18thâ€“19th century portrait data\n",
    "8ï¸âƒ£ Victoria and Albert Museum (V&A) ğŸ‡¬ğŸ‡§ UK Open API\n",
    "Extensive images of fashion and body posture\n",
    "9ï¸âƒ£ Museu Nacional dâ€™Art de Catalunya (MNAC) ğŸ‡ªğŸ‡¸ Spain CC BY API\n",
    "Gothic/Baroque figure data\n",
    "0ï¸âƒ£ National Gallery of Australia (NGA-AUS) ğŸ‡¦ğŸ‡º Australia CC BY API / CSV\n",
    "Expanding non-Eurocentric perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f9dc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# main.py\n",
    "# Embodied Aesthetic Reconstruction â€” Streamlit App (Resonance-first UI)\n",
    "# --------------------------------------------------------------------\n",
    "# - CLIP + Pose + Color + Significance fusion\n",
    "# - Human-centered UI: semantic \"resonance\" labels instead of raw numeric scores\n",
    "# - Sidebar toggle \"Debug mode\" to show raw similarity when needed\n",
    "# - Significance score (0â€“100) + badges + \"Learn more\"\n",
    "# - Extra metadata fields: price_estimate_usd, significance_text, interpretive_note_cn\n",
    "# - iCloud placeholder avoidance, robust pathing, cached embeddings\n",
    "# - YOLOv8n-pose via HuggingFace (optional), proper device mapping for MPS/CUDA/CPU\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "from __future__ import annotations\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "import streamlit as st\n",
    "\n",
    "# -------------------- Torch / device --------------------\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_OK = True\n",
    "except Exception:\n",
    "    TORCH_OK = False\n",
    "\n",
    "def get_device() -> str:\n",
    "    \"\"\"Return 'mps' on Apple Silicon, 'cuda' on Nvidia, else 'cpu'.\"\"\"\n",
    "    if not TORCH_OK:\n",
    "        return \"cpu\"\n",
    "    try:\n",
    "        if torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"cpu\"\n",
    "\n",
    "DEVICE = get_device()\n",
    "if TORCH_OK:\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"medium\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def yolo_device() -> str:\n",
    "    \"\"\"Map torch device to Ultralytics' expected device string.\"\"\"\n",
    "    if DEVICE == \"mps\":\n",
    "        return \"mps\"\n",
    "    if DEVICE == \"cuda\":\n",
    "        return \"0\"\n",
    "    return \"cpu\"\n",
    "\n",
    "# -------------------- Paths --------------------\n",
    "APP_DIR = Path(__file__).parent.resolve()\n",
    "DATA_DIR = APP_DIR / \"data\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "INTERIM_DIR = DATA_DIR / \"interim\"\n",
    "CACHE_DIR = APP_DIR / \".clip_cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METADATA_CSV = DATA_DIR / \"portrait_works.csv\"\n",
    "\n",
    "# -------------------- UI config --------------------\n",
    "st.set_page_config(\n",
    "    page_title=\"Embodied Aesthetic Reconstruction\",\n",
    "    page_icon=\"ğŸ–¼ï¸\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "# -------------------- Utils --------------------\n",
    "def is_icloud_placeholder(p: Path) -> bool:\n",
    "    return p.suffix == \".icloud\" or p.name.endswith(\".icloud\")\n",
    "\n",
    "def load_image_safe(path: Path) -> Optional[Image.Image]:\n",
    "    if not path.exists() or is_icloud_placeholder(path):\n",
    "        return None\n",
    "    try:\n",
    "        with Image.open(path) as im:\n",
    "            return im.convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def pil_from_bytes(b: bytes) -> Image.Image:\n",
    "    return Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "\n",
    "def center_crop_long_edge(im: Image.Image, size: int = 512) -> Image.Image:\n",
    "    im = ImageOps.exif_transpose(im)\n",
    "    w, h = im.size\n",
    "    s = min(w, h)\n",
    "    left = (w - s) // 2\n",
    "    top = (h - s) // 2\n",
    "    im = im.crop((left, top, left + s, top + s))\n",
    "    return im.resize((size, size), Image.BICUBIC)\n",
    "\n",
    "# -------------------- Sidebar --------------------\n",
    "with st.sidebar:\n",
    "    st.header(\"âš™ï¸ Matching Weights\")\n",
    "    w_clip = st.slider(\"CLIP weight\", 0.0, 1.0, 1.00, 0.05)\n",
    "    w_pose = st.slider(\"Pose weight\", 0.0, 1.0, 0.30, 0.05)\n",
    "    w_color = st.slider(\"Color weight\", 0.0, 1.0, 0.20, 0.05)\n",
    "    w_sig  = st.slider(\"Significance weight\", 0.0, 1.0, 0.20, 0.05)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"ğŸ” Filters\")\n",
    "    require_public = st.checkbox(\"Require Public-Domain license\", value=False)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"ğŸ§ª Debug\")\n",
    "    show_debug = st.checkbox(\"Show raw similarity score\", value=False)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"ğŸ“Š Results\")\n",
    "    top_k = st.slider(\"Top-K artworks\", 1, 20, 6, 1)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"ğŸ’» Active device\")\n",
    "    st.success(f\"{DEVICE}\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"ğŸ§ Overlay\")\n",
    "    overlay_skeleton = st.checkbox(\"Draw skeleton on preview (if available)\", value=True)\n",
    "\n",
    "# -------------------- Metadata & dataset --------------------\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expected (optional) columns:\n",
    "    filename,title,artist,year,license,museum,accession,movement,\n",
    "    is_masterwork,citations,exhibitions,auction_price_usd,views_per_year,\n",
    "    price_estimate_usd,significance_text,interpretive_note_cn,notable_tags,source_links\n",
    "    \"\"\"\n",
    "    if METADATA_CSV.exists():\n",
    "        try:\n",
    "            return pd.read_csv(METADATA_CSV)\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not read metadata CSV ({METADATA_CSV.name}): {e}\")\n",
    "    # Fallback minimal schema\n",
    "    return pd.DataFrame({\n",
    "        \"filename\": [], \"title\": [], \"artist\": [], \"year\": [], \"license\": [],\n",
    "        \"museum\": [], \"accession\": [], \"movement\": [], \"is_masterwork\": [],\n",
    "        \"citations\": [], \"exhibitions\": [], \"auction_price_usd\": [], \"views_per_year\": [],\n",
    "        \"price_estimate_usd\": [], \"significance_text\": [], \"interpretive_note_cn\": [],\n",
    "        \"notable_tags\": [], \"source_links\": []\n",
    "    })\n",
    "\n",
    "META = load_metadata()\n",
    "\n",
    "def list_dataset_images() -> List[Path]:\n",
    "    if not IMAGES_DIR.exists():\n",
    "        return []\n",
    "    files: List[Path] = []\n",
    "    for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"):\n",
    "        files += list(IMAGES_DIR.glob(ext))\n",
    "    files = [p for p in files if not is_icloud_placeholder(p)]\n",
    "    return sorted(files)\n",
    "\n",
    "DATASET_FILES = list_dataset_images()\n",
    "\n",
    "# -------------------- Significance (0..100) --------------------\n",
    "TOP_MUSEUMS = [\"met\", \"national gallery\", \"tate\", \"louvre\", \"uffizi\", \"hermitage\", \"ng london\"]\n",
    "\n",
    "def _logn(x, d=1.0):\n",
    "    try:\n",
    "        return math.log10(max(float(x), 1.0)) / d\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def compute_significance_row(row) -> float:\n",
    "    \"\"\"Heuristic significance score (0..100).\"\"\"\n",
    "    w = dict(master=0.35, museum=0.20, cites=0.15, exhib=0.10, auction=0.10, views=0.10)\n",
    "    score = 0.0\n",
    "    if str(row.get(\"is_masterwork\", \"0\")).lower() in [\"1\", \"true\", \"yes\", \"y\"]:\n",
    "        score += w[\"master\"]\n",
    "    museum = str(row.get(\"museum\", \"\")).lower()\n",
    "    if museum:\n",
    "        score += w[\"museum\"] * (1.0 if any(m in museum for m in TOP_MUSEUMS) else 0.5)\n",
    "    score += w[\"cites\"]   * _logn(row.get(\"citations\", 0),        d=3.0)\n",
    "    score += w[\"exhib\"]   * _logn(row.get(\"exhibitions\", 0),      d=2.0)\n",
    "    score += w[\"auction\"] * _logn(row.get(\"auction_price_usd\",0), d=8.0)\n",
    "    score += w[\"views\"]   * _logn(row.get(\"views_per_year\", 0),   d=6.0)\n",
    "    return float(np.clip(score, 0.0, 1.0) * 100.0)\n",
    "\n",
    "def significance_badges(row) -> List[str]:\n",
    "    badges = []\n",
    "    if str(row.get(\"is_masterwork\",\"0\")).lower() in [\"1\",\"true\",\"yes\",\"y\"]:\n",
    "        badges.append(\"Masterwork\")\n",
    "    if str(row.get(\"museum\",\"\")).strip():\n",
    "        badges.append(\"Permanent Collection\")\n",
    "    try:\n",
    "        if int(row.get(\"citations\", 0)) >= 50:\n",
    "            badges.append(\"Canon\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return badges[:3]\n",
    "\n",
    "def filename_key(path_or_name: str) -> str:\n",
    "    return Path(path_or_name).name\n",
    "\n",
    "META_BY_NAME = {}\n",
    "if not META.empty and \"filename\" in META.columns:\n",
    "    for _, r in META.iterrows():\n",
    "        META_BY_NAME[filename_key(str(r.get(\"filename\",\"\")))] = r\n",
    "\n",
    "# -------------------- OpenCLIP --------------------\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_openclip():\n",
    "    try:\n",
    "        import open_clip\n",
    "    except Exception as e:\n",
    "        st.error(f\"open_clip_torch not installed: {e}\")\n",
    "        return None, None, None\n",
    "    try:\n",
    "        model_name, pretrained = \"ViT-B-32\", \"laion2b_s34b_b79k\"\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "            model_name, pretrained=pretrained, device=DEVICE\n",
    "        )\n",
    "        tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        model.eval()\n",
    "        return model, preprocess, tokenizer\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to load OpenCLIP: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "MODEL_CLIP, PRE_CLIP, TOKENIZER = load_openclip()\n",
    "\n",
    "def tensor_from_pil_clip(im: Image.Image):\n",
    "    if PRE_CLIP is None:\n",
    "        return None\n",
    "    t = PRE_CLIP(im).unsqueeze(0)\n",
    "    if TORCH_OK:\n",
    "        t = t.to(DEVICE)\n",
    "    return t\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def embed_image_clip(img_bytes: bytes) -> Optional[np.ndarray]:\n",
    "    if MODEL_CLIP is None:\n",
    "        return None\n",
    "    try:\n",
    "        im = pil_from_bytes(img_bytes)\n",
    "        im = center_crop_long_edge(im, size=224)\n",
    "        x = tensor_from_pil_clip(im)\n",
    "        with torch.no_grad():\n",
    "            feat = MODEL_CLIP.encode_image(x)\n",
    "            feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "        return feat.cpu().numpy().astype(\"float32\")[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------- YOLO weights via HuggingFace (optional) --------------------\n",
    "def get_yolo_pose_weights_path() -> Optional[str]:\n",
    "    try:\n",
    "        from huggingface_hub import hf_hub_download\n",
    "    except Exception:\n",
    "        return \"yolov8n-pose.pt\"\n",
    "    try:\n",
    "        local = hf_hub_download(\n",
    "            repo_id=\"ultralytics/yolov8n-pose\",\n",
    "            filename=\"yolov8n-pose.pt\",\n",
    "            local_dir=str(Path.home() / \".cache\" / \"hf\"),\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        return local\n",
    "    except Exception:\n",
    "        return \"yolov8n-pose.pt\"\n",
    "\n",
    "# -------------------- Feature extraction & cache --------------------\n",
    "EMB_PATH   = CACHE_DIR / \"embeddings.npy\"\n",
    "IDS_PATH   = CACHE_DIR / \"ids.json\"\n",
    "POSE_PATH  = CACHE_DIR / \"pose.npy\"\n",
    "COLOR_PATH = CACHE_DIR / \"color.npy\"\n",
    "SIG_PATH   = CACHE_DIR / \"significance.npy\"\n",
    "\n",
    "def _color_feature(im: Image.Image) -> np.ndarray:\n",
    "    im = im.resize((256, 256), Image.BILINEAR)\n",
    "    arr = np.array(im.convert(\"HSV\"))\n",
    "    h, s, v = arr[..., 0], arr[..., 1], arr[..., 2]\n",
    "    hist_h, _ = np.histogram(h, bins=32, range=(0, 255), density=True)\n",
    "    hist_s, _ = np.histogram(s, bins=16, range=(0, 255), density=True)\n",
    "    hist_v, _ = np.histogram(v, bins=8,  range=(0, 255), density=True)\n",
    "    feat = np.concatenate([hist_h, hist_s, hist_v]).astype(\"float32\")\n",
    "    return feat / (np.linalg.norm(feat) + 1e-8)\n",
    "\n",
    "def _pose_feature_from_kpts(kpts_xyv: np.ndarray) -> np.ndarray:\n",
    "    if kpts_xyv is None or len(kpts_xyv) == 0:\n",
    "        return np.zeros(34, dtype=\"float32\")\n",
    "    xy = kpts_xyv[:, :2]\n",
    "    min_xy = xy.min(0); max_xy = xy.max(0)\n",
    "    wh = np.maximum(max_xy - min_xy, 1e-6)\n",
    "    xy_n = (xy - min_xy) / wh\n",
    "    feat = xy_n.reshape(-1).astype(\"float32\")\n",
    "    if feat.shape[0] < 34:\n",
    "        feat = np.pad(feat, (0, 34 - feat.shape[0]))\n",
    "    else:\n",
    "        feat = feat[:34]\n",
    "    return feat / (np.linalg.norm(feat) + 1e-8)\n",
    "\n",
    "def _detect_pose_pil(im: Image.Image) -> Optional[np.ndarray]:\n",
    "    try:\n",
    "        from ultralytics import YOLO\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        weights = get_yolo_pose_weights_path()\n",
    "        model = YOLO(weights)\n",
    "        arr = np.array(im.convert(\"RGB\"))\n",
    "        res = model.predict(\n",
    "            source=arr, imgsz=512, conf=0.25, verbose=False, device=yolo_device()\n",
    "        )\n",
    "        if not res or len(res[0].keypoints) == 0:\n",
    "            return None\n",
    "        kpts = res[0].keypoints.xy.cpu().numpy()\n",
    "        if kpts.ndim == 4: kpts = kpts[0]\n",
    "        if kpts.ndim == 3: k = kpts[0]\n",
    "        else: return None\n",
    "        v = np.ones((k.shape[0], 1), dtype=\"float32\")\n",
    "        kxyv = np.concatenate([k.astype(\"float32\"), v], axis=1)\n",
    "        return _pose_feature_from_kpts(kxyv)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _dataset_pose_feature(path: Path) -> Optional[np.ndarray]:\n",
    "    im = load_image_safe(path)\n",
    "    if im is None:\n",
    "        return None\n",
    "    im_small = center_crop_long_edge(im, 512)\n",
    "    return _detect_pose_pil(im_small)\n",
    "\n",
    "@st.cache_data(show_spinner=True, persist=True)\n",
    "def build_dataset_embeddings(files: List[Path]):\n",
    "    \"\"\"Return (clip_embeds, ids, pose_feats, color_feats, signif_norm).\"\"\"\n",
    "    ids = [str(p.relative_to(APP_DIR)) for p in files]\n",
    "\n",
    "    # Use cache if aligned\n",
    "    if EMB_PATH.exists() and IDS_PATH.exists() and SIG_PATH.exists():\n",
    "        try:\n",
    "            cached_ids = json.loads(IDS_PATH.read_text())\n",
    "            if cached_ids == ids:\n",
    "                clip_arr  = np.load(EMB_PATH)\n",
    "                pose_arr  = np.load(POSE_PATH)  if POSE_PATH.exists()  else None\n",
    "                color_arr = np.load(COLOR_PATH) if COLOR_PATH.exists() else None\n",
    "                sig_arr   = np.load(SIG_PATH)\n",
    "                return clip_arr, ids, pose_arr, color_arr, sig_arr\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    clip_feats, pose_feats, color_feats, signif_vals = [], [], [], []\n",
    "\n",
    "    if MODEL_CLIP is None:\n",
    "        st.error(\"OpenCLIP not ready; cannot build embeddings.\")\n",
    "        return np.zeros((0, 512), dtype=\"float32\"), ids, None, None, np.zeros((0,), dtype=\"float32\")\n",
    "\n",
    "    progress = st.progress(0.0, text=\"Building dataset embeddings/featuresâ€¦\")\n",
    "    for i, p in enumerate(files):\n",
    "        progress.progress((i + 1) / max(1, len(files)))\n",
    "        im = load_image_safe(p)\n",
    "\n",
    "        # CLIP\n",
    "        if im is not None:\n",
    "            try:\n",
    "                im_clip = center_crop_long_edge(im, 224)\n",
    "                x = tensor_from_pil_clip(im_clip)\n",
    "                with torch.no_grad():\n",
    "                    z = MODEL_CLIP.encode_image(x)\n",
    "                    z = z / z.norm(dim=-1, keepdim=True)\n",
    "                clip_feats.append(z.cpu().numpy().astype(\"float32\")[0])\n",
    "            except Exception:\n",
    "                clip_feats.append(np.zeros(512, dtype=\"float32\"))\n",
    "        else:\n",
    "            clip_feats.append(np.zeros(512, dtype=\"float32\"))\n",
    "\n",
    "        # Color\n",
    "        if im is not None:\n",
    "            try:\n",
    "                color_feats.append(_color_feature(im))\n",
    "            except Exception:\n",
    "                color_feats.append(np.zeros(56, dtype=\"float32\"))\n",
    "        else:\n",
    "            color_feats.append(np.zeros(56, dtype=\"float32\"))\n",
    "\n",
    "        # Pose\n",
    "        pf = _dataset_pose_feature(p)\n",
    "        pose_feats.append(pf if pf is not None else np.zeros(34, dtype=\"float32\"))\n",
    "\n",
    "        # Significance prior from metadata (0..1)\n",
    "        fname = filename_key(p.name)\n",
    "        row = META_BY_NAME.get(fname)\n",
    "        sig = compute_significance_row(row) / 100.0 if row is not None else 0.0\n",
    "        signif_vals.append(float(sig))\n",
    "\n",
    "    clip_arr  = np.vstack(clip_feats).astype(\"float32\") if clip_feats else np.zeros((0, 512), dtype=\"float32\")\n",
    "    pose_arr  = np.vstack(pose_feats).astype(\"float32\") if pose_feats else None\n",
    "    color_arr = np.vstack(color_feats).astype(\"float32\") if color_feats else None\n",
    "    sig_arr   = np.array(signif_vals, dtype=\"float32\")\n",
    "\n",
    "    # Cache\n",
    "    try:\n",
    "        np.save(EMB_PATH, clip_arr)\n",
    "        np.save(POSE_PATH, pose_arr)\n",
    "        np.save(COLOR_PATH, color_arr)\n",
    "        np.save(SIG_PATH,  sig_arr)\n",
    "        IDS_PATH.write_text(json.dumps(ids))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return clip_arr, ids, pose_arr, color_arr, sig_arr\n",
    "\n",
    "# ---- Build dataset features ----\n",
    "if len(DATASET_FILES) == 0:\n",
    "    st.error(f\"âŒ No images found or folder is empty: {IMAGES_DIR}\\nMake sure iCloud files are downloaded locally (no .icloud).\")\n",
    "else:\n",
    "    st.success(f\"âœ… Dataset images: {len(DATASET_FILES)}\")\n",
    "\n",
    "CLIP_DS, IDS, POSE_DS, COLOR_DS, SIG_DS = build_dataset_embeddings(DATASET_FILES)  # SIG_DS in [0,1]\n",
    "\n",
    "# ---- Ready checks ----\n",
    "READY = True\n",
    "reasons = []\n",
    "if MODEL_CLIP is None:\n",
    "    READY = False; reasons.append(\"OpenCLIP not loaded.\")\n",
    "if len(DATASET_FILES) == 0:\n",
    "    READY = False; reasons.append(\"No images in data/images (or still .icloud).\")\n",
    "if CLIP_DS is None or getattr(CLIP_DS, \"shape\", (0,))[0] == 0:\n",
    "    READY = False; reasons.append(\"Dataset embeddings not built yet.\")\n",
    "if not READY:\n",
    "    st.warning(\"Matching is not ready: \" + \" \".join(reasons))\n",
    "else:\n",
    "    st.success(\"Matching is ready âœ”\")\n",
    "\n",
    "# -------------------- Similarity + Resonance --------------------\n",
    "def cos_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    if a is None or b is None:\n",
    "        return None\n",
    "    if a.ndim == 1:\n",
    "        a = a[None, :]\n",
    "    a = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-8)\n",
    "    b = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-8)\n",
    "    return (a @ b.T)\n",
    "\n",
    "def interpret_score(score: float) -> str:\n",
    "    \"\"\"Map numeric similarity to a poetic/semantic label.\"\"\"\n",
    "    if score > 0.80: return \"Strong resonance ğŸ’«\"\n",
    "    if score > 0.65: return \"Aesthetic kinship âœ¨\"\n",
    "    if score > 0.50: return \"Subtle correspondence ğŸŒ™\"\n",
    "    return \"Distant echo ğŸŒ«ï¸\"\n",
    "\n",
    "# -------------------- Pose overlay (preview only) --------------------\n",
    "def draw_skeleton_overlay(im: Image.Image) -> Tuple[Image.Image, Optional[np.ndarray]]:\n",
    "    try:\n",
    "        from ultralytics import YOLO\n",
    "        import cv2\n",
    "    except Exception:\n",
    "        return im, None\n",
    "\n",
    "    weights = get_yolo_pose_weights_path()\n",
    "    model = YOLO(weights)\n",
    "    arr = np.array(im.convert(\"RGB\"))\n",
    "    res = model.predict(source=arr, imgsz=512, conf=0.25, verbose=False, device=yolo_device())\n",
    "    if not res or len(res[0].keypoints) == 0:\n",
    "        return im, None\n",
    "\n",
    "    canvas = arr.copy()\n",
    "    pts = res[0].keypoints.xy.cpu().numpy()\n",
    "    if pts.ndim == 4: pts = pts[0]\n",
    "    if pts.ndim == 3: pts = pts[0]\n",
    "\n",
    "    PAIRS = [(5,7),(7,9),(6,8),(8,10),(11,13),(13,15),(12,14),(14,16),(5,6),(11,12),(5,11),(6,12)]\n",
    "    try:\n",
    "        import cv2\n",
    "        for (a,b) in PAIRS:\n",
    "            xa,ya = pts[a]; xb,yb = pts[b]\n",
    "            cv2.line(canvas,(int(xa),int(ya)),(int(xb),int(yb)),(0,255,0),3)\n",
    "        for (x,y) in pts:\n",
    "            cv2.circle(canvas,(int(x),int(y)),4,(0,0,255),-1)\n",
    "        im_draw = Image.fromarray(canvas)\n",
    "    except Exception:\n",
    "        im_draw = im\n",
    "\n",
    "    v = np.ones((pts.shape[0],1),dtype=\"float32\")\n",
    "    kxyv = np.concatenate([pts.astype(\"float32\"), v], axis=1)\n",
    "    pf = _pose_feature_from_kpts(kxyv)\n",
    "    return im_draw, pf\n",
    "\n",
    "# -------------------- UI â€” Main --------------------\n",
    "st.title(\"Embodied Aesthetic Reconstruction\")\n",
    "st.caption(\"Camera / Upload â†’ CLIP + Pose + Color + Significance â†’ Resonant artworks\")\n",
    "\n",
    "status_cols = st.columns(3)\n",
    "with status_cols[0]:\n",
    "    st.success(f\"Images dir: {IMAGES_DIR.relative_to(APP_DIR)}\" if IMAGES_DIR.exists() else f\"Missing: {IMAGES_DIR}\")\n",
    "with status_cols[1]:\n",
    "    st.info(f\"Dataset files: {len(DATASET_FILES)}\")\n",
    "with status_cols[2]:\n",
    "    st.info(f\"Models: OpenCLIP={'âœ”ï¸' if MODEL_CLIP else 'âœ–ï¸'} Â· Pose(ultralytics)={'âœ”ï¸'}\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "\n",
    "left, right = st.columns([1, 1])\n",
    "query_img: Optional[Image.Image] = None\n",
    "query_pose_feat: Optional[np.ndarray] = None\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"ğŸ“· Camera (auto center-crop)\")\n",
    "    cam = st.camera_input(\"Take a photo (allow permission first)\", key=\"camera\")\n",
    "    if cam is not None:\n",
    "        try:\n",
    "            img = pil_from_bytes(cam.getvalue())\n",
    "            img = center_crop_long_edge(img, 640)\n",
    "            out_path = INTERIM_DIR / \"locked_frame.jpg\"\n",
    "            img.save(out_path, quality=92)\n",
    "            st.success(f\"Saved: {out_path.as_posix()}\")\n",
    "            if overlay_skeleton:\n",
    "                img_draw, q_pose = draw_skeleton_overlay(img)\n",
    "                st.image(img_draw, caption=\"Camera preview\", use_container_width=True)\n",
    "                query_img = img_draw\n",
    "                query_pose_feat = q_pose\n",
    "            else:\n",
    "                st.image(img, caption=\"Camera preview\", use_container_width=True)\n",
    "                query_img = img\n",
    "        except Exception as e:\n",
    "            st.error(f\"Camera decode failed: {e}\")\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"ğŸ–¼ï¸ Or upload an image\")\n",
    "    up = st.file_uploader(\"JPG/PNG\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "    if up is not None:\n",
    "        try:\n",
    "            img = pil_from_bytes(up.getvalue())\n",
    "            img = center_crop_long_edge(img, 640)\n",
    "            if overlay_skeleton:\n",
    "                img_draw, q_pose = draw_skeleton_overlay(img)\n",
    "                st.image(img_draw, caption=\"Uploaded preview\", use_container_width=True)\n",
    "                query_img = img_draw\n",
    "                query_pose_feat = q_pose\n",
    "            else:\n",
    "                st.image(img, caption=\"Uploaded preview\", use_container_width=True)\n",
    "                query_img = img\n",
    "        except Exception as e:\n",
    "            st.error(f\"Upload decode failed: {e}\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "run = st.button(\"ğŸ” Run Matching\", disabled=not READY)\n",
    "\n",
    "# -------------------- Display helpers --------------------\n",
    "def display_results(idx_scores: List[Tuple[int, float]], k: int):\n",
    "    if len(idx_scores) == 0:\n",
    "        st.warning(\"No results to display.\")\n",
    "        return\n",
    "    cols = st.columns(min(3, k))\n",
    "    for i, (idx, sc) in enumerate(idx_scores[:k]):\n",
    "        fn_rel = IDS[idx]\n",
    "        p = (APP_DIR / fn_rel).resolve()\n",
    "        im = load_image_safe(p)\n",
    "\n",
    "        row = META_BY_NAME.get(filename_key(p.name))\n",
    "        if row is not None:\n",
    "            title  = str(row.get(\"title\", \"\")).strip()\n",
    "            artist = str(row.get(\"artist\", \"\")).strip()\n",
    "            year   = str(row.get(\"year\", \"\")).strip()\n",
    "            museum = str(row.get(\"museum\", \"\")).strip()\n",
    "            price  = str(row.get(\"price_estimate_usd\", \"\")).strip()\n",
    "            sigtxt = str(row.get(\"significance_text\", \"\")).strip()\n",
    "            sigcn  = str(row.get(\"interpretive_note_cn\", \"\")).strip()\n",
    "            links  = str(row.get(\"source_links\", \"\")).strip()\n",
    "            sig100 = compute_significance_row(row)\n",
    "            badges = significance_badges(row)\n",
    "        else:\n",
    "            title=artist=year=museum=price=sigtxt=sigcn=links=\"\"\n",
    "            sig100, badges = 0.0, []\n",
    "\n",
    "        with cols[i % len(cols)]:\n",
    "            if im is not None:\n",
    "                st.image(im, use_container_width=True)\n",
    "\n",
    "            # Resonance-first display\n",
    "            if show_debug:\n",
    "                st.markdown(f\"**Score:** {sc:.3f}\")\n",
    "            else:\n",
    "                st.markdown(f\"**{interpret_score(sc)}**\")\n",
    "\n",
    "            if title or artist or year:\n",
    "                st.caption(f\"{title} â€” {artist} ({year})\")\n",
    "            if museum:\n",
    "                st.caption(f\"ğŸ›ï¸ {museum}\")\n",
    "            if price:\n",
    "                st.caption(f\"ğŸ’° Estimated value: ${price}\")\n",
    "\n",
    "            st.markdown(f\"**Significance:** {sig100:.0f}/100\")\n",
    "            if badges:\n",
    "                st.caption(\" Â· \".join(badges))\n",
    "            if sigtxt:\n",
    "                st.markdown(f\"_{sigtxt}_\")\n",
    "            if sigcn:\n",
    "                st.markdown(f\"**Curatorial note (CN):** {sigcn}\")\n",
    "            if links:\n",
    "                first = links.split(\";\")[0].strip()\n",
    "                if first:\n",
    "                    st.write(f\"[Learn more]({first})\")\n",
    "\n",
    "# -------------------- Matching --------------------\n",
    "if run:\n",
    "    if query_img is None:\n",
    "        st.warning(\"Please take a photo or upload an image first.\")\n",
    "    else:\n",
    "        st.write(\"Computing embeddingsâ€¦\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        q_clip = None\n",
    "        if MODEL_CLIP is not None:\n",
    "            try:\n",
    "                buf = io.BytesIO()\n",
    "                query_img.save(buf, format=\"JPEG\", quality=92)\n",
    "                q_clip = embed_image_clip(buf.getvalue())\n",
    "            except Exception:\n",
    "                q_clip = None\n",
    "\n",
    "        try:\n",
    "            q_color = _color_feature(query_img)\n",
    "        except Exception:\n",
    "            q_color = None\n",
    "\n",
    "        if query_pose_feat is None and w_pose > 0:\n",
    "            query_pose_feat = _detect_pose_pil(center_crop_long_edge(query_img, 512))\n",
    "        w_pose_eff = w_pose if query_pose_feat is not None and POSE_DS is not None else 0.0\n",
    "        if w_pose > 0 and w_pose_eff == 0.0:\n",
    "            st.info(\"No pose feature detected or dataset pose not precomputed; treating Pose weight as 0.\")\n",
    "\n",
    "        valid_indices = np.arange(len(IDS))\n",
    "        if require_public and not META.empty and {\"filename\", \"license\"}.issubset(META.columns):\n",
    "            pd_mask = META[\"license\"].astype(str).str.contains(\"Public Domain\", case=False, na=False)\n",
    "            pool_names = set(META.loc[pd_mask, \"filename\"].astype(str).apply(lambda s: Path(s).name))\n",
    "            filt = [i for i, fn in enumerate(IDS) if Path(fn).name in pool_names]\n",
    "            if len(filt) > 0:\n",
    "                valid_indices = np.array(filt, dtype=int)\n",
    "            else:\n",
    "                st.warning(\"No entries meet Public-Domain filter; ignoring filter.\")\n",
    "\n",
    "        def slicer(arr):\n",
    "            if arr is None or arr.shape[0] != len(IDS):\n",
    "                return None\n",
    "            return arr[valid_indices]\n",
    "\n",
    "        CLIP_POOL  = slicer(CLIP_DS)\n",
    "        POSE_POOL  = slicer(POSE_DS)\n",
    "        COLOR_POOL = slicer(COLOR_DS)\n",
    "        SIG_POOL   = SIG_DS[valid_indices] if SIG_DS is not None and SIG_DS.shape[0] == len(IDS) else None\n",
    "\n",
    "        def cos_sim_local(q, bank):\n",
    "            if q is None or bank is None:\n",
    "                return None\n",
    "            a = q[None, :] if q.ndim == 1 else q\n",
    "            a = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-8)\n",
    "            b = bank / (np.linalg.norm(bank, axis=1, keepdims=True) + 1e-8)\n",
    "            return (a @ b.T)[0]\n",
    "\n",
    "        score = np.zeros(valid_indices.size, dtype=\"float32\")\n",
    "        s = cos_sim_local(q_clip, CLIP_POOL)\n",
    "        if s is not None: score += w_clip * s\n",
    "        s = cos_sim_local(query_pose_feat, POSE_POOL) if w_pose_eff > 0 else None\n",
    "        if s is not None: score += w_pose_eff * s\n",
    "        s = cos_sim_local(q_color, COLOR_POOL)\n",
    "        if s is not None: score += w_color * s\n",
    "        if SIG_POOL is not None and w_sig > 0.0:\n",
    "            score += w_sig * SIG_POOL\n",
    "\n",
    "        topk_local = np.argsort(-score)[:min(top_k, score.size)]\n",
    "        results = [(int(valid_indices[i]), float(score[i])) for i in topk_local]\n",
    "\n",
    "        dt = (time.time() - t0) * 1000\n",
    "        st.info(f\"Search time: {dt:.1f} ms\")\n",
    "\n",
    "        display_results(results, k=top_k)\n",
    "\n",
    "# -------------------- Tips --------------------\n",
    "with st.expander(\"Tips\"):\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "- If you see **iCloud .icloud** placeholders, open the folder in Finder and choose **â€œDownload Nowâ€**.\n",
    "- Adjust **CLIP / Pose / Color / Significance** weights to influence ranking.\n",
    "- **Debug mode** reveals raw similarity scores; keep it off for exhibitions to maintain a poetic tone.\n",
    "- The **Public-Domain** filter works only if `license` exists in your metadata CSV.\n",
    "- First run builds and caches dataset features in `.clip_cache/`.\n",
    "- Pose detection uses `ultralytics` (YOLOv8n-pose). Weights are pulled from Hugging Face when possible.\n",
    "- *Significance* is a heuristic combining museum presence, citations, and attention. It reflects historical canons yet encourages plural aesthetics.\n",
    "\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db849671",
   "metadata": {},
   "source": [
    "#frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad84f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_webrtc import (\n",
    "    WebRtcMode,\n",
    "    RTCConfiguration,\n",
    "    VideoProcessorBase,\n",
    "    webrtc_streamer,\n",
    ")\n",
    "\n",
    "# =================== è·¯å¾„ä¸å¸¸é‡ ===================\n",
    "\n",
    "FRONTEND_DIR = Path(__file__).resolve().parent\n",
    "ROOT_DIR = FRONTEND_DIR.parent\n",
    "\n",
    "# æœ¬åœ°æ•°æ®ï¼ˆç”¨äºæ ¹æ® filename æ‰¾ metaï¼‰\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"local\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "# å¯èƒ½å­˜åœ¨çš„ meta CSV\n",
    "META_CSV_CANDIDATES = [\n",
    "    DATA_DIR / \"portrait_works_enhanced_english.csv\",\n",
    "    DATA_DIR / \"portrait_works.csv\",\n",
    "]\n",
    "\n",
    "# åç«¯ APIï¼ˆéœ€è¦æ—¶ä½ å¯ä»¥æ”¹æˆ HuggingFace åœ°å€ï¼‰\n",
    "DEFAULT_API_URL = \"http://127.0.0.1:8000/match\"\n",
    "\n",
    "APP_TITLE = \"Embodied Aesthetic Reconstruction\"\n",
    "\n",
    "# YOLOv8-Pose æ¨¡å‹è·¯å¾„ï¼ˆæ”¾åœ¨ frontend ç›®å½•ä¸‹ï¼‰\n",
    "YOLO_MODEL_PATH = FRONTEND_DIR / \"yolov8n-pose.pt\"\n",
    "\n",
    "# WebRTC é…ç½®ï¼ˆSafari éœ€è¦ STUNï¼‰\n",
    "RTC_CONFIGURATION = RTCConfiguration(\n",
    "    {\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]}\n",
    ")\n",
    "\n",
    "# Stillness æ£€æµ‹ï¼ˆä¸ç­–å±•ç‰ˆä¸€è‡´ï¼‰\n",
    "STILLNESS_SEC = 3.5\n",
    "MAX_BUF_SEC = 5.0\n",
    "FPS_ASSUMED = 12\n",
    "MOTION_EPS_CXCY = 4.0\n",
    "MOTION_EPS_AREA = 0.03\n",
    "MIN_FACE_AREA = 0.06\n",
    "\n",
    "# é¢œè‰²\n",
    "YELLOW = (255, 235, 59)\n",
    "BLACK = (0, 0, 0)\n",
    "HOT_PINK = (255, 30, 180)\n",
    "\n",
    "# å±•ç¤ºåŒºåŸŸå®½åº¦\n",
    "RIGHT_IMG_MAXW = 900\n",
    "\n",
    "\n",
    "# =================== å°å·¥å…·å‡½æ•° ===================\n",
    "\n",
    "_META_CACHE: Optional[Dict[str, Dict]] = None\n",
    "\n",
    "\n",
    "def load_meta_mapping() -> Dict[str, Dict]:\n",
    "    \"\"\"åŠ è½½æœ¬åœ° CSVï¼ŒæŒ‰ filename å»ºç«‹ç´¢å¼•ã€‚\"\"\"\n",
    "    global _META_CACHE\n",
    "    if _META_CACHE is not None:\n",
    "        return _META_CACHE\n",
    "\n",
    "    import csv\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    for p in META_CSV_CANDIDATES:\n",
    "        if p.exists():\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                rows = list(csv.DictReader(f))\n",
    "            break\n",
    "\n",
    "    mapping: Dict[str, Dict] = {}\n",
    "    for r in rows:\n",
    "        fname = (\n",
    "            r.get(\"filename\")\n",
    "            or r.get(\"image_path\")\n",
    "            or r.get(\"path\")\n",
    "            or r.get(\"file\")\n",
    "        )\n",
    "        if fname:\n",
    "            mapping[str(fname)] = r\n",
    "\n",
    "    _META_CACHE = mapping\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def lookup_meta(filename: str) -> Dict:\n",
    "    mapping = load_meta_mapping()\n",
    "    return mapping.get(filename, {})\n",
    "\n",
    "\n",
    "def safe_open_image(p: Path) -> Optional[Image.Image]:\n",
    "    try:\n",
    "        return Image.open(p).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_image_path(filename: str) -> Optional[Path]:\n",
    "    \"\"\"æ ¹æ® filename åœ¨ IMAGES_DIR é‡Œæ‰¾åˆ°å›¾ç‰‡ã€‚\"\"\"\n",
    "    if not filename:\n",
    "        return None\n",
    "    p = Path(filename)\n",
    "    if not p.is_absolute():\n",
    "        p = IMAGES_DIR / p\n",
    "    return p if p.exists() else None\n",
    "\n",
    "\n",
    "def _load_font(size: int = 40) -> ImageFont.FreeTypeFont:\n",
    "    \"\"\"ä¼˜å…ˆç”¨ Courier / Courier Newï¼Œæ‰¾ä¸åˆ°å†é€€å› Arial / é»˜è®¤ã€‚\"\"\"\n",
    "    candidates = [\n",
    "        \"/Library/Fonts/Courier New.ttf\",\n",
    "        \"/System/Library/Fonts/Courier.dfont\",\n",
    "        \"/System/Library/Fonts/Supplemental/Courier New.ttf\",\n",
    "        \"/Library/Fonts/Arial.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                return ImageFont.truetype(p, size=size)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def draw_tiny_metrics_top_right(\n",
    "    im: Image.Image, lines: List[str], size: int = 16, margin: int = 10\n",
    ") -> Image.Image:\n",
    "    \"\"\"å³ä¸Šè§’ç²‰è‰²æ–‡å­—ï¼šå§¿æ€æŒ‡æ ‡ï¼ˆä¸ç­–å±•ç‰ˆä¸€è‡´ï¼‰ã€‚\"\"\"\n",
    "    if not lines:\n",
    "        return im\n",
    "    img = im.copy()\n",
    "    d = ImageDraw.Draw(img)\n",
    "    font = _load_font(size)\n",
    "\n",
    "    widths = []\n",
    "    for s in lines:\n",
    "        l, t, r, b = d.textbbox((0, 0), s, font=font)\n",
    "        widths.append(r - l)\n",
    "    wmax = max(widths) if widths else 0\n",
    "\n",
    "    x = img.width - margin - wmax\n",
    "    y = margin\n",
    "    for s in lines:\n",
    "        d.text((x, y), s, fill=HOT_PINK, font=font)\n",
    "        _, _, _, b = d.textbbox((0, 0), s, font=font)\n",
    "        y += int(b * 0.95)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _angle_deg(p1, p2):\n",
    "    if p1 is None or p2 is None:\n",
    "        return None\n",
    "    vx, vy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "    return float(np.degrees(np.arctan2(vy, vx)))\n",
    "\n",
    "\n",
    "def _elbow_angle(shoulder, elbow, wrist):\n",
    "    if None in (shoulder, elbow, wrist):\n",
    "        return None\n",
    "    v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]], float)\n",
    "    v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]], float)\n",
    "    n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if n1 < 1e-5 or n2 < 1e-5:\n",
    "        return None\n",
    "    cosv = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cosv)))\n",
    "\n",
    "\n",
    "def format_metrics(kps: Dict[int, Tuple[float, float] | None]) -> List[str]:\n",
    "    \"\"\"æŠŠå…³é”®ç‚¹å˜æˆå‡ è¡Œå°å­—ï¼ˆä¸ç­–å±•ç‰ˆåŒæ ·ç»“æ„ï¼‰ã€‚\"\"\"\n",
    "    le, re = kps.get(1), kps.get(2)\n",
    "    lsh, rsh = kps.get(5), kps.get(6)\n",
    "    lel, rel = kps.get(7), kps.get(8)\n",
    "    lwr, rwr = kps.get(9), kps.get(10)\n",
    "\n",
    "    fdeg = lambda v: \"â€”\" if v is None else f\"{v:+.1f}Â°\"\n",
    "    fpt = lambda p: \"(0, 0)\" if p is None else f\"({int(p[0])}, {int(p[1])})\"\n",
    "\n",
    "    return [\n",
    "        f\"Head tilt: {fdeg(_angle_deg(re, le))}\",\n",
    "        f\"Shoulder:  {fdeg(_angle_deg(rsh, lsh))}\",\n",
    "        f\"L elbow:   {fdeg(_elbow_angle(lsh, lel, lwr))}\",\n",
    "        f\"R elbow:   {fdeg(_elbow_angle(rsh, rel, rwr))}\",\n",
    "        f\"L wrist:   {fpt(lwr)}\",\n",
    "        f\"R wrist:   {fpt(rwr)}\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def overlay_right_labels(painting: Image.Image, meta: Dict) -> Image.Image:\n",
    "    \"\"\"\n",
    "    åœ¨ç”»ä½œä¸Šå åŠ  3 ä¸ªé»„è‰²çŸ©å½¢æ ‡ç­¾ï¼š\n",
    "    1. ä»·æ ¼ï¼ˆprice_text / auction_price_usdï¼‰\n",
    "    2. å¹´ä»½ï¼ˆyearï¼‰\n",
    "    3. è‰ºæœ¯å®¶ï¼ˆartistï¼‰\n",
    "    å­—ä½“ï¼šCourierï¼Œé»‘å­—ï¼Œé»„è‰²èƒŒæ™¯ï¼Œé¡ºåºå›ºå®šã€‚\n",
    "    \"\"\"\n",
    "    im = painting.convert(\"RGB\").copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    font_big = _load_font(44)\n",
    "    font_small = _load_font(36)\n",
    "\n",
    "    price = (\n",
    "        meta.get(\"price_text\")\n",
    "        or meta.get(\"auction_price_usd\")\n",
    "        or meta.get(\"price\")\n",
    "        or \"â€”\"\n",
    "    )\n",
    "    year = str(meta.get(\"year\") or \"â€”\")\n",
    "    artist = meta.get(\"artist\") or \"artist name\"\n",
    "\n",
    "    lines = [price, year, artist]\n",
    "    fonts = [font_big, font_small, font_big]\n",
    "\n",
    "    margin_x = 24\n",
    "    # ä»ç”»é¢ä¸­éƒ¨ç•¥é ä¸Šå¼€å§‹\n",
    "    y = int(im.height * 0.50)\n",
    "\n",
    "    for text, font in zip(lines, fonts):\n",
    "        # ç”¨ textbboxï¼Œé¿å… textsize æŠ¥é”™\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font)\n",
    "        w, h = r - l, b - t\n",
    "\n",
    "        pad_x, pad_y = 16, 10\n",
    "        box_w, box_h = w + 2 * pad_x, h + 2 * pad_y\n",
    "\n",
    "        x = margin_x\n",
    "        draw.rectangle([x, y, x + box_w, y + box_h], fill=YELLOW)\n",
    "        draw.text((x + pad_x, y + pad_y), text, fill=BLACK, font=font)\n",
    "\n",
    "        y += box_h + 10\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def force_rerun():\n",
    "    try:\n",
    "        st.rerun()\n",
    "    except Exception:\n",
    "        try:\n",
    "            st.experimental_rerun()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =================== YOLO è§†é¢‘å¤„ç†ï¼ˆå®Œå…¨æ²¿ç”¨ç­–å±•éª¨æ¶é£æ ¼ï¼‰ ===================\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "    HAS_YOLO = True\n",
    "except Exception:\n",
    "    HAS_YOLO = False\n",
    "\n",
    "\n",
    "class CuratorialProcessor(VideoProcessorBase):\n",
    "    \"\"\"\n",
    "    ä¸ç­–å±•ç‰ˆä¸€è‡´ï¼š\n",
    "    - ä½¿ç”¨ YOLOv8-Pose ç»˜åˆ¶è“è‰²æ¡† + ç»¿è‰²éª¨æ¶\n",
    "    - å³ä¸Šè§’ç²‰è‰²æ–‡æœ¬æ˜¾ç¤ºå§¿æ€æŒ‡æ ‡\n",
    "    - æ”¯æŒè‡ªåŠ¨é™æ­¢æŠ“æ‹ & æ‰‹åŠ¨æŠ“æ‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        if HAS_YOLO:\n",
    "            try:\n",
    "                local = YOLO_MODEL_PATH\n",
    "                self.model = YOLO(str(local if local.exists() else \"yolov8n-pose.pt\"))\n",
    "            except Exception:\n",
    "                self.model = None\n",
    "\n",
    "        maxlen = max(6, int(MAX_BUF_SEC * FPS_ASSUMED))\n",
    "        self.cx_buf = deque(maxlen=maxlen)\n",
    "        self.cy_buf = deque(maxlen=maxlen)\n",
    "        self.area_buf = deque(maxlen=maxlen)\n",
    "        self.last_stable_ts: Optional[float] = None\n",
    "\n",
    "        self.captured_img: Optional[Image.Image] = None\n",
    "        self.captured_ts: float = 0.0\n",
    "        self.captured_metrics: List[str] = []\n",
    "\n",
    "        self.latest_rgb: Optional[np.ndarray] = None\n",
    "        self.last_metrics_lines: List[str] = []\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _detect_bbox(self, rgb: np.ndarray) -> Optional[Tuple[int, int, int, int]]:\n",
    "        if not self.model:\n",
    "            return None\n",
    "        res = self.model.predict(rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "        if len(res) == 0 or res[0].boxes is None:\n",
    "            return None\n",
    "        b = res[0].boxes.xyxy\n",
    "        if b is None or len(b) == 0:\n",
    "            return None\n",
    "        b = b.cpu().numpy()\n",
    "        areas = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "        i = int(np.argmax(areas))\n",
    "        x1, y1, x2, y2 = b[i].astype(int).tolist()\n",
    "        return x1, y1, x2, y2\n",
    "\n",
    "    def _extract_keypoints(self, res) -> Dict[int, Tuple[float, float] | None]:\n",
    "        kps: Dict[int, Tuple[float, float] | None] = {}\n",
    "        try:\n",
    "            if res and res[0].keypoints is not None and len(res[0].keypoints) > 0:\n",
    "                xy = res[0].keypoints.xy[0].cpu().numpy()\n",
    "                for i in range(xy.shape[0]):\n",
    "                    kps[i] = (float(xy[i, 0]), float(xy[i, 1]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return kps\n",
    "\n",
    "    def _update_stillness(self, bbox, w, h) -> bool:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cx = 0.5 * (x1 + x2)\n",
    "        cy = 0.5 * (y1 + y2)\n",
    "        area = max(1.0, (x2 - x1) * (y2 - y1))\n",
    "        rel_area = area / float(w * h)\n",
    "\n",
    "        if rel_area < MIN_FACE_AREA:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        self.cx_buf.append(cx)\n",
    "        self.cy_buf.append(cy)\n",
    "        self.area_buf.append(rel_area)\n",
    "\n",
    "        need_len = int(STILLNESS_SEC * FPS_ASSUMED * 0.6)\n",
    "        if len(self.cx_buf) < max(3, need_len):\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        stdx = float(np.std(self.cx_buf))\n",
    "        stdy = float(np.std(self.cy_buf))\n",
    "        stda = float(np.std(self.area_buf))\n",
    "        stable_now = (\n",
    "            stdx < MOTION_EPS_CXCY\n",
    "            and stdy < MOTION_EPS_CXCY\n",
    "            and stda < MOTION_EPS_AREA\n",
    "        )\n",
    "\n",
    "        now = time.time()\n",
    "        if stable_now:\n",
    "            if self.last_stable_ts is None:\n",
    "                self.last_stable_ts = now\n",
    "            return (now - self.last_stable_ts) >= STILLNESS_SEC\n",
    "        else:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "    def _stamp_capture(self):\n",
    "        \"\"\"æŠŠå½“å‰å¸§ä¸å½“å‰æŒ‡æ ‡ä½œä¸ºä¸€æ¬¡æŠ“æ‹ã€‚\"\"\"\n",
    "        self.captured_img = Image.fromarray(self.latest_rgb)\n",
    "        self.captured_ts = time.time()\n",
    "        self.captured_metrics = list(self.last_metrics_lines)\n",
    "        self.last_stable_ts = None\n",
    "\n",
    "    def capture_now(self) -> bool:\n",
    "        if self.latest_rgb is None:\n",
    "            return False\n",
    "        with self.lock:\n",
    "            self._stamp_capture()\n",
    "        return True\n",
    "\n",
    "    def recv(self, frame):\n",
    "        import av\n",
    "\n",
    "        img_bgr = frame.to_ndarray(format=\"bgr24\")\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        h, w, _ = img_rgb.shape\n",
    "        self.latest_rgb = img_rgb\n",
    "\n",
    "        if self.model:\n",
    "            res = self.model.predict(img_rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "            plotted = res[0].plot()[:, :, ::-1]  # YOLO è‡ªå¸¦è“æ¡†+ç»¿éª¨æ¶\n",
    "            kps = self._extract_keypoints(res)\n",
    "            lines = format_metrics(kps)\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = lines\n",
    "            pil = Image.fromarray(plotted)\n",
    "            pil = draw_tiny_metrics_top_right(pil, lines, size=16, margin=10)\n",
    "            out_rgb = np.array(pil)\n",
    "        else:\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = [\"(pose model not available)\"]\n",
    "            out_rgb = img_rgb\n",
    "\n",
    "        bbox = self._detect_bbox(img_rgb) if self.model else None\n",
    "        if bbox and self._update_stillness(bbox, w, h):\n",
    "            with self.lock:\n",
    "                if time.time() - self.captured_ts > 0.35:\n",
    "                    self._stamp_capture()\n",
    "\n",
    "        out_bgr = out_rgb[:, :, ::-1]\n",
    "        return av.VideoFrame.from_ndarray(out_bgr, format=\"bgr24\")\n",
    "\n",
    "\n",
    "# =================== Streamlit é¡µé¢ï¼ˆç­–å±•å¸ƒå±€ï¼‰ ===================\n",
    "\n",
    "st.set_page_config(page_title=APP_TITLE, layout=\"wide\")\n",
    "\n",
    "# ç®€å• CSSï¼šä¸¤åˆ—ç»“æ„ï¼Œå·¦æ‘„åƒå¤´ç«–å±å±…ä¸­ï¼Œå³å›¾åƒåŒºåŸŸå›ºå®šå®½åº¦\n",
    "st.markdown(\n",
    "    f\"\"\"\n",
    "<style>\n",
    "section[data-testid=\"stSidebar\"] {{ display: none !important; }}\n",
    "header, footer, [data-testid=\"stToolbar\"] {{ visibility: hidden !important; }}\n",
    ".block-container {{ padding-top: 0.6rem; padding-bottom: 0.6rem; max-width: 1700px; }}\n",
    ".left-col .cam-wrap {{\n",
    "  position: relative;\n",
    "  height: 92vh;\n",
    "  width: 100%;\n",
    "  overflow: hidden;\n",
    "  border-radius: 12px;\n",
    "  background: #111;\n",
    "}}\n",
    ".left-col .cam-wrap video {{\n",
    "  height: 100% !important;\n",
    "  width: auto !important;\n",
    "  object-fit: cover !important;\n",
    "  border-radius: 12px !important;\n",
    "}}\n",
    ".right-col .art-wrap {{\n",
    "  position: relative;\n",
    "  height: 92vh;\n",
    "  max-width: {RIGHT_IMG_MAXW}px;\n",
    "  overflow: hidden;\n",
    "  margin: 0 auto;\n",
    "}}\n",
    ".right-col .art-wrap img {{\n",
    "  display: block;\n",
    "  width: 100% !important;\n",
    "  height: auto !important;\n",
    "}}\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "st.title(APP_TITLE)\n",
    "st.caption(\n",
    "    \"Hold still for ~3â€“5 seconds to auto-capture, or press the button to capture on demand. \"\n",
    "    \"Left: live with pose. Right: matched artwork with tiny pink metrics.\"\n",
    ")\n",
    "\n",
    "# å°è¯•è½»å¾®è‡ªåŠ¨åˆ·æ–°ï¼Œè®©è‡ªåŠ¨æŠ“æ‹æ—¶å³ä¾§è‡ªåŠ¨æ›´æ–°\n",
    "try:\n",
    "    st.autorefresh(interval=700, key=\"ear_auto\", limit=None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "left, right = st.columns([1, 1], gap=\"large\")\n",
    "\n",
    "if \"countdown_target\" not in st.session_state:\n",
    "    st.session_state[\"countdown_target\"] = None\n",
    "\n",
    "if \"last_match\" not in st.session_state:\n",
    "    st.session_state[\"last_match\"] = None  # ä¿å­˜æœ€åä¸€æ¬¡ API è¿”å›å†…å®¹\n",
    "if \"last_metrics\" not in st.session_state:\n",
    "    st.session_state[\"last_metrics\"] = []\n",
    "if \"last_ts\" not in st.session_state:\n",
    "    st.session_state[\"last_ts\"] = 0.0\n",
    "\n",
    "API_URL = DEFAULT_API_URL  # å¦‚æœä½ å¸Œæœ›åœ¨ UI é‡Œå¯ç¼–è¾‘ï¼Œä¹Ÿå¯ä»¥åšæˆ text_input\n",
    "\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"Live\")\n",
    "    st.markdown('<div class=\"left-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"cam-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ctx = webrtc_streamer(\n",
    "        key=\"ear-curatorial-final\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "        video_processor_factory=CuratorialProcessor,\n",
    "        async_processing=True,\n",
    "    )\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /cam-wrap\n",
    "    st.markdown(\"<div style='height:10px'></div>\", unsafe_allow_html=True)\n",
    "\n",
    "    c1, c2 = st.columns(2)\n",
    "    with c1:\n",
    "        if st.button(\"ğŸ“¸ Capture (wait 3s)\", use_container_width=True):\n",
    "            st.session_state[\"countdown_target\"] = time.time() + 3.0\n",
    "    with c2:\n",
    "        if st.button(\"âš¡ Instant Capture\", use_container_width=True):\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"No frame yet, try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "\n",
    "    # å€’è®¡æ—¶é€»è¾‘\n",
    "    if st.session_state[\"countdown_target\"]:\n",
    "        remain = st.session_state[\"countdown_target\"] - time.time()\n",
    "        if remain > 0:\n",
    "            st.info(f\"Capturing in {remain:.1f}sâ€¦ Please hold still.\")\n",
    "        else:\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"No frame yet.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "            st.session_state[\"countdown_target\"] = None\n",
    "            force_rerun()\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /left-col\n",
    "\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"Matched artwork\")\n",
    "    st.markdown('<div class=\"right-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"art-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ph = st.empty()\n",
    "\n",
    "    proc: Optional[CuratorialProcessor] = None\n",
    "    if ctx and ctx.video_processor:\n",
    "        proc = ctx.video_processor\n",
    "\n",
    "    if not proc:\n",
    "        ph.info(\"Initializing cameraâ€¦\")\n",
    "    else:\n",
    "        # ä»å¤„ç†å™¨è¯»å–æœ€æ–°æŠ“æ‹\n",
    "        with proc.lock:\n",
    "            cap_img = proc.captured_img\n",
    "            cap_ts = getattr(proc, \"captured_ts\", 0.0)\n",
    "            cap_metrics = list(getattr(proc, \"captured_metrics\", []))\n",
    "\n",
    "        if cap_img is not None and cap_ts > st.session_state[\"last_ts\"]:\n",
    "            # æ›´æ–°æœ¬åœ°æ—¶é—´æˆ³\n",
    "            st.session_state[\"last_ts\"] = cap_ts\n",
    "            st.session_state[\"last_metrics\"] = cap_metrics\n",
    "\n",
    "            # æŠŠæŠ“æ‹å›¾å‘ç»™åç«¯ /match\n",
    "            buf = io.BytesIO()\n",
    "            cap_img.save(buf, format=\"JPEG\")\n",
    "            buf.seek(0)\n",
    "\n",
    "            files = {\"image\": (\"frame.jpg\", buf.getvalue(), \"image/jpeg\")}\n",
    "            data = {\"museum\": \"local\", \"topk\": 3}\n",
    "\n",
    "            try:\n",
    "                resp = requests.post(API_URL, files=files, data=data, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                payload = resp.json()\n",
    "                st.session_state[\"last_match\"] = payload\n",
    "            except Exception as exc:\n",
    "                st.session_state[\"last_match\"] = {\"error\": str(exc)}\n",
    "\n",
    "            force_rerun()\n",
    "\n",
    "        payload = st.session_state.get(\"last_match\")\n",
    "\n",
    "        if not payload:\n",
    "            ph.info(\"Hold still or press capture to trigger matchingâ€¦\")\n",
    "        elif \"error\" in payload:\n",
    "            ph.error(f\"Error from backend: {payload['error']}\")\n",
    "        else:\n",
    "            results = payload.get(\"results\") or []\n",
    "            if not results:\n",
    "                ph.warning(\"No matches returned from backend.\")\n",
    "            else:\n",
    "                # åªå– Top-1ï¼Œå±•é™ˆæ•ˆæœæ›´å¹²å‡€\n",
    "                top = results[0]\n",
    "                filename = top.get(\"filename\") or top.get(\"file\")\n",
    "\n",
    "                img_path = ensure_image_path(filename or \"\")\n",
    "                if not img_path:\n",
    "                    ph.error(f\"Image file not found for: {filename}\")\n",
    "                else:\n",
    "                    painting = safe_open_image(img_path)\n",
    "                    if painting is None:\n",
    "                        ph.error(f\"Failed to open image: {img_path}\")\n",
    "                    else:\n",
    "                        # ä»æœ¬åœ° CSV æŸ¥è¡¥å…… meta\n",
    "                        meta_row = lookup_meta(str(filename))\n",
    "                        meta = {\n",
    "                            \"artist\": meta_row.get(\"artist\") or top.get(\"artist\") or \"artist name\",\n",
    "                            \"year\": meta_row.get(\"year\") or top.get(\"year\") or \"\",\n",
    "                            \"price_text\": meta_row.get(\"price_text\")\n",
    "                            or meta_row.get(\"auction_price_usd\")\n",
    "                            or \"\",\n",
    "                        }\n",
    "\n",
    "                        painted = overlay_right_labels(painting, meta)\n",
    "\n",
    "                        # æŠŠæŠ“æ‹æ—¶çš„å§¿æ€æŒ‡æ ‡å åˆ°å³ä¸Šè§’ï¼ˆç²‰è‰²æ–‡æœ¬ï¼‰\n",
    "                        metrics = st.session_state.get(\"last_metrics\") or []\n",
    "                        painted = draw_tiny_metrics_top_right(\n",
    "                            painted, metrics, size=16, margin=12\n",
    "                        )\n",
    "\n",
    "                        w = min(RIGHT_IMG_MAXW, painted.width)\n",
    "                        caption = f\"{meta_row.get('title','')} â€” {meta.get('artist','')}\"\n",
    "                        ph.image(painted, caption=caption, width=w)\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /art-wrap\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /right-col\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
