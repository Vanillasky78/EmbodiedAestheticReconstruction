{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecb7f18",
   "metadata": {},
   "source": [
    "Recommended Top 10 Open Source Art Museums\n",
    "\n",
    "# Art Museum Country Data License Data Access Recommended Use\n",
    "\n",
    "1Ô∏è‚É£ The Metropolitan Museum of Art (The Met) üá∫üá∏ USA CC0 API\n",
    "\n",
    "/ CSV Largest, free, high-quality images\n",
    "\n",
    "2Ô∏è‚É£ Rijksmuseum üá≥üá± Netherlands CC0 API\n",
    "\n",
    "Excellent portraits with light and shadow and distinctive clothing features\n",
    "\n",
    "3Ô∏è‚É£ Tate Britain üá¨üáß UK CC-BY-NC GitHub CSV\n",
    "\n",
    "Pre-Raphaelite, Rossetti (key focus)\n",
    "\n",
    "4Ô∏è‚É£ National Gallery (London) üá¨üáß UK CC BY API\n",
    "Core of European classical portraiture\n",
    "\n",
    "5Ô∏è‚É£ Art Institute of Chicago (AIC) üá∫üá∏ USA CC0 API\n",
    "\n",
    "High-quality portraits, American Impressionism\n",
    "\n",
    "6Ô∏è‚É£ Cleveland Museum of Art (CMA) üá∫üá∏ USA CC0 API\n",
    "Baroque to Modern\n",
    "7Ô∏è‚É£ National Gallery of Art (Washington, NGA) üá∫üá∏ USA CC0 API\n",
    "Complete 18th‚Äì19th century portrait data\n",
    "8Ô∏è‚É£ Victoria and Albert Museum (V&A) üá¨üáß UK Open API\n",
    "Extensive images of fashion and body posture\n",
    "9Ô∏è‚É£ Museu Nacional d‚ÄôArt de Catalunya (MNAC) üá™üá∏ Spain CC BY API\n",
    "Gothic/Baroque figure data\n",
    "0Ô∏è‚É£ National Gallery of Australia (NGA-AUS) üá¶üá∫ Australia CC BY API / CSV\n",
    "Expanding non-Eurocentric perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f9dc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# main.py\n",
    "# Embodied Aesthetic Reconstruction ‚Äî Streamlit App (Resonance-first UI)\n",
    "# --------------------------------------------------------------------\n",
    "# - CLIP + Pose + Color + Significance fusion\n",
    "# - Human-centered UI: semantic \"resonance\" labels instead of raw numeric scores\n",
    "# - Sidebar toggle \"Debug mode\" to show raw similarity when needed\n",
    "# - Significance score (0‚Äì100) + badges + \"Learn more\"\n",
    "# - Extra metadata fields: price_estimate_usd, significance_text, interpretive_note_cn\n",
    "# - iCloud placeholder avoidance, robust pathing, cached embeddings\n",
    "# - YOLOv8n-pose via HuggingFace (optional), proper device mapping for MPS/CUDA/CPU\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "from __future__ import annotations\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "import streamlit as st\n",
    "\n",
    "# -------------------- Torch / device --------------------\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_OK = True\n",
    "except Exception:\n",
    "    TORCH_OK = False\n",
    "\n",
    "def get_device() -> str:\n",
    "    \"\"\"Return 'mps' on Apple Silicon, 'cuda' on Nvidia, else 'cpu'.\"\"\"\n",
    "    if not TORCH_OK:\n",
    "        return \"cpu\"\n",
    "    try:\n",
    "        if torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"cpu\"\n",
    "\n",
    "DEVICE = get_device()\n",
    "if TORCH_OK:\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"medium\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def yolo_device() -> str:\n",
    "    \"\"\"Map torch device to Ultralytics' expected device string.\"\"\"\n",
    "    if DEVICE == \"mps\":\n",
    "        return \"mps\"\n",
    "    if DEVICE == \"cuda\":\n",
    "        return \"0\"\n",
    "    return \"cpu\"\n",
    "\n",
    "# -------------------- Paths --------------------\n",
    "APP_DIR = Path(__file__).parent.resolve()\n",
    "DATA_DIR = APP_DIR / \"data\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "INTERIM_DIR = DATA_DIR / \"interim\"\n",
    "CACHE_DIR = APP_DIR / \".clip_cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METADATA_CSV = DATA_DIR / \"portrait_works.csv\"\n",
    "\n",
    "# -------------------- UI config --------------------\n",
    "st.set_page_config(\n",
    "    page_title=\"Embodied Aesthetic Reconstruction\",\n",
    "    page_icon=\"üñºÔ∏è\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "# -------------------- Utils --------------------\n",
    "def is_icloud_placeholder(p: Path) -> bool:\n",
    "    return p.suffix == \".icloud\" or p.name.endswith(\".icloud\")\n",
    "\n",
    "def load_image_safe(path: Path) -> Optional[Image.Image]:\n",
    "    if not path.exists() or is_icloud_placeholder(path):\n",
    "        return None\n",
    "    try:\n",
    "        with Image.open(path) as im:\n",
    "            return im.convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def pil_from_bytes(b: bytes) -> Image.Image:\n",
    "    return Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "\n",
    "def center_crop_long_edge(im: Image.Image, size: int = 512) -> Image.Image:\n",
    "    im = ImageOps.exif_transpose(im)\n",
    "    w, h = im.size\n",
    "    s = min(w, h)\n",
    "    left = (w - s) // 2\n",
    "    top = (h - s) // 2\n",
    "    im = im.crop((left, top, left + s, top + s))\n",
    "    return im.resize((size, size), Image.BICUBIC)\n",
    "\n",
    "# -------------------- Sidebar --------------------\n",
    "with st.sidebar:\n",
    "    st.header(\"‚öôÔ∏è Matching Weights\")\n",
    "    w_clip = st.slider(\"CLIP weight\", 0.0, 1.0, 1.00, 0.05)\n",
    "    w_pose = st.slider(\"Pose weight\", 0.0, 1.0, 0.30, 0.05)\n",
    "    w_color = st.slider(\"Color weight\", 0.0, 1.0, 0.20, 0.05)\n",
    "    w_sig  = st.slider(\"Significance weight\", 0.0, 1.0, 0.20, 0.05)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"üîé Filters\")\n",
    "    require_public = st.checkbox(\"Require Public-Domain license\", value=False)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"üß™ Debug\")\n",
    "    show_debug = st.checkbox(\"Show raw similarity score\", value=False)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"üìä Results\")\n",
    "    top_k = st.slider(\"Top-K artworks\", 1, 20, 6, 1)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"üíª Active device\")\n",
    "    st.success(f\"{DEVICE}\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"üßç Overlay\")\n",
    "    overlay_skeleton = st.checkbox(\"Draw skeleton on preview (if available)\", value=True)\n",
    "\n",
    "# -------------------- Metadata & dataset --------------------\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expected (optional) columns:\n",
    "    filename,title,artist,year,license,museum,accession,movement,\n",
    "    is_masterwork,citations,exhibitions,auction_price_usd,views_per_year,\n",
    "    price_estimate_usd,significance_text,interpretive_note_cn,notable_tags,source_links\n",
    "    \"\"\"\n",
    "    if METADATA_CSV.exists():\n",
    "        try:\n",
    "            return pd.read_csv(METADATA_CSV)\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not read metadata CSV ({METADATA_CSV.name}): {e}\")\n",
    "    # Fallback minimal schema\n",
    "    return pd.DataFrame({\n",
    "        \"filename\": [], \"title\": [], \"artist\": [], \"year\": [], \"license\": [],\n",
    "        \"museum\": [], \"accession\": [], \"movement\": [], \"is_masterwork\": [],\n",
    "        \"citations\": [], \"exhibitions\": [], \"auction_price_usd\": [], \"views_per_year\": [],\n",
    "        \"price_estimate_usd\": [], \"significance_text\": [], \"interpretive_note_cn\": [],\n",
    "        \"notable_tags\": [], \"source_links\": []\n",
    "    })\n",
    "\n",
    "META = load_metadata()\n",
    "\n",
    "def list_dataset_images() -> List[Path]:\n",
    "    if not IMAGES_DIR.exists():\n",
    "        return []\n",
    "    files: List[Path] = []\n",
    "    for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"):\n",
    "        files += list(IMAGES_DIR.glob(ext))\n",
    "    files = [p for p in files if not is_icloud_placeholder(p)]\n",
    "    return sorted(files)\n",
    "\n",
    "DATASET_FILES = list_dataset_images()\n",
    "\n",
    "# -------------------- Significance (0..100) --------------------\n",
    "TOP_MUSEUMS = [\"met\", \"national gallery\", \"tate\", \"louvre\", \"uffizi\", \"hermitage\", \"ng london\"]\n",
    "\n",
    "def _logn(x, d=1.0):\n",
    "    try:\n",
    "        return math.log10(max(float(x), 1.0)) / d\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def compute_significance_row(row) -> float:\n",
    "    \"\"\"Heuristic significance score (0..100).\"\"\"\n",
    "    w = dict(master=0.35, museum=0.20, cites=0.15, exhib=0.10, auction=0.10, views=0.10)\n",
    "    score = 0.0\n",
    "    if str(row.get(\"is_masterwork\", \"0\")).lower() in [\"1\", \"true\", \"yes\", \"y\"]:\n",
    "        score += w[\"master\"]\n",
    "    museum = str(row.get(\"museum\", \"\")).lower()\n",
    "    if museum:\n",
    "        score += w[\"museum\"] * (1.0 if any(m in museum for m in TOP_MUSEUMS) else 0.5)\n",
    "    score += w[\"cites\"]   * _logn(row.get(\"citations\", 0),        d=3.0)\n",
    "    score += w[\"exhib\"]   * _logn(row.get(\"exhibitions\", 0),      d=2.0)\n",
    "    score += w[\"auction\"] * _logn(row.get(\"auction_price_usd\",0), d=8.0)\n",
    "    score += w[\"views\"]   * _logn(row.get(\"views_per_year\", 0),   d=6.0)\n",
    "    return float(np.clip(score, 0.0, 1.0) * 100.0)\n",
    "\n",
    "def significance_badges(row) -> List[str]:\n",
    "    badges = []\n",
    "    if str(row.get(\"is_masterwork\",\"0\")).lower() in [\"1\",\"true\",\"yes\",\"y\"]:\n",
    "        badges.append(\"Masterwork\")\n",
    "    if str(row.get(\"museum\",\"\")).strip():\n",
    "        badges.append(\"Permanent Collection\")\n",
    "    try:\n",
    "        if int(row.get(\"citations\", 0)) >= 50:\n",
    "            badges.append(\"Canon\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return badges[:3]\n",
    "\n",
    "def filename_key(path_or_name: str) -> str:\n",
    "    return Path(path_or_name).name\n",
    "\n",
    "META_BY_NAME = {}\n",
    "if not META.empty and \"filename\" in META.columns:\n",
    "    for _, r in META.iterrows():\n",
    "        META_BY_NAME[filename_key(str(r.get(\"filename\",\"\")))] = r\n",
    "\n",
    "# -------------------- OpenCLIP --------------------\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_openclip():\n",
    "    try:\n",
    "        import open_clip\n",
    "    except Exception as e:\n",
    "        st.error(f\"open_clip_torch not installed: {e}\")\n",
    "        return None, None, None\n",
    "    try:\n",
    "        model_name, pretrained = \"ViT-B-32\", \"laion2b_s34b_b79k\"\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "            model_name, pretrained=pretrained, device=DEVICE\n",
    "        )\n",
    "        tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        model.eval()\n",
    "        return model, preprocess, tokenizer\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to load OpenCLIP: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "MODEL_CLIP, PRE_CLIP, TOKENIZER = load_openclip()\n",
    "\n",
    "def tensor_from_pil_clip(im: Image.Image):\n",
    "    if PRE_CLIP is None:\n",
    "        return None\n",
    "    t = PRE_CLIP(im).unsqueeze(0)\n",
    "    if TORCH_OK:\n",
    "        t = t.to(DEVICE)\n",
    "    return t\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def embed_image_clip(img_bytes: bytes) -> Optional[np.ndarray]:\n",
    "    if MODEL_CLIP is None:\n",
    "        return None\n",
    "    try:\n",
    "        im = pil_from_bytes(img_bytes)\n",
    "        im = center_crop_long_edge(im, size=224)\n",
    "        x = tensor_from_pil_clip(im)\n",
    "        with torch.no_grad():\n",
    "            feat = MODEL_CLIP.encode_image(x)\n",
    "            feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "        return feat.cpu().numpy().astype(\"float32\")[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------- YOLO weights via HuggingFace (optional) --------------------\n",
    "def get_yolo_pose_weights_path() -> Optional[str]:\n",
    "    try:\n",
    "        from huggingface_hub import hf_hub_download\n",
    "    except Exception:\n",
    "        return \"yolov8n-pose.pt\"\n",
    "    try:\n",
    "        local = hf_hub_download(\n",
    "            repo_id=\"ultralytics/yolov8n-pose\",\n",
    "            filename=\"yolov8n-pose.pt\",\n",
    "            local_dir=str(Path.home() / \".cache\" / \"hf\"),\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        return local\n",
    "    except Exception:\n",
    "        return \"yolov8n-pose.pt\"\n",
    "\n",
    "# -------------------- Feature extraction & cache --------------------\n",
    "EMB_PATH   = CACHE_DIR / \"embeddings.npy\"\n",
    "IDS_PATH   = CACHE_DIR / \"ids.json\"\n",
    "POSE_PATH  = CACHE_DIR / \"pose.npy\"\n",
    "COLOR_PATH = CACHE_DIR / \"color.npy\"\n",
    "SIG_PATH   = CACHE_DIR / \"significance.npy\"\n",
    "\n",
    "def _color_feature(im: Image.Image) -> np.ndarray:\n",
    "    im = im.resize((256, 256), Image.BILINEAR)\n",
    "    arr = np.array(im.convert(\"HSV\"))\n",
    "    h, s, v = arr[..., 0], arr[..., 1], arr[..., 2]\n",
    "    hist_h, _ = np.histogram(h, bins=32, range=(0, 255), density=True)\n",
    "    hist_s, _ = np.histogram(s, bins=16, range=(0, 255), density=True)\n",
    "    hist_v, _ = np.histogram(v, bins=8,  range=(0, 255), density=True)\n",
    "    feat = np.concatenate([hist_h, hist_s, hist_v]).astype(\"float32\")\n",
    "    return feat / (np.linalg.norm(feat) + 1e-8)\n",
    "\n",
    "def _pose_feature_from_kpts(kpts_xyv: np.ndarray) -> np.ndarray:\n",
    "    if kpts_xyv is None or len(kpts_xyv) == 0:\n",
    "        return np.zeros(34, dtype=\"float32\")\n",
    "    xy = kpts_xyv[:, :2]\n",
    "    min_xy = xy.min(0); max_xy = xy.max(0)\n",
    "    wh = np.maximum(max_xy - min_xy, 1e-6)\n",
    "    xy_n = (xy - min_xy) / wh\n",
    "    feat = xy_n.reshape(-1).astype(\"float32\")\n",
    "    if feat.shape[0] < 34:\n",
    "        feat = np.pad(feat, (0, 34 - feat.shape[0]))\n",
    "    else:\n",
    "        feat = feat[:34]\n",
    "    return feat / (np.linalg.norm(feat) + 1e-8)\n",
    "\n",
    "def _detect_pose_pil(im: Image.Image) -> Optional[np.ndarray]:\n",
    "    try:\n",
    "        from ultralytics import YOLO\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        weights = get_yolo_pose_weights_path()\n",
    "        model = YOLO(weights)\n",
    "        arr = np.array(im.convert(\"RGB\"))\n",
    "        res = model.predict(\n",
    "            source=arr, imgsz=512, conf=0.25, verbose=False, device=yolo_device()\n",
    "        )\n",
    "        if not res or len(res[0].keypoints) == 0:\n",
    "            return None\n",
    "        kpts = res[0].keypoints.xy.cpu().numpy()\n",
    "        if kpts.ndim == 4: kpts = kpts[0]\n",
    "        if kpts.ndim == 3: k = kpts[0]\n",
    "        else: return None\n",
    "        v = np.ones((k.shape[0], 1), dtype=\"float32\")\n",
    "        kxyv = np.concatenate([k.astype(\"float32\"), v], axis=1)\n",
    "        return _pose_feature_from_kpts(kxyv)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _dataset_pose_feature(path: Path) -> Optional[np.ndarray]:\n",
    "    im = load_image_safe(path)\n",
    "    if im is None:\n",
    "        return None\n",
    "    im_small = center_crop_long_edge(im, 512)\n",
    "    return _detect_pose_pil(im_small)\n",
    "\n",
    "@st.cache_data(show_spinner=True, persist=True)\n",
    "def build_dataset_embeddings(files: List[Path]):\n",
    "    \"\"\"Return (clip_embeds, ids, pose_feats, color_feats, signif_norm).\"\"\"\n",
    "    ids = [str(p.relative_to(APP_DIR)) for p in files]\n",
    "\n",
    "    # Use cache if aligned\n",
    "    if EMB_PATH.exists() and IDS_PATH.exists() and SIG_PATH.exists():\n",
    "        try:\n",
    "            cached_ids = json.loads(IDS_PATH.read_text())\n",
    "            if cached_ids == ids:\n",
    "                clip_arr  = np.load(EMB_PATH)\n",
    "                pose_arr  = np.load(POSE_PATH)  if POSE_PATH.exists()  else None\n",
    "                color_arr = np.load(COLOR_PATH) if COLOR_PATH.exists() else None\n",
    "                sig_arr   = np.load(SIG_PATH)\n",
    "                return clip_arr, ids, pose_arr, color_arr, sig_arr\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    clip_feats, pose_feats, color_feats, signif_vals = [], [], [], []\n",
    "\n",
    "    if MODEL_CLIP is None:\n",
    "        st.error(\"OpenCLIP not ready; cannot build embeddings.\")\n",
    "        return np.zeros((0, 512), dtype=\"float32\"), ids, None, None, np.zeros((0,), dtype=\"float32\")\n",
    "\n",
    "    progress = st.progress(0.0, text=\"Building dataset embeddings/features‚Ä¶\")\n",
    "    for i, p in enumerate(files):\n",
    "        progress.progress((i + 1) / max(1, len(files)))\n",
    "        im = load_image_safe(p)\n",
    "\n",
    "        # CLIP\n",
    "        if im is not None:\n",
    "            try:\n",
    "                im_clip = center_crop_long_edge(im, 224)\n",
    "                x = tensor_from_pil_clip(im_clip)\n",
    "                with torch.no_grad():\n",
    "                    z = MODEL_CLIP.encode_image(x)\n",
    "                    z = z / z.norm(dim=-1, keepdim=True)\n",
    "                clip_feats.append(z.cpu().numpy().astype(\"float32\")[0])\n",
    "            except Exception:\n",
    "                clip_feats.append(np.zeros(512, dtype=\"float32\"))\n",
    "        else:\n",
    "            clip_feats.append(np.zeros(512, dtype=\"float32\"))\n",
    "\n",
    "        # Color\n",
    "        if im is not None:\n",
    "            try:\n",
    "                color_feats.append(_color_feature(im))\n",
    "            except Exception:\n",
    "                color_feats.append(np.zeros(56, dtype=\"float32\"))\n",
    "        else:\n",
    "            color_feats.append(np.zeros(56, dtype=\"float32\"))\n",
    "\n",
    "        # Pose\n",
    "        pf = _dataset_pose_feature(p)\n",
    "        pose_feats.append(pf if pf is not None else np.zeros(34, dtype=\"float32\"))\n",
    "\n",
    "        # Significance prior from metadata (0..1)\n",
    "        fname = filename_key(p.name)\n",
    "        row = META_BY_NAME.get(fname)\n",
    "        sig = compute_significance_row(row) / 100.0 if row is not None else 0.0\n",
    "        signif_vals.append(float(sig))\n",
    "\n",
    "    clip_arr  = np.vstack(clip_feats).astype(\"float32\") if clip_feats else np.zeros((0, 512), dtype=\"float32\")\n",
    "    pose_arr  = np.vstack(pose_feats).astype(\"float32\") if pose_feats else None\n",
    "    color_arr = np.vstack(color_feats).astype(\"float32\") if color_feats else None\n",
    "    sig_arr   = np.array(signif_vals, dtype=\"float32\")\n",
    "\n",
    "    # Cache\n",
    "    try:\n",
    "        np.save(EMB_PATH, clip_arr)\n",
    "        np.save(POSE_PATH, pose_arr)\n",
    "        np.save(COLOR_PATH, color_arr)\n",
    "        np.save(SIG_PATH,  sig_arr)\n",
    "        IDS_PATH.write_text(json.dumps(ids))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return clip_arr, ids, pose_arr, color_arr, sig_arr\n",
    "\n",
    "# ---- Build dataset features ----\n",
    "if len(DATASET_FILES) == 0:\n",
    "    st.error(f\"‚ùå No images found or folder is empty: {IMAGES_DIR}\\nMake sure iCloud files are downloaded locally (no .icloud).\")\n",
    "else:\n",
    "    st.success(f\"‚úÖ Dataset images: {len(DATASET_FILES)}\")\n",
    "\n",
    "CLIP_DS, IDS, POSE_DS, COLOR_DS, SIG_DS = build_dataset_embeddings(DATASET_FILES)  # SIG_DS in [0,1]\n",
    "\n",
    "# ---- Ready checks ----\n",
    "READY = True\n",
    "reasons = []\n",
    "if MODEL_CLIP is None:\n",
    "    READY = False; reasons.append(\"OpenCLIP not loaded.\")\n",
    "if len(DATASET_FILES) == 0:\n",
    "    READY = False; reasons.append(\"No images in data/images (or still .icloud).\")\n",
    "if CLIP_DS is None or getattr(CLIP_DS, \"shape\", (0,))[0] == 0:\n",
    "    READY = False; reasons.append(\"Dataset embeddings not built yet.\")\n",
    "if not READY:\n",
    "    st.warning(\"Matching is not ready: \" + \" \".join(reasons))\n",
    "else:\n",
    "    st.success(\"Matching is ready ‚úî\")\n",
    "\n",
    "# -------------------- Similarity + Resonance --------------------\n",
    "def cos_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    if a is None or b is None:\n",
    "        return None\n",
    "    if a.ndim == 1:\n",
    "        a = a[None, :]\n",
    "    a = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-8)\n",
    "    b = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-8)\n",
    "    return (a @ b.T)\n",
    "\n",
    "def interpret_score(score: float) -> str:\n",
    "    \"\"\"Map numeric similarity to a poetic/semantic label.\"\"\"\n",
    "    if score > 0.80: return \"Strong resonance üí´\"\n",
    "    if score > 0.65: return \"Aesthetic kinship ‚ú®\"\n",
    "    if score > 0.50: return \"Subtle correspondence üåô\"\n",
    "    return \"Distant echo üå´Ô∏è\"\n",
    "\n",
    "# -------------------- Pose overlay (preview only) --------------------\n",
    "def draw_skeleton_overlay(im: Image.Image) -> Tuple[Image.Image, Optional[np.ndarray]]:\n",
    "    try:\n",
    "        from ultralytics import YOLO\n",
    "        import cv2\n",
    "    except Exception:\n",
    "        return im, None\n",
    "\n",
    "    weights = get_yolo_pose_weights_path()\n",
    "    model = YOLO(weights)\n",
    "    arr = np.array(im.convert(\"RGB\"))\n",
    "    res = model.predict(source=arr, imgsz=512, conf=0.25, verbose=False, device=yolo_device())\n",
    "    if not res or len(res[0].keypoints) == 0:\n",
    "        return im, None\n",
    "\n",
    "    canvas = arr.copy()\n",
    "    pts = res[0].keypoints.xy.cpu().numpy()\n",
    "    if pts.ndim == 4: pts = pts[0]\n",
    "    if pts.ndim == 3: pts = pts[0]\n",
    "\n",
    "    PAIRS = [(5,7),(7,9),(6,8),(8,10),(11,13),(13,15),(12,14),(14,16),(5,6),(11,12),(5,11),(6,12)]\n",
    "    try:\n",
    "        import cv2\n",
    "        for (a,b) in PAIRS:\n",
    "            xa,ya = pts[a]; xb,yb = pts[b]\n",
    "            cv2.line(canvas,(int(xa),int(ya)),(int(xb),int(yb)),(0,255,0),3)\n",
    "        for (x,y) in pts:\n",
    "            cv2.circle(canvas,(int(x),int(y)),4,(0,0,255),-1)\n",
    "        im_draw = Image.fromarray(canvas)\n",
    "    except Exception:\n",
    "        im_draw = im\n",
    "\n",
    "    v = np.ones((pts.shape[0],1),dtype=\"float32\")\n",
    "    kxyv = np.concatenate([pts.astype(\"float32\"), v], axis=1)\n",
    "    pf = _pose_feature_from_kpts(kxyv)\n",
    "    return im_draw, pf\n",
    "\n",
    "# -------------------- UI ‚Äî Main --------------------\n",
    "st.title(\"Embodied Aesthetic Reconstruction\")\n",
    "st.caption(\"Camera / Upload ‚Üí CLIP + Pose + Color + Significance ‚Üí Resonant artworks\")\n",
    "\n",
    "status_cols = st.columns(3)\n",
    "with status_cols[0]:\n",
    "    st.success(f\"Images dir: {IMAGES_DIR.relative_to(APP_DIR)}\" if IMAGES_DIR.exists() else f\"Missing: {IMAGES_DIR}\")\n",
    "with status_cols[1]:\n",
    "    st.info(f\"Dataset files: {len(DATASET_FILES)}\")\n",
    "with status_cols[2]:\n",
    "    st.info(f\"Models: OpenCLIP={'‚úîÔ∏é' if MODEL_CLIP else '‚úñÔ∏é'} ¬∑ Pose(ultralytics)={'‚úîÔ∏é'}\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "\n",
    "left, right = st.columns([1, 1])\n",
    "query_img: Optional[Image.Image] = None\n",
    "query_pose_feat: Optional[np.ndarray] = None\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"üì∑ Camera (auto center-crop)\")\n",
    "    cam = st.camera_input(\"Take a photo (allow permission first)\", key=\"camera\")\n",
    "    if cam is not None:\n",
    "        try:\n",
    "            img = pil_from_bytes(cam.getvalue())\n",
    "            img = center_crop_long_edge(img, 640)\n",
    "            out_path = INTERIM_DIR / \"locked_frame.jpg\"\n",
    "            img.save(out_path, quality=92)\n",
    "            st.success(f\"Saved: {out_path.as_posix()}\")\n",
    "            if overlay_skeleton:\n",
    "                img_draw, q_pose = draw_skeleton_overlay(img)\n",
    "                st.image(img_draw, caption=\"Camera preview\", use_container_width=True)\n",
    "                query_img = img_draw\n",
    "                query_pose_feat = q_pose\n",
    "            else:\n",
    "                st.image(img, caption=\"Camera preview\", use_container_width=True)\n",
    "                query_img = img\n",
    "        except Exception as e:\n",
    "            st.error(f\"Camera decode failed: {e}\")\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"üñºÔ∏è Or upload an image\")\n",
    "    up = st.file_uploader(\"JPG/PNG\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "    if up is not None:\n",
    "        try:\n",
    "            img = pil_from_bytes(up.getvalue())\n",
    "            img = center_crop_long_edge(img, 640)\n",
    "            if overlay_skeleton:\n",
    "                img_draw, q_pose = draw_skeleton_overlay(img)\n",
    "                st.image(img_draw, caption=\"Uploaded preview\", use_container_width=True)\n",
    "                query_img = img_draw\n",
    "                query_pose_feat = q_pose\n",
    "            else:\n",
    "                st.image(img, caption=\"Uploaded preview\", use_container_width=True)\n",
    "                query_img = img\n",
    "        except Exception as e:\n",
    "            st.error(f\"Upload decode failed: {e}\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "run = st.button(\"üîé Run Matching\", disabled=not READY)\n",
    "\n",
    "# -------------------- Display helpers --------------------\n",
    "def display_results(idx_scores: List[Tuple[int, float]], k: int):\n",
    "    if len(idx_scores) == 0:\n",
    "        st.warning(\"No results to display.\")\n",
    "        return\n",
    "    cols = st.columns(min(3, k))\n",
    "    for i, (idx, sc) in enumerate(idx_scores[:k]):\n",
    "        fn_rel = IDS[idx]\n",
    "        p = (APP_DIR / fn_rel).resolve()\n",
    "        im = load_image_safe(p)\n",
    "\n",
    "        row = META_BY_NAME.get(filename_key(p.name))\n",
    "        if row is not None:\n",
    "            title  = str(row.get(\"title\", \"\")).strip()\n",
    "            artist = str(row.get(\"artist\", \"\")).strip()\n",
    "            year   = str(row.get(\"year\", \"\")).strip()\n",
    "            museum = str(row.get(\"museum\", \"\")).strip()\n",
    "            price  = str(row.get(\"price_estimate_usd\", \"\")).strip()\n",
    "            sigtxt = str(row.get(\"significance_text\", \"\")).strip()\n",
    "            sigcn  = str(row.get(\"interpretive_note_cn\", \"\")).strip()\n",
    "            links  = str(row.get(\"source_links\", \"\")).strip()\n",
    "            sig100 = compute_significance_row(row)\n",
    "            badges = significance_badges(row)\n",
    "        else:\n",
    "            title=artist=year=museum=price=sigtxt=sigcn=links=\"\"\n",
    "            sig100, badges = 0.0, []\n",
    "\n",
    "        with cols[i % len(cols)]:\n",
    "            if im is not None:\n",
    "                st.image(im, use_container_width=True)\n",
    "\n",
    "            # Resonance-first display\n",
    "            if show_debug:\n",
    "                st.markdown(f\"**Score:** {sc:.3f}\")\n",
    "            else:\n",
    "                st.markdown(f\"**{interpret_score(sc)}**\")\n",
    "\n",
    "            if title or artist or year:\n",
    "                st.caption(f\"{title} ‚Äî {artist} ({year})\")\n",
    "            if museum:\n",
    "                st.caption(f\"üèõÔ∏è {museum}\")\n",
    "            if price:\n",
    "                st.caption(f\"üí∞ Estimated value: ${price}\")\n",
    "\n",
    "            st.markdown(f\"**Significance:** {sig100:.0f}/100\")\n",
    "            if badges:\n",
    "                st.caption(\" ¬∑ \".join(badges))\n",
    "            if sigtxt:\n",
    "                st.markdown(f\"_{sigtxt}_\")\n",
    "            if sigcn:\n",
    "                st.markdown(f\"**Curatorial note (CN):** {sigcn}\")\n",
    "            if links:\n",
    "                first = links.split(\";\")[0].strip()\n",
    "                if first:\n",
    "                    st.write(f\"[Learn more]({first})\")\n",
    "\n",
    "# -------------------- Matching --------------------\n",
    "if run:\n",
    "    if query_img is None:\n",
    "        st.warning(\"Please take a photo or upload an image first.\")\n",
    "    else:\n",
    "        st.write(\"Computing embeddings‚Ä¶\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        q_clip = None\n",
    "        if MODEL_CLIP is not None:\n",
    "            try:\n",
    "                buf = io.BytesIO()\n",
    "                query_img.save(buf, format=\"JPEG\", quality=92)\n",
    "                q_clip = embed_image_clip(buf.getvalue())\n",
    "            except Exception:\n",
    "                q_clip = None\n",
    "\n",
    "        try:\n",
    "            q_color = _color_feature(query_img)\n",
    "        except Exception:\n",
    "            q_color = None\n",
    "\n",
    "        if query_pose_feat is None and w_pose > 0:\n",
    "            query_pose_feat = _detect_pose_pil(center_crop_long_edge(query_img, 512))\n",
    "        w_pose_eff = w_pose if query_pose_feat is not None and POSE_DS is not None else 0.0\n",
    "        if w_pose > 0 and w_pose_eff == 0.0:\n",
    "            st.info(\"No pose feature detected or dataset pose not precomputed; treating Pose weight as 0.\")\n",
    "\n",
    "        valid_indices = np.arange(len(IDS))\n",
    "        if require_public and not META.empty and {\"filename\", \"license\"}.issubset(META.columns):\n",
    "            pd_mask = META[\"license\"].astype(str).str.contains(\"Public Domain\", case=False, na=False)\n",
    "            pool_names = set(META.loc[pd_mask, \"filename\"].astype(str).apply(lambda s: Path(s).name))\n",
    "            filt = [i for i, fn in enumerate(IDS) if Path(fn).name in pool_names]\n",
    "            if len(filt) > 0:\n",
    "                valid_indices = np.array(filt, dtype=int)\n",
    "            else:\n",
    "                st.warning(\"No entries meet Public-Domain filter; ignoring filter.\")\n",
    "\n",
    "        def slicer(arr):\n",
    "            if arr is None or arr.shape[0] != len(IDS):\n",
    "                return None\n",
    "            return arr[valid_indices]\n",
    "\n",
    "        CLIP_POOL  = slicer(CLIP_DS)\n",
    "        POSE_POOL  = slicer(POSE_DS)\n",
    "        COLOR_POOL = slicer(COLOR_DS)\n",
    "        SIG_POOL   = SIG_DS[valid_indices] if SIG_DS is not None and SIG_DS.shape[0] == len(IDS) else None\n",
    "\n",
    "        def cos_sim_local(q, bank):\n",
    "            if q is None or bank is None:\n",
    "                return None\n",
    "            a = q[None, :] if q.ndim == 1 else q\n",
    "            a = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-8)\n",
    "            b = bank / (np.linalg.norm(bank, axis=1, keepdims=True) + 1e-8)\n",
    "            return (a @ b.T)[0]\n",
    "\n",
    "        score = np.zeros(valid_indices.size, dtype=\"float32\")\n",
    "        s = cos_sim_local(q_clip, CLIP_POOL)\n",
    "        if s is not None: score += w_clip * s\n",
    "        s = cos_sim_local(query_pose_feat, POSE_POOL) if w_pose_eff > 0 else None\n",
    "        if s is not None: score += w_pose_eff * s\n",
    "        s = cos_sim_local(q_color, COLOR_POOL)\n",
    "        if s is not None: score += w_color * s\n",
    "        if SIG_POOL is not None and w_sig > 0.0:\n",
    "            score += w_sig * SIG_POOL\n",
    "\n",
    "        topk_local = np.argsort(-score)[:min(top_k, score.size)]\n",
    "        results = [(int(valid_indices[i]), float(score[i])) for i in topk_local]\n",
    "\n",
    "        dt = (time.time() - t0) * 1000\n",
    "        st.info(f\"Search time: {dt:.1f} ms\")\n",
    "\n",
    "        display_results(results, k=top_k)\n",
    "\n",
    "# -------------------- Tips --------------------\n",
    "with st.expander(\"Tips\"):\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "- If you see **iCloud .icloud** placeholders, open the folder in Finder and choose **‚ÄúDownload Now‚Äù**.\n",
    "- Adjust **CLIP / Pose / Color / Significance** weights to influence ranking.\n",
    "- **Debug mode** reveals raw similarity scores; keep it off for exhibitions to maintain a poetic tone.\n",
    "- The **Public-Domain** filter works only if `license` exists in your metadata CSV.\n",
    "- First run builds and caches dataset features in `.clip_cache/`.\n",
    "- Pose detection uses `ultralytics` (YOLOv8n-pose). Weights are pulled from Hugging Face when possible.\n",
    "- *Significance* is a heuristic combining museum presence, citations, and attention. It reflects historical canons yet encourages plural aesthetics.\n",
    "\"\"\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
