{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecb7f18",
   "metadata": {},
   "source": [
    "Recommended Top 10 Open Source Art Museums\n",
    "\n",
    "# Art Museum Country Data License Data Access Recommended Use\n",
    "\n",
    "1ï¸âƒ£ The Metropolitan Museum of Art (The Met) ğŸ‡ºğŸ‡¸ USA CC0 API\n",
    "\n",
    "/ CSV Largest, free, high-quality images\n",
    "\n",
    "2ï¸âƒ£ Rijksmuseum ğŸ‡³ğŸ‡± Netherlands CC0 API\n",
    "\n",
    "Excellent portraits with light and shadow and distinctive clothing features\n",
    "\n",
    "3ï¸âƒ£ Tate Britain ğŸ‡¬ğŸ‡§ UK CC-BY-NC GitHub CSV\n",
    "\n",
    "Pre-Raphaelite, Rossetti (key focus)\n",
    "\n",
    "4ï¸âƒ£ National Gallery (London) ğŸ‡¬ğŸ‡§ UK CC BY API\n",
    "Core of European classical portraiture\n",
    "\n",
    "5ï¸âƒ£ Art Institute of Chicago (AIC) ğŸ‡ºğŸ‡¸ USA CC0 API\n",
    "\n",
    "High-quality portraits, American Impressionism\n",
    "\n",
    "6ï¸âƒ£ Cleveland Museum of Art (CMA) ğŸ‡ºğŸ‡¸ USA CC0 API\n",
    "Baroque to Modern\n",
    "7ï¸âƒ£ National Gallery of Art (Washington, NGA) ğŸ‡ºğŸ‡¸ USA CC0 API\n",
    "Complete 18thâ€“19th century portrait data\n",
    "8ï¸âƒ£ Victoria and Albert Museum (V&A) ğŸ‡¬ğŸ‡§ UK Open API\n",
    "Extensive images of fashion and body posture\n",
    "9ï¸âƒ£ Museu Nacional dâ€™Art de Catalunya (MNAC) ğŸ‡ªğŸ‡¸ Spain CC BY API\n",
    "Gothic/Baroque figure data\n",
    "0ï¸âƒ£ National Gallery of Australia (NGA-AUS) ğŸ‡¦ğŸ‡º Australia CC BY API / CSV\n",
    "Expanding non-Eurocentric perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f9dc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# main.py\n",
    "# Embodied Aesthetic Reconstruction â€” Streamlit App (Resonance-first UI)\n",
    "# --------------------------------------------------------------------\n",
    "# - CLIP + Pose + Color + Significance fusion\n",
    "# - Human-centered UI: semantic \"resonance\" labels instead of raw numeric scores\n",
    "# - Sidebar toggle \"Debug mode\" to show raw similarity when needed\n",
    "# - Significance score (0â€“100) + badges + \"Learn more\"\n",
    "# - Extra metadata fields: price_estimate_usd, significance_text, interpretive_note_cn\n",
    "# - iCloud placeholder avoidance, robust pathing, cached embeddings\n",
    "# - YOLOv8n-pose via HuggingFace (optional), proper device mapping for MPS/CUDA/CPU\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "from __future__ import annotations\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "import streamlit as st\n",
    "\n",
    "# -------------------- Torch / device --------------------\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_OK = True\n",
    "except Exception:\n",
    "    TORCH_OK = False\n",
    "\n",
    "def get_device() -> str:\n",
    "    \"\"\"Return 'mps' on Apple Silicon, 'cuda' on Nvidia, else 'cpu'.\"\"\"\n",
    "    if not TORCH_OK:\n",
    "        return \"cpu\"\n",
    "    try:\n",
    "        if torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"cpu\"\n",
    "\n",
    "DEVICE = get_device()\n",
    "if TORCH_OK:\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"medium\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def yolo_device() -> str:\n",
    "    \"\"\"Map torch device to Ultralytics' expected device string.\"\"\"\n",
    "    if DEVICE == \"mps\":\n",
    "        return \"mps\"\n",
    "    if DEVICE == \"cuda\":\n",
    "        return \"0\"\n",
    "    return \"cpu\"\n",
    "\n",
    "# -------------------- Paths --------------------\n",
    "APP_DIR = Path(__file__).parent.resolve()\n",
    "DATA_DIR = APP_DIR / \"data\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "INTERIM_DIR = DATA_DIR / \"interim\"\n",
    "CACHE_DIR = APP_DIR / \".clip_cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METADATA_CSV = DATA_DIR / \"portrait_works.csv\"\n",
    "\n",
    "# -------------------- UI config --------------------\n",
    "st.set_page_config(\n",
    "    page_title=\"Embodied Aesthetic Reconstruction\",\n",
    "    page_icon=\"ğŸ–¼ï¸\",\n",
    "    layout=\"wide\",\n",
    ")\n",
    "\n",
    "# -------------------- Utils --------------------\n",
    "def is_icloud_placeholder(p: Path) -> bool:\n",
    "    return p.suffix == \".icloud\" or p.name.endswith(\".icloud\")\n",
    "\n",
    "def load_image_safe(path: Path) -> Optional[Image.Image]:\n",
    "    if not path.exists() or is_icloud_placeholder(path):\n",
    "        return None\n",
    "    try:\n",
    "        with Image.open(path) as im:\n",
    "            return im.convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def pil_from_bytes(b: bytes) -> Image.Image:\n",
    "    return Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "\n",
    "def center_crop_long_edge(im: Image.Image, size: int = 512) -> Image.Image:\n",
    "    im = ImageOps.exif_transpose(im)\n",
    "    w, h = im.size\n",
    "    s = min(w, h)\n",
    "    left = (w - s) // 2\n",
    "    top = (h - s) // 2\n",
    "    im = im.crop((left, top, left + s, top + s))\n",
    "    return im.resize((size, size), Image.BICUBIC)\n",
    "\n",
    "# -------------------- Sidebar --------------------\n",
    "with st.sidebar:\n",
    "    st.header(\"âš™ï¸ Matching Weights\")\n",
    "    w_clip = st.slider(\"CLIP weight\", 0.0, 1.0, 1.00, 0.05)\n",
    "    w_pose = st.slider(\"Pose weight\", 0.0, 1.0, 0.30, 0.05)\n",
    "    w_color = st.slider(\"Color weight\", 0.0, 1.0, 0.20, 0.05)\n",
    "    w_sig  = st.slider(\"Significance weight\", 0.0, 1.0, 0.20, 0.05)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"ğŸ” Filters\")\n",
    "    require_public = st.checkbox(\"Require Public-Domain license\", value=False)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"ğŸ§ª Debug\")\n",
    "    show_debug = st.checkbox(\"Show raw similarity score\", value=False)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"ğŸ“Š Results\")\n",
    "    top_k = st.slider(\"Top-K artworks\", 1, 20, 6, 1)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"ğŸ’» Active device\")\n",
    "    st.success(f\"{DEVICE}\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.header(\"ğŸ§ Overlay\")\n",
    "    overlay_skeleton = st.checkbox(\"Draw skeleton on preview (if available)\", value=True)\n",
    "\n",
    "# -------------------- Metadata & dataset --------------------\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expected (optional) columns:\n",
    "    filename,title,artist,year,license,museum,accession,movement,\n",
    "    is_masterwork,citations,exhibitions,auction_price_usd,views_per_year,\n",
    "    price_estimate_usd,significance_text,interpretive_note_cn,notable_tags,source_links\n",
    "    \"\"\"\n",
    "    if METADATA_CSV.exists():\n",
    "        try:\n",
    "            return pd.read_csv(METADATA_CSV)\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Could not read metadata CSV ({METADATA_CSV.name}): {e}\")\n",
    "    # Fallback minimal schema\n",
    "    return pd.DataFrame({\n",
    "        \"filename\": [], \"title\": [], \"artist\": [], \"year\": [], \"license\": [],\n",
    "        \"museum\": [], \"accession\": [], \"movement\": [], \"is_masterwork\": [],\n",
    "        \"citations\": [], \"exhibitions\": [], \"auction_price_usd\": [], \"views_per_year\": [],\n",
    "        \"price_estimate_usd\": [], \"significance_text\": [], \"interpretive_note_cn\": [],\n",
    "        \"notable_tags\": [], \"source_links\": []\n",
    "    })\n",
    "\n",
    "META = load_metadata()\n",
    "\n",
    "def list_dataset_images() -> List[Path]:\n",
    "    if not IMAGES_DIR.exists():\n",
    "        return []\n",
    "    files: List[Path] = []\n",
    "    for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"):\n",
    "        files += list(IMAGES_DIR.glob(ext))\n",
    "    files = [p for p in files if not is_icloud_placeholder(p)]\n",
    "    return sorted(files)\n",
    "\n",
    "DATASET_FILES = list_dataset_images()\n",
    "\n",
    "# -------------------- Significance (0..100) --------------------\n",
    "TOP_MUSEUMS = [\"met\", \"national gallery\", \"tate\", \"louvre\", \"uffizi\", \"hermitage\", \"ng london\"]\n",
    "\n",
    "def _logn(x, d=1.0):\n",
    "    try:\n",
    "        return math.log10(max(float(x), 1.0)) / d\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def compute_significance_row(row) -> float:\n",
    "    \"\"\"Heuristic significance score (0..100).\"\"\"\n",
    "    w = dict(master=0.35, museum=0.20, cites=0.15, exhib=0.10, auction=0.10, views=0.10)\n",
    "    score = 0.0\n",
    "    if str(row.get(\"is_masterwork\", \"0\")).lower() in [\"1\", \"true\", \"yes\", \"y\"]:\n",
    "        score += w[\"master\"]\n",
    "    museum = str(row.get(\"museum\", \"\")).lower()\n",
    "    if museum:\n",
    "        score += w[\"museum\"] * (1.0 if any(m in museum for m in TOP_MUSEUMS) else 0.5)\n",
    "    score += w[\"cites\"]   * _logn(row.get(\"citations\", 0),        d=3.0)\n",
    "    score += w[\"exhib\"]   * _logn(row.get(\"exhibitions\", 0),      d=2.0)\n",
    "    score += w[\"auction\"] * _logn(row.get(\"auction_price_usd\",0), d=8.0)\n",
    "    score += w[\"views\"]   * _logn(row.get(\"views_per_year\", 0),   d=6.0)\n",
    "    return float(np.clip(score, 0.0, 1.0) * 100.0)\n",
    "\n",
    "def significance_badges(row) -> List[str]:\n",
    "    badges = []\n",
    "    if str(row.get(\"is_masterwork\",\"0\")).lower() in [\"1\",\"true\",\"yes\",\"y\"]:\n",
    "        badges.append(\"Masterwork\")\n",
    "    if str(row.get(\"museum\",\"\")).strip():\n",
    "        badges.append(\"Permanent Collection\")\n",
    "    try:\n",
    "        if int(row.get(\"citations\", 0)) >= 50:\n",
    "            badges.append(\"Canon\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return badges[:3]\n",
    "\n",
    "def filename_key(path_or_name: str) -> str:\n",
    "    return Path(path_or_name).name\n",
    "\n",
    "META_BY_NAME = {}\n",
    "if not META.empty and \"filename\" in META.columns:\n",
    "    for _, r in META.iterrows():\n",
    "        META_BY_NAME[filename_key(str(r.get(\"filename\",\"\")))] = r\n",
    "\n",
    "# -------------------- OpenCLIP --------------------\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_openclip():\n",
    "    try:\n",
    "        import open_clip\n",
    "    except Exception as e:\n",
    "        st.error(f\"open_clip_torch not installed: {e}\")\n",
    "        return None, None, None\n",
    "    try:\n",
    "        model_name, pretrained = \"ViT-B-32\", \"laion2b_s34b_b79k\"\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "            model_name, pretrained=pretrained, device=DEVICE\n",
    "        )\n",
    "        tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        model.eval()\n",
    "        return model, preprocess, tokenizer\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to load OpenCLIP: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "MODEL_CLIP, PRE_CLIP, TOKENIZER = load_openclip()\n",
    "\n",
    "def tensor_from_pil_clip(im: Image.Image):\n",
    "    if PRE_CLIP is None:\n",
    "        return None\n",
    "    t = PRE_CLIP(im).unsqueeze(0)\n",
    "    if TORCH_OK:\n",
    "        t = t.to(DEVICE)\n",
    "    return t\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def embed_image_clip(img_bytes: bytes) -> Optional[np.ndarray]:\n",
    "    if MODEL_CLIP is None:\n",
    "        return None\n",
    "    try:\n",
    "        im = pil_from_bytes(img_bytes)\n",
    "        im = center_crop_long_edge(im, size=224)\n",
    "        x = tensor_from_pil_clip(im)\n",
    "        with torch.no_grad():\n",
    "            feat = MODEL_CLIP.encode_image(x)\n",
    "            feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "        return feat.cpu().numpy().astype(\"float32\")[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------- YOLO weights via HuggingFace (optional) --------------------\n",
    "def get_yolo_pose_weights_path() -> Optional[str]:\n",
    "    try:\n",
    "        from huggingface_hub import hf_hub_download\n",
    "    except Exception:\n",
    "        return \"yolov8n-pose.pt\"\n",
    "    try:\n",
    "        local = hf_hub_download(\n",
    "            repo_id=\"ultralytics/yolov8n-pose\",\n",
    "            filename=\"yolov8n-pose.pt\",\n",
    "            local_dir=str(Path.home() / \".cache\" / \"hf\"),\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        return local\n",
    "    except Exception:\n",
    "        return \"yolov8n-pose.pt\"\n",
    "\n",
    "# -------------------- Feature extraction & cache --------------------\n",
    "EMB_PATH   = CACHE_DIR / \"embeddings.npy\"\n",
    "IDS_PATH   = CACHE_DIR / \"ids.json\"\n",
    "POSE_PATH  = CACHE_DIR / \"pose.npy\"\n",
    "COLOR_PATH = CACHE_DIR / \"color.npy\"\n",
    "SIG_PATH   = CACHE_DIR / \"significance.npy\"\n",
    "\n",
    "def _color_feature(im: Image.Image) -> np.ndarray:\n",
    "    im = im.resize((256, 256), Image.BILINEAR)\n",
    "    arr = np.array(im.convert(\"HSV\"))\n",
    "    h, s, v = arr[..., 0], arr[..., 1], arr[..., 2]\n",
    "    hist_h, _ = np.histogram(h, bins=32, range=(0, 255), density=True)\n",
    "    hist_s, _ = np.histogram(s, bins=16, range=(0, 255), density=True)\n",
    "    hist_v, _ = np.histogram(v, bins=8,  range=(0, 255), density=True)\n",
    "    feat = np.concatenate([hist_h, hist_s, hist_v]).astype(\"float32\")\n",
    "    return feat / (np.linalg.norm(feat) + 1e-8)\n",
    "\n",
    "def _pose_feature_from_kpts(kpts_xyv: np.ndarray) -> np.ndarray:\n",
    "    if kpts_xyv is None or len(kpts_xyv) == 0:\n",
    "        return np.zeros(34, dtype=\"float32\")\n",
    "    xy = kpts_xyv[:, :2]\n",
    "    min_xy = xy.min(0); max_xy = xy.max(0)\n",
    "    wh = np.maximum(max_xy - min_xy, 1e-6)\n",
    "    xy_n = (xy - min_xy) / wh\n",
    "    feat = xy_n.reshape(-1).astype(\"float32\")\n",
    "    if feat.shape[0] < 34:\n",
    "        feat = np.pad(feat, (0, 34 - feat.shape[0]))\n",
    "    else:\n",
    "        feat = feat[:34]\n",
    "    return feat / (np.linalg.norm(feat) + 1e-8)\n",
    "\n",
    "def _detect_pose_pil(im: Image.Image) -> Optional[np.ndarray]:\n",
    "    try:\n",
    "        from ultralytics import YOLO\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        weights = get_yolo_pose_weights_path()\n",
    "        model = YOLO(weights)\n",
    "        arr = np.array(im.convert(\"RGB\"))\n",
    "        res = model.predict(\n",
    "            source=arr, imgsz=512, conf=0.25, verbose=False, device=yolo_device()\n",
    "        )\n",
    "        if not res or len(res[0].keypoints) == 0:\n",
    "            return None\n",
    "        kpts = res[0].keypoints.xy.cpu().numpy()\n",
    "        if kpts.ndim == 4: kpts = kpts[0]\n",
    "        if kpts.ndim == 3: k = kpts[0]\n",
    "        else: return None\n",
    "        v = np.ones((k.shape[0], 1), dtype=\"float32\")\n",
    "        kxyv = np.concatenate([k.astype(\"float32\"), v], axis=1)\n",
    "        return _pose_feature_from_kpts(kxyv)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _dataset_pose_feature(path: Path) -> Optional[np.ndarray]:\n",
    "    im = load_image_safe(path)\n",
    "    if im is None:\n",
    "        return None\n",
    "    im_small = center_crop_long_edge(im, 512)\n",
    "    return _detect_pose_pil(im_small)\n",
    "\n",
    "@st.cache_data(show_spinner=True, persist=True)\n",
    "def build_dataset_embeddings(files: List[Path]):\n",
    "    \"\"\"Return (clip_embeds, ids, pose_feats, color_feats, signif_norm).\"\"\"\n",
    "    ids = [str(p.relative_to(APP_DIR)) for p in files]\n",
    "\n",
    "    # Use cache if aligned\n",
    "    if EMB_PATH.exists() and IDS_PATH.exists() and SIG_PATH.exists():\n",
    "        try:\n",
    "            cached_ids = json.loads(IDS_PATH.read_text())\n",
    "            if cached_ids == ids:\n",
    "                clip_arr  = np.load(EMB_PATH)\n",
    "                pose_arr  = np.load(POSE_PATH)  if POSE_PATH.exists()  else None\n",
    "                color_arr = np.load(COLOR_PATH) if COLOR_PATH.exists() else None\n",
    "                sig_arr   = np.load(SIG_PATH)\n",
    "                return clip_arr, ids, pose_arr, color_arr, sig_arr\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    clip_feats, pose_feats, color_feats, signif_vals = [], [], [], []\n",
    "\n",
    "    if MODEL_CLIP is None:\n",
    "        st.error(\"OpenCLIP not ready; cannot build embeddings.\")\n",
    "        return np.zeros((0, 512), dtype=\"float32\"), ids, None, None, np.zeros((0,), dtype=\"float32\")\n",
    "\n",
    "    progress = st.progress(0.0, text=\"Building dataset embeddings/featuresâ€¦\")\n",
    "    for i, p in enumerate(files):\n",
    "        progress.progress((i + 1) / max(1, len(files)))\n",
    "        im = load_image_safe(p)\n",
    "\n",
    "        # CLIP\n",
    "        if im is not None:\n",
    "            try:\n",
    "                im_clip = center_crop_long_edge(im, 224)\n",
    "                x = tensor_from_pil_clip(im_clip)\n",
    "                with torch.no_grad():\n",
    "                    z = MODEL_CLIP.encode_image(x)\n",
    "                    z = z / z.norm(dim=-1, keepdim=True)\n",
    "                clip_feats.append(z.cpu().numpy().astype(\"float32\")[0])\n",
    "            except Exception:\n",
    "                clip_feats.append(np.zeros(512, dtype=\"float32\"))\n",
    "        else:\n",
    "            clip_feats.append(np.zeros(512, dtype=\"float32\"))\n",
    "\n",
    "        # Color\n",
    "        if im is not None:\n",
    "            try:\n",
    "                color_feats.append(_color_feature(im))\n",
    "            except Exception:\n",
    "                color_feats.append(np.zeros(56, dtype=\"float32\"))\n",
    "        else:\n",
    "            color_feats.append(np.zeros(56, dtype=\"float32\"))\n",
    "\n",
    "        # Pose\n",
    "        pf = _dataset_pose_feature(p)\n",
    "        pose_feats.append(pf if pf is not None else np.zeros(34, dtype=\"float32\"))\n",
    "\n",
    "        # Significance prior from metadata (0..1)\n",
    "        fname = filename_key(p.name)\n",
    "        row = META_BY_NAME.get(fname)\n",
    "        sig = compute_significance_row(row) / 100.0 if row is not None else 0.0\n",
    "        signif_vals.append(float(sig))\n",
    "\n",
    "    clip_arr  = np.vstack(clip_feats).astype(\"float32\") if clip_feats else np.zeros((0, 512), dtype=\"float32\")\n",
    "    pose_arr  = np.vstack(pose_feats).astype(\"float32\") if pose_feats else None\n",
    "    color_arr = np.vstack(color_feats).astype(\"float32\") if color_feats else None\n",
    "    sig_arr   = np.array(signif_vals, dtype=\"float32\")\n",
    "\n",
    "    # Cache\n",
    "    try:\n",
    "        np.save(EMB_PATH, clip_arr)\n",
    "        np.save(POSE_PATH, pose_arr)\n",
    "        np.save(COLOR_PATH, color_arr)\n",
    "        np.save(SIG_PATH,  sig_arr)\n",
    "        IDS_PATH.write_text(json.dumps(ids))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return clip_arr, ids, pose_arr, color_arr, sig_arr\n",
    "\n",
    "# ---- Build dataset features ----\n",
    "if len(DATASET_FILES) == 0:\n",
    "    st.error(f\"âŒ No images found or folder is empty: {IMAGES_DIR}\\nMake sure iCloud files are downloaded locally (no .icloud).\")\n",
    "else:\n",
    "    st.success(f\"âœ… Dataset images: {len(DATASET_FILES)}\")\n",
    "\n",
    "CLIP_DS, IDS, POSE_DS, COLOR_DS, SIG_DS = build_dataset_embeddings(DATASET_FILES)  # SIG_DS in [0,1]\n",
    "\n",
    "# ---- Ready checks ----\n",
    "READY = True\n",
    "reasons = []\n",
    "if MODEL_CLIP is None:\n",
    "    READY = False; reasons.append(\"OpenCLIP not loaded.\")\n",
    "if len(DATASET_FILES) == 0:\n",
    "    READY = False; reasons.append(\"No images in data/images (or still .icloud).\")\n",
    "if CLIP_DS is None or getattr(CLIP_DS, \"shape\", (0,))[0] == 0:\n",
    "    READY = False; reasons.append(\"Dataset embeddings not built yet.\")\n",
    "if not READY:\n",
    "    st.warning(\"Matching is not ready: \" + \" \".join(reasons))\n",
    "else:\n",
    "    st.success(\"Matching is ready âœ”\")\n",
    "\n",
    "# -------------------- Similarity + Resonance --------------------\n",
    "def cos_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    if a is None or b is None:\n",
    "        return None\n",
    "    if a.ndim == 1:\n",
    "        a = a[None, :]\n",
    "    a = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-8)\n",
    "    b = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-8)\n",
    "    return (a @ b.T)\n",
    "\n",
    "def interpret_score(score: float) -> str:\n",
    "    \"\"\"Map numeric similarity to a poetic/semantic label.\"\"\"\n",
    "    if score > 0.80: return \"Strong resonance ğŸ’«\"\n",
    "    if score > 0.65: return \"Aesthetic kinship âœ¨\"\n",
    "    if score > 0.50: return \"Subtle correspondence ğŸŒ™\"\n",
    "    return \"Distant echo ğŸŒ«ï¸\"\n",
    "\n",
    "# -------------------- Pose overlay (preview only) --------------------\n",
    "def draw_skeleton_overlay(im: Image.Image) -> Tuple[Image.Image, Optional[np.ndarray]]:\n",
    "    try:\n",
    "        from ultralytics import YOLO\n",
    "        import cv2\n",
    "    except Exception:\n",
    "        return im, None\n",
    "\n",
    "    weights = get_yolo_pose_weights_path()\n",
    "    model = YOLO(weights)\n",
    "    arr = np.array(im.convert(\"RGB\"))\n",
    "    res = model.predict(source=arr, imgsz=512, conf=0.25, verbose=False, device=yolo_device())\n",
    "    if not res or len(res[0].keypoints) == 0:\n",
    "        return im, None\n",
    "\n",
    "    canvas = arr.copy()\n",
    "    pts = res[0].keypoints.xy.cpu().numpy()\n",
    "    if pts.ndim == 4: pts = pts[0]\n",
    "    if pts.ndim == 3: pts = pts[0]\n",
    "\n",
    "    PAIRS = [(5,7),(7,9),(6,8),(8,10),(11,13),(13,15),(12,14),(14,16),(5,6),(11,12),(5,11),(6,12)]\n",
    "    try:\n",
    "        import cv2\n",
    "        for (a,b) in PAIRS:\n",
    "            xa,ya = pts[a]; xb,yb = pts[b]\n",
    "            cv2.line(canvas,(int(xa),int(ya)),(int(xb),int(yb)),(0,255,0),3)\n",
    "        for (x,y) in pts:\n",
    "            cv2.circle(canvas,(int(x),int(y)),4,(0,0,255),-1)\n",
    "        im_draw = Image.fromarray(canvas)\n",
    "    except Exception:\n",
    "        im_draw = im\n",
    "\n",
    "    v = np.ones((pts.shape[0],1),dtype=\"float32\")\n",
    "    kxyv = np.concatenate([pts.astype(\"float32\"), v], axis=1)\n",
    "    pf = _pose_feature_from_kpts(kxyv)\n",
    "    return im_draw, pf\n",
    "\n",
    "# -------------------- UI â€” Main --------------------\n",
    "st.title(\"Embodied Aesthetic Reconstruction\")\n",
    "st.caption(\"Camera / Upload â†’ CLIP + Pose + Color + Significance â†’ Resonant artworks\")\n",
    "\n",
    "status_cols = st.columns(3)\n",
    "with status_cols[0]:\n",
    "    st.success(f\"Images dir: {IMAGES_DIR.relative_to(APP_DIR)}\" if IMAGES_DIR.exists() else f\"Missing: {IMAGES_DIR}\")\n",
    "with status_cols[1]:\n",
    "    st.info(f\"Dataset files: {len(DATASET_FILES)}\")\n",
    "with status_cols[2]:\n",
    "    st.info(f\"Models: OpenCLIP={'âœ”ï¸' if MODEL_CLIP else 'âœ–ï¸'} Â· Pose(ultralytics)={'âœ”ï¸'}\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "\n",
    "left, right = st.columns([1, 1])\n",
    "query_img: Optional[Image.Image] = None\n",
    "query_pose_feat: Optional[np.ndarray] = None\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"ğŸ“· Camera (auto center-crop)\")\n",
    "    cam = st.camera_input(\"Take a photo (allow permission first)\", key=\"camera\")\n",
    "    if cam is not None:\n",
    "        try:\n",
    "            img = pil_from_bytes(cam.getvalue())\n",
    "            img = center_crop_long_edge(img, 640)\n",
    "            out_path = INTERIM_DIR / \"locked_frame.jpg\"\n",
    "            img.save(out_path, quality=92)\n",
    "            st.success(f\"Saved: {out_path.as_posix()}\")\n",
    "            if overlay_skeleton:\n",
    "                img_draw, q_pose = draw_skeleton_overlay(img)\n",
    "                st.image(img_draw, caption=\"Camera preview\", use_container_width=True)\n",
    "                query_img = img_draw\n",
    "                query_pose_feat = q_pose\n",
    "            else:\n",
    "                st.image(img, caption=\"Camera preview\", use_container_width=True)\n",
    "                query_img = img\n",
    "        except Exception as e:\n",
    "            st.error(f\"Camera decode failed: {e}\")\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"ğŸ–¼ï¸ Or upload an image\")\n",
    "    up = st.file_uploader(\"JPG/PNG\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "    if up is not None:\n",
    "        try:\n",
    "            img = pil_from_bytes(up.getvalue())\n",
    "            img = center_crop_long_edge(img, 640)\n",
    "            if overlay_skeleton:\n",
    "                img_draw, q_pose = draw_skeleton_overlay(img)\n",
    "                st.image(img_draw, caption=\"Uploaded preview\", use_container_width=True)\n",
    "                query_img = img_draw\n",
    "                query_pose_feat = q_pose\n",
    "            else:\n",
    "                st.image(img, caption=\"Uploaded preview\", use_container_width=True)\n",
    "                query_img = img\n",
    "        except Exception as e:\n",
    "            st.error(f\"Upload decode failed: {e}\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "run = st.button(\"ğŸ” Run Matching\", disabled=not READY)\n",
    "\n",
    "# -------------------- Display helpers --------------------\n",
    "def display_results(idx_scores: List[Tuple[int, float]], k: int):\n",
    "    if len(idx_scores) == 0:\n",
    "        st.warning(\"No results to display.\")\n",
    "        return\n",
    "    cols = st.columns(min(3, k))\n",
    "    for i, (idx, sc) in enumerate(idx_scores[:k]):\n",
    "        fn_rel = IDS[idx]\n",
    "        p = (APP_DIR / fn_rel).resolve()\n",
    "        im = load_image_safe(p)\n",
    "\n",
    "        row = META_BY_NAME.get(filename_key(p.name))\n",
    "        if row is not None:\n",
    "            title  = str(row.get(\"title\", \"\")).strip()\n",
    "            artist = str(row.get(\"artist\", \"\")).strip()\n",
    "            year   = str(row.get(\"year\", \"\")).strip()\n",
    "            museum = str(row.get(\"museum\", \"\")).strip()\n",
    "            price  = str(row.get(\"price_estimate_usd\", \"\")).strip()\n",
    "            sigtxt = str(row.get(\"significance_text\", \"\")).strip()\n",
    "            sigcn  = str(row.get(\"interpretive_note_cn\", \"\")).strip()\n",
    "            links  = str(row.get(\"source_links\", \"\")).strip()\n",
    "            sig100 = compute_significance_row(row)\n",
    "            badges = significance_badges(row)\n",
    "        else:\n",
    "            title=artist=year=museum=price=sigtxt=sigcn=links=\"\"\n",
    "            sig100, badges = 0.0, []\n",
    "\n",
    "        with cols[i % len(cols)]:\n",
    "            if im is not None:\n",
    "                st.image(im, use_container_width=True)\n",
    "\n",
    "            # Resonance-first display\n",
    "            if show_debug:\n",
    "                st.markdown(f\"**Score:** {sc:.3f}\")\n",
    "            else:\n",
    "                st.markdown(f\"**{interpret_score(sc)}**\")\n",
    "\n",
    "            if title or artist or year:\n",
    "                st.caption(f\"{title} â€” {artist} ({year})\")\n",
    "            if museum:\n",
    "                st.caption(f\"ğŸ›ï¸ {museum}\")\n",
    "            if price:\n",
    "                st.caption(f\"ğŸ’° Estimated value: ${price}\")\n",
    "\n",
    "            st.markdown(f\"**Significance:** {sig100:.0f}/100\")\n",
    "            if badges:\n",
    "                st.caption(\" Â· \".join(badges))\n",
    "            if sigtxt:\n",
    "                st.markdown(f\"_{sigtxt}_\")\n",
    "            if sigcn:\n",
    "                st.markdown(f\"**Curatorial note (CN):** {sigcn}\")\n",
    "            if links:\n",
    "                first = links.split(\";\")[0].strip()\n",
    "                if first:\n",
    "                    st.write(f\"[Learn more]({first})\")\n",
    "\n",
    "# -------------------- Matching --------------------\n",
    "if run:\n",
    "    if query_img is None:\n",
    "        st.warning(\"Please take a photo or upload an image first.\")\n",
    "    else:\n",
    "        st.write(\"Computing embeddingsâ€¦\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        q_clip = None\n",
    "        if MODEL_CLIP is not None:\n",
    "            try:\n",
    "                buf = io.BytesIO()\n",
    "                query_img.save(buf, format=\"JPEG\", quality=92)\n",
    "                q_clip = embed_image_clip(buf.getvalue())\n",
    "            except Exception:\n",
    "                q_clip = None\n",
    "\n",
    "        try:\n",
    "            q_color = _color_feature(query_img)\n",
    "        except Exception:\n",
    "            q_color = None\n",
    "\n",
    "        if query_pose_feat is None and w_pose > 0:\n",
    "            query_pose_feat = _detect_pose_pil(center_crop_long_edge(query_img, 512))\n",
    "        w_pose_eff = w_pose if query_pose_feat is not None and POSE_DS is not None else 0.0\n",
    "        if w_pose > 0 and w_pose_eff == 0.0:\n",
    "            st.info(\"No pose feature detected or dataset pose not precomputed; treating Pose weight as 0.\")\n",
    "\n",
    "        valid_indices = np.arange(len(IDS))\n",
    "        if require_public and not META.empty and {\"filename\", \"license\"}.issubset(META.columns):\n",
    "            pd_mask = META[\"license\"].astype(str).str.contains(\"Public Domain\", case=False, na=False)\n",
    "            pool_names = set(META.loc[pd_mask, \"filename\"].astype(str).apply(lambda s: Path(s).name))\n",
    "            filt = [i for i, fn in enumerate(IDS) if Path(fn).name in pool_names]\n",
    "            if len(filt) > 0:\n",
    "                valid_indices = np.array(filt, dtype=int)\n",
    "            else:\n",
    "                st.warning(\"No entries meet Public-Domain filter; ignoring filter.\")\n",
    "\n",
    "        def slicer(arr):\n",
    "            if arr is None or arr.shape[0] != len(IDS):\n",
    "                return None\n",
    "            return arr[valid_indices]\n",
    "\n",
    "        CLIP_POOL  = slicer(CLIP_DS)\n",
    "        POSE_POOL  = slicer(POSE_DS)\n",
    "        COLOR_POOL = slicer(COLOR_DS)\n",
    "        SIG_POOL   = SIG_DS[valid_indices] if SIG_DS is not None and SIG_DS.shape[0] == len(IDS) else None\n",
    "\n",
    "        def cos_sim_local(q, bank):\n",
    "            if q is None or bank is None:\n",
    "                return None\n",
    "            a = q[None, :] if q.ndim == 1 else q\n",
    "            a = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-8)\n",
    "            b = bank / (np.linalg.norm(bank, axis=1, keepdims=True) + 1e-8)\n",
    "            return (a @ b.T)[0]\n",
    "\n",
    "        score = np.zeros(valid_indices.size, dtype=\"float32\")\n",
    "        s = cos_sim_local(q_clip, CLIP_POOL)\n",
    "        if s is not None: score += w_clip * s\n",
    "        s = cos_sim_local(query_pose_feat, POSE_POOL) if w_pose_eff > 0 else None\n",
    "        if s is not None: score += w_pose_eff * s\n",
    "        s = cos_sim_local(q_color, COLOR_POOL)\n",
    "        if s is not None: score += w_color * s\n",
    "        if SIG_POOL is not None and w_sig > 0.0:\n",
    "            score += w_sig * SIG_POOL\n",
    "\n",
    "        topk_local = np.argsort(-score)[:min(top_k, score.size)]\n",
    "        results = [(int(valid_indices[i]), float(score[i])) for i in topk_local]\n",
    "\n",
    "        dt = (time.time() - t0) * 1000\n",
    "        st.info(f\"Search time: {dt:.1f} ms\")\n",
    "\n",
    "        display_results(results, k=top_k)\n",
    "\n",
    "# -------------------- Tips --------------------\n",
    "with st.expander(\"Tips\"):\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "- If you see **iCloud .icloud** placeholders, open the folder in Finder and choose **â€œDownload Nowâ€**.\n",
    "- Adjust **CLIP / Pose / Color / Significance** weights to influence ranking.\n",
    "- **Debug mode** reveals raw similarity scores; keep it off for exhibitions to maintain a poetic tone.\n",
    "- The **Public-Domain** filter works only if `license` exists in your metadata CSV.\n",
    "- First run builds and caches dataset features in `.clip_cache/`.\n",
    "- Pose detection uses `ultralytics` (YOLOv8n-pose). Weights are pulled from Hugging Face when possible.\n",
    "- *Significance* is a heuristic combining museum presence, citations, and attention. It reflects historical canons yet encourages plural aesthetics.\n",
    "\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db849671",
   "metadata": {},
   "source": [
    "#frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad84f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_webrtc import (\n",
    "    WebRtcMode,\n",
    "    RTCConfiguration,\n",
    "    VideoProcessorBase,\n",
    "    webrtc_streamer,\n",
    ")\n",
    "\n",
    "# =================== è·¯å¾„ä¸å¸¸é‡ ===================\n",
    "\n",
    "FRONTEND_DIR = Path(__file__).resolve().parent\n",
    "ROOT_DIR = FRONTEND_DIR.parent\n",
    "\n",
    "# æœ¬åœ°æ•°æ®ï¼ˆç”¨äºæ ¹æ® filename æ‰¾ metaï¼‰\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"local\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "# å¯èƒ½å­˜åœ¨çš„ meta CSV\n",
    "META_CSV_CANDIDATES = [\n",
    "    DATA_DIR / \"portrait_works_enhanced_english.csv\",\n",
    "    DATA_DIR / \"portrait_works.csv\",\n",
    "]\n",
    "\n",
    "# åç«¯ APIï¼ˆéœ€è¦æ—¶ä½ å¯ä»¥æ”¹æˆ HuggingFace åœ°å€ï¼‰\n",
    "DEFAULT_API_URL = \"http://127.0.0.1:8000/match\"\n",
    "\n",
    "APP_TITLE = \"Embodied Aesthetic Reconstruction\"\n",
    "\n",
    "# YOLOv8-Pose æ¨¡å‹è·¯å¾„ï¼ˆæ”¾åœ¨ frontend ç›®å½•ä¸‹ï¼‰\n",
    "YOLO_MODEL_PATH = FRONTEND_DIR / \"yolov8n-pose.pt\"\n",
    "\n",
    "# WebRTC é…ç½®ï¼ˆSafari éœ€è¦ STUNï¼‰\n",
    "RTC_CONFIGURATION = RTCConfiguration(\n",
    "    {\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]}\n",
    ")\n",
    "\n",
    "# Stillness æ£€æµ‹ï¼ˆä¸ç­–å±•ç‰ˆä¸€è‡´ï¼‰\n",
    "STILLNESS_SEC = 3.5\n",
    "MAX_BUF_SEC = 5.0\n",
    "FPS_ASSUMED = 12\n",
    "MOTION_EPS_CXCY = 4.0\n",
    "MOTION_EPS_AREA = 0.03\n",
    "MIN_FACE_AREA = 0.06\n",
    "\n",
    "# é¢œè‰²\n",
    "YELLOW = (255, 235, 59)\n",
    "BLACK = (0, 0, 0)\n",
    "HOT_PINK = (255, 30, 180)\n",
    "\n",
    "# å±•ç¤ºåŒºåŸŸå®½åº¦\n",
    "RIGHT_IMG_MAXW = 900\n",
    "\n",
    "\n",
    "# =================== å°å·¥å…·å‡½æ•° ===================\n",
    "\n",
    "_META_CACHE: Optional[Dict[str, Dict]] = None\n",
    "\n",
    "\n",
    "def load_meta_mapping() -> Dict[str, Dict]:\n",
    "    \"\"\"åŠ è½½æœ¬åœ° CSVï¼ŒæŒ‰ filename å»ºç«‹ç´¢å¼•ã€‚\"\"\"\n",
    "    global _META_CACHE\n",
    "    if _META_CACHE is not None:\n",
    "        return _META_CACHE\n",
    "\n",
    "    import csv\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    for p in META_CSV_CANDIDATES:\n",
    "        if p.exists():\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                rows = list(csv.DictReader(f))\n",
    "            break\n",
    "\n",
    "    mapping: Dict[str, Dict] = {}\n",
    "    for r in rows:\n",
    "        fname = (\n",
    "            r.get(\"filename\")\n",
    "            or r.get(\"image_path\")\n",
    "            or r.get(\"path\")\n",
    "            or r.get(\"file\")\n",
    "        )\n",
    "        if fname:\n",
    "            mapping[str(fname)] = r\n",
    "\n",
    "    _META_CACHE = mapping\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def lookup_meta(filename: str) -> Dict:\n",
    "    mapping = load_meta_mapping()\n",
    "    return mapping.get(filename, {})\n",
    "\n",
    "\n",
    "def safe_open_image(p: Path) -> Optional[Image.Image]:\n",
    "    try:\n",
    "        return Image.open(p).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_image_path(filename: str) -> Optional[Path]:\n",
    "    \"\"\"æ ¹æ® filename åœ¨ IMAGES_DIR é‡Œæ‰¾åˆ°å›¾ç‰‡ã€‚\"\"\"\n",
    "    if not filename:\n",
    "        return None\n",
    "    p = Path(filename)\n",
    "    if not p.is_absolute():\n",
    "        p = IMAGES_DIR / p\n",
    "    return p if p.exists() else None\n",
    "\n",
    "\n",
    "def _load_font(size: int = 40) -> ImageFont.FreeTypeFont:\n",
    "    \"\"\"ä¼˜å…ˆç”¨ Courier / Courier Newï¼Œæ‰¾ä¸åˆ°å†é€€å› Arial / é»˜è®¤ã€‚\"\"\"\n",
    "    candidates = [\n",
    "        \"/Library/Fonts/Courier New.ttf\",\n",
    "        \"/System/Library/Fonts/Courier.dfont\",\n",
    "        \"/System/Library/Fonts/Supplemental/Courier New.ttf\",\n",
    "        \"/Library/Fonts/Arial.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                return ImageFont.truetype(p, size=size)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def draw_tiny_metrics_top_right(\n",
    "    im: Image.Image, lines: List[str], size: int = 16, margin: int = 10\n",
    ") -> Image.Image:\n",
    "    \"\"\"å³ä¸Šè§’ç²‰è‰²æ–‡å­—ï¼šå§¿æ€æŒ‡æ ‡ï¼ˆä¸ç­–å±•ç‰ˆä¸€è‡´ï¼‰ã€‚\"\"\"\n",
    "    if not lines:\n",
    "        return im\n",
    "    img = im.copy()\n",
    "    d = ImageDraw.Draw(img)\n",
    "    font = _load_font(size)\n",
    "\n",
    "    widths = []\n",
    "    for s in lines:\n",
    "        l, t, r, b = d.textbbox((0, 0), s, font=font)\n",
    "        widths.append(r - l)\n",
    "    wmax = max(widths) if widths else 0\n",
    "\n",
    "    x = img.width - margin - wmax\n",
    "    y = margin\n",
    "    for s in lines:\n",
    "        d.text((x, y), s, fill=HOT_PINK, font=font)\n",
    "        _, _, _, b = d.textbbox((0, 0), s, font=font)\n",
    "        y += int(b * 0.95)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _angle_deg(p1, p2):\n",
    "    if p1 is None or p2 is None:\n",
    "        return None\n",
    "    vx, vy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "    return float(np.degrees(np.arctan2(vy, vx)))\n",
    "\n",
    "\n",
    "def _elbow_angle(shoulder, elbow, wrist):\n",
    "    if None in (shoulder, elbow, wrist):\n",
    "        return None\n",
    "    v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]], float)\n",
    "    v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]], float)\n",
    "    n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if n1 < 1e-5 or n2 < 1e-5:\n",
    "        return None\n",
    "    cosv = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cosv)))\n",
    "\n",
    "\n",
    "def format_metrics(kps: Dict[int, Tuple[float, float] | None]) -> List[str]:\n",
    "    \"\"\"æŠŠå…³é”®ç‚¹å˜æˆå‡ è¡Œå°å­—ï¼ˆä¸ç­–å±•ç‰ˆåŒæ ·ç»“æ„ï¼‰ã€‚\"\"\"\n",
    "    le, re = kps.get(1), kps.get(2)\n",
    "    lsh, rsh = kps.get(5), kps.get(6)\n",
    "    lel, rel = kps.get(7), kps.get(8)\n",
    "    lwr, rwr = kps.get(9), kps.get(10)\n",
    "\n",
    "    fdeg = lambda v: \"â€”\" if v is None else f\"{v:+.1f}Â°\"\n",
    "    fpt = lambda p: \"(0, 0)\" if p is None else f\"({int(p[0])}, {int(p[1])})\"\n",
    "\n",
    "    return [\n",
    "        f\"Head tilt: {fdeg(_angle_deg(re, le))}\",\n",
    "        f\"Shoulder:  {fdeg(_angle_deg(rsh, lsh))}\",\n",
    "        f\"L elbow:   {fdeg(_elbow_angle(lsh, lel, lwr))}\",\n",
    "        f\"R elbow:   {fdeg(_elbow_angle(rsh, rel, rwr))}\",\n",
    "        f\"L wrist:   {fpt(lwr)}\",\n",
    "        f\"R wrist:   {fpt(rwr)}\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def overlay_right_labels(painting: Image.Image, meta: Dict) -> Image.Image:\n",
    "    \"\"\"\n",
    "    åœ¨ç”»ä½œä¸Šå åŠ  3 ä¸ªé»„è‰²çŸ©å½¢æ ‡ç­¾ï¼š\n",
    "    1. ä»·æ ¼ï¼ˆprice_text / auction_price_usdï¼‰\n",
    "    2. å¹´ä»½ï¼ˆyearï¼‰\n",
    "    3. è‰ºæœ¯å®¶ï¼ˆartistï¼‰\n",
    "    å­—ä½“ï¼šCourierï¼Œé»‘å­—ï¼Œé»„è‰²èƒŒæ™¯ï¼Œé¡ºåºå›ºå®šã€‚\n",
    "    \"\"\"\n",
    "    im = painting.convert(\"RGB\").copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    font_big = _load_font(44)\n",
    "    font_small = _load_font(36)\n",
    "\n",
    "    price = (\n",
    "        meta.get(\"price_text\")\n",
    "        or meta.get(\"auction_price_usd\")\n",
    "        or meta.get(\"price\")\n",
    "        or \"â€”\"\n",
    "    )\n",
    "    year = str(meta.get(\"year\") or \"â€”\")\n",
    "    artist = meta.get(\"artist\") or \"artist name\"\n",
    "\n",
    "    lines = [price, year, artist]\n",
    "    fonts = [font_big, font_small, font_big]\n",
    "\n",
    "    margin_x = 24\n",
    "    # ä»ç”»é¢ä¸­éƒ¨ç•¥é ä¸Šå¼€å§‹\n",
    "    y = int(im.height * 0.50)\n",
    "\n",
    "    for text, font in zip(lines, fonts):\n",
    "        # ç”¨ textbboxï¼Œé¿å… textsize æŠ¥é”™\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font)\n",
    "        w, h = r - l, b - t\n",
    "\n",
    "        pad_x, pad_y = 16, 10\n",
    "        box_w, box_h = w + 2 * pad_x, h + 2 * pad_y\n",
    "\n",
    "        x = margin_x\n",
    "        draw.rectangle([x, y, x + box_w, y + box_h], fill=YELLOW)\n",
    "        draw.text((x + pad_x, y + pad_y), text, fill=BLACK, font=font)\n",
    "\n",
    "        y += box_h + 10\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def force_rerun():\n",
    "    try:\n",
    "        st.rerun()\n",
    "    except Exception:\n",
    "        try:\n",
    "            st.experimental_rerun()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =================== YOLO è§†é¢‘å¤„ç†ï¼ˆå®Œå…¨æ²¿ç”¨ç­–å±•éª¨æ¶é£æ ¼ï¼‰ ===================\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "    HAS_YOLO = True\n",
    "except Exception:\n",
    "    HAS_YOLO = False\n",
    "\n",
    "\n",
    "class CuratorialProcessor(VideoProcessorBase):\n",
    "    \"\"\"\n",
    "    ä¸ç­–å±•ç‰ˆä¸€è‡´ï¼š\n",
    "    - ä½¿ç”¨ YOLOv8-Pose ç»˜åˆ¶è“è‰²æ¡† + ç»¿è‰²éª¨æ¶\n",
    "    - å³ä¸Šè§’ç²‰è‰²æ–‡æœ¬æ˜¾ç¤ºå§¿æ€æŒ‡æ ‡\n",
    "    - æ”¯æŒè‡ªåŠ¨é™æ­¢æŠ“æ‹ & æ‰‹åŠ¨æŠ“æ‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        if HAS_YOLO:\n",
    "            try:\n",
    "                local = YOLO_MODEL_PATH\n",
    "                self.model = YOLO(str(local if local.exists() else \"yolov8n-pose.pt\"))\n",
    "            except Exception:\n",
    "                self.model = None\n",
    "\n",
    "        maxlen = max(6, int(MAX_BUF_SEC * FPS_ASSUMED))\n",
    "        self.cx_buf = deque(maxlen=maxlen)\n",
    "        self.cy_buf = deque(maxlen=maxlen)\n",
    "        self.area_buf = deque(maxlen=maxlen)\n",
    "        self.last_stable_ts: Optional[float] = None\n",
    "\n",
    "        self.captured_img: Optional[Image.Image] = None\n",
    "        self.captured_ts: float = 0.0\n",
    "        self.captured_metrics: List[str] = []\n",
    "\n",
    "        self.latest_rgb: Optional[np.ndarray] = None\n",
    "        self.last_metrics_lines: List[str] = []\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _detect_bbox(self, rgb: np.ndarray) -> Optional[Tuple[int, int, int, int]]:\n",
    "        if not self.model:\n",
    "            return None\n",
    "        res = self.model.predict(rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "        if len(res) == 0 or res[0].boxes is None:\n",
    "            return None\n",
    "        b = res[0].boxes.xyxy\n",
    "        if b is None or len(b) == 0:\n",
    "            return None\n",
    "        b = b.cpu().numpy()\n",
    "        areas = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "        i = int(np.argmax(areas))\n",
    "        x1, y1, x2, y2 = b[i].astype(int).tolist()\n",
    "        return x1, y1, x2, y2\n",
    "\n",
    "    def _extract_keypoints(self, res) -> Dict[int, Tuple[float, float] | None]:\n",
    "        kps: Dict[int, Tuple[float, float] | None] = {}\n",
    "        try:\n",
    "            if res and res[0].keypoints is not None and len(res[0].keypoints) > 0:\n",
    "                xy = res[0].keypoints.xy[0].cpu().numpy()\n",
    "                for i in range(xy.shape[0]):\n",
    "                    kps[i] = (float(xy[i, 0]), float(xy[i, 1]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return kps\n",
    "\n",
    "    def _update_stillness(self, bbox, w, h) -> bool:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cx = 0.5 * (x1 + x2)\n",
    "        cy = 0.5 * (y1 + y2)\n",
    "        area = max(1.0, (x2 - x1) * (y2 - y1))\n",
    "        rel_area = area / float(w * h)\n",
    "\n",
    "        if rel_area < MIN_FACE_AREA:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        self.cx_buf.append(cx)\n",
    "        self.cy_buf.append(cy)\n",
    "        self.area_buf.append(rel_area)\n",
    "\n",
    "        need_len = int(STILLNESS_SEC * FPS_ASSUMED * 0.6)\n",
    "        if len(self.cx_buf) < max(3, need_len):\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        stdx = float(np.std(self.cx_buf))\n",
    "        stdy = float(np.std(self.cy_buf))\n",
    "        stda = float(np.std(self.area_buf))\n",
    "        stable_now = (\n",
    "            stdx < MOTION_EPS_CXCY\n",
    "            and stdy < MOTION_EPS_CXCY\n",
    "            and stda < MOTION_EPS_AREA\n",
    "        )\n",
    "\n",
    "        now = time.time()\n",
    "        if stable_now:\n",
    "            if self.last_stable_ts is None:\n",
    "                self.last_stable_ts = now\n",
    "            return (now - self.last_stable_ts) >= STILLNESS_SEC\n",
    "        else:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "    def _stamp_capture(self):\n",
    "        \"\"\"æŠŠå½“å‰å¸§ä¸å½“å‰æŒ‡æ ‡ä½œä¸ºä¸€æ¬¡æŠ“æ‹ã€‚\"\"\"\n",
    "        self.captured_img = Image.fromarray(self.latest_rgb)\n",
    "        self.captured_ts = time.time()\n",
    "        self.captured_metrics = list(self.last_metrics_lines)\n",
    "        self.last_stable_ts = None\n",
    "\n",
    "    def capture_now(self) -> bool:\n",
    "        if self.latest_rgb is None:\n",
    "            return False\n",
    "        with self.lock:\n",
    "            self._stamp_capture()\n",
    "        return True\n",
    "\n",
    "    def recv(self, frame):\n",
    "        import av\n",
    "\n",
    "        img_bgr = frame.to_ndarray(format=\"bgr24\")\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        h, w, _ = img_rgb.shape\n",
    "        self.latest_rgb = img_rgb\n",
    "\n",
    "        if self.model:\n",
    "            res = self.model.predict(img_rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "            plotted = res[0].plot()[:, :, ::-1]  # YOLO è‡ªå¸¦è“æ¡†+ç»¿éª¨æ¶\n",
    "            kps = self._extract_keypoints(res)\n",
    "            lines = format_metrics(kps)\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = lines\n",
    "            pil = Image.fromarray(plotted)\n",
    "            pil = draw_tiny_metrics_top_right(pil, lines, size=16, margin=10)\n",
    "            out_rgb = np.array(pil)\n",
    "        else:\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = [\"(pose model not available)\"]\n",
    "            out_rgb = img_rgb\n",
    "\n",
    "        bbox = self._detect_bbox(img_rgb) if self.model else None\n",
    "        if bbox and self._update_stillness(bbox, w, h):\n",
    "            with self.lock:\n",
    "                if time.time() - self.captured_ts > 0.35:\n",
    "                    self._stamp_capture()\n",
    "\n",
    "        out_bgr = out_rgb[:, :, ::-1]\n",
    "        return av.VideoFrame.from_ndarray(out_bgr, format=\"bgr24\")\n",
    "\n",
    "\n",
    "# =================== Streamlit é¡µé¢ï¼ˆç­–å±•å¸ƒå±€ï¼‰ ===================\n",
    "\n",
    "st.set_page_config(page_title=APP_TITLE, layout=\"wide\")\n",
    "\n",
    "# ç®€å• CSSï¼šä¸¤åˆ—ç»“æ„ï¼Œå·¦æ‘„åƒå¤´ç«–å±å±…ä¸­ï¼Œå³å›¾åƒåŒºåŸŸå›ºå®šå®½åº¦\n",
    "st.markdown(\n",
    "    f\"\"\"\n",
    "<style>\n",
    "section[data-testid=\"stSidebar\"] {{ display: none !important; }}\n",
    "header, footer, [data-testid=\"stToolbar\"] {{ visibility: hidden !important; }}\n",
    ".block-container {{ padding-top: 0.6rem; padding-bottom: 0.6rem; max-width: 1700px; }}\n",
    ".left-col .cam-wrap {{\n",
    "  position: relative;\n",
    "  height: 92vh;\n",
    "  width: 100%;\n",
    "  overflow: hidden;\n",
    "  border-radius: 12px;\n",
    "  background: #111;\n",
    "}}\n",
    ".left-col .cam-wrap video {{\n",
    "  height: 100% !important;\n",
    "  width: auto !important;\n",
    "  object-fit: cover !important;\n",
    "  border-radius: 12px !important;\n",
    "}}\n",
    ".right-col .art-wrap {{\n",
    "  position: relative;\n",
    "  height: 92vh;\n",
    "  max-width: {RIGHT_IMG_MAXW}px;\n",
    "  overflow: hidden;\n",
    "  margin: 0 auto;\n",
    "}}\n",
    ".right-col .art-wrap img {{\n",
    "  display: block;\n",
    "  width: 100% !important;\n",
    "  height: auto !important;\n",
    "}}\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "st.title(APP_TITLE)\n",
    "st.caption(\n",
    "    \"Hold still for ~3â€“5 seconds to auto-capture, or press the button to capture on demand. \"\n",
    "    \"Left: live with pose. Right: matched artwork with tiny pink metrics.\"\n",
    ")\n",
    "\n",
    "# å°è¯•è½»å¾®è‡ªåŠ¨åˆ·æ–°ï¼Œè®©è‡ªåŠ¨æŠ“æ‹æ—¶å³ä¾§è‡ªåŠ¨æ›´æ–°\n",
    "try:\n",
    "    st.autorefresh(interval=700, key=\"ear_auto\", limit=None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "left, right = st.columns([1, 1], gap=\"large\")\n",
    "\n",
    "if \"countdown_target\" not in st.session_state:\n",
    "    st.session_state[\"countdown_target\"] = None\n",
    "\n",
    "if \"last_match\" not in st.session_state:\n",
    "    st.session_state[\"last_match\"] = None  # ä¿å­˜æœ€åä¸€æ¬¡ API è¿”å›å†…å®¹\n",
    "if \"last_metrics\" not in st.session_state:\n",
    "    st.session_state[\"last_metrics\"] = []\n",
    "if \"last_ts\" not in st.session_state:\n",
    "    st.session_state[\"last_ts\"] = 0.0\n",
    "\n",
    "API_URL = DEFAULT_API_URL  # å¦‚æœä½ å¸Œæœ›åœ¨ UI é‡Œå¯ç¼–è¾‘ï¼Œä¹Ÿå¯ä»¥åšæˆ text_input\n",
    "\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"Live\")\n",
    "    st.markdown('<div class=\"left-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"cam-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ctx = webrtc_streamer(\n",
    "        key=\"ear-curatorial-final\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "        video_processor_factory=CuratorialProcessor,\n",
    "        async_processing=True,\n",
    "    )\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /cam-wrap\n",
    "    st.markdown(\"<div style='height:10px'></div>\", unsafe_allow_html=True)\n",
    "\n",
    "    c1, c2 = st.columns(2)\n",
    "    with c1:\n",
    "        if st.button(\"ğŸ“¸ Capture (wait 3s)\", use_container_width=True):\n",
    "            st.session_state[\"countdown_target\"] = time.time() + 3.0\n",
    "    with c2:\n",
    "        if st.button(\"âš¡ Instant Capture\", use_container_width=True):\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"No frame yet, try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "\n",
    "    # å€’è®¡æ—¶é€»è¾‘\n",
    "    if st.session_state[\"countdown_target\"]:\n",
    "        remain = st.session_state[\"countdown_target\"] - time.time()\n",
    "        if remain > 0:\n",
    "            st.info(f\"Capturing in {remain:.1f}sâ€¦ Please hold still.\")\n",
    "        else:\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"No frame yet.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "            st.session_state[\"countdown_target\"] = None\n",
    "            force_rerun()\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /left-col\n",
    "\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"Matched artwork\")\n",
    "    st.markdown('<div class=\"right-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"art-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ph = st.empty()\n",
    "\n",
    "    proc: Optional[CuratorialProcessor] = None\n",
    "    if ctx and ctx.video_processor:\n",
    "        proc = ctx.video_processor\n",
    "\n",
    "    if not proc:\n",
    "        ph.info(\"Initializing cameraâ€¦\")\n",
    "    else:\n",
    "        # ä»å¤„ç†å™¨è¯»å–æœ€æ–°æŠ“æ‹\n",
    "        with proc.lock:\n",
    "            cap_img = proc.captured_img\n",
    "            cap_ts = getattr(proc, \"captured_ts\", 0.0)\n",
    "            cap_metrics = list(getattr(proc, \"captured_metrics\", []))\n",
    "\n",
    "        if cap_img is not None and cap_ts > st.session_state[\"last_ts\"]:\n",
    "            # æ›´æ–°æœ¬åœ°æ—¶é—´æˆ³\n",
    "            st.session_state[\"last_ts\"] = cap_ts\n",
    "            st.session_state[\"last_metrics\"] = cap_metrics\n",
    "\n",
    "            # æŠŠæŠ“æ‹å›¾å‘ç»™åç«¯ /match\n",
    "            buf = io.BytesIO()\n",
    "            cap_img.save(buf, format=\"JPEG\")\n",
    "            buf.seek(0)\n",
    "\n",
    "            files = {\"image\": (\"frame.jpg\", buf.getvalue(), \"image/jpeg\")}\n",
    "            data = {\"museum\": \"local\", \"topk\": 3}\n",
    "\n",
    "            try:\n",
    "                resp = requests.post(API_URL, files=files, data=data, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                payload = resp.json()\n",
    "                st.session_state[\"last_match\"] = payload\n",
    "            except Exception as exc:\n",
    "                st.session_state[\"last_match\"] = {\"error\": str(exc)}\n",
    "\n",
    "            force_rerun()\n",
    "\n",
    "        payload = st.session_state.get(\"last_match\")\n",
    "\n",
    "        if not payload:\n",
    "            ph.info(\"Hold still or press capture to trigger matchingâ€¦\")\n",
    "        elif \"error\" in payload:\n",
    "            ph.error(f\"Error from backend: {payload['error']}\")\n",
    "        else:\n",
    "            results = payload.get(\"results\") or []\n",
    "            if not results:\n",
    "                ph.warning(\"No matches returned from backend.\")\n",
    "            else:\n",
    "                # åªå– Top-1ï¼Œå±•é™ˆæ•ˆæœæ›´å¹²å‡€\n",
    "                top = results[0]\n",
    "                filename = top.get(\"filename\") or top.get(\"file\")\n",
    "\n",
    "                img_path = ensure_image_path(filename or \"\")\n",
    "                if not img_path:\n",
    "                    ph.error(f\"Image file not found for: {filename}\")\n",
    "                else:\n",
    "                    painting = safe_open_image(img_path)\n",
    "                    if painting is None:\n",
    "                        ph.error(f\"Failed to open image: {img_path}\")\n",
    "                    else:\n",
    "                        # ä»æœ¬åœ° CSV æŸ¥è¡¥å…… meta\n",
    "                        meta_row = lookup_meta(str(filename))\n",
    "                        meta = {\n",
    "                            \"artist\": meta_row.get(\"artist\") or top.get(\"artist\") or \"artist name\",\n",
    "                            \"year\": meta_row.get(\"year\") or top.get(\"year\") or \"\",\n",
    "                            \"price_text\": meta_row.get(\"price_text\")\n",
    "                            or meta_row.get(\"auction_price_usd\")\n",
    "                            or \"\",\n",
    "                        }\n",
    "\n",
    "                        painted = overlay_right_labels(painting, meta)\n",
    "\n",
    "                        # æŠŠæŠ“æ‹æ—¶çš„å§¿æ€æŒ‡æ ‡å åˆ°å³ä¸Šè§’ï¼ˆç²‰è‰²æ–‡æœ¬ï¼‰\n",
    "                        metrics = st.session_state.get(\"last_metrics\") or []\n",
    "                        painted = draw_tiny_metrics_top_right(\n",
    "                            painted, metrics, size=16, margin=12\n",
    "                        )\n",
    "\n",
    "                        w = min(RIGHT_IMG_MAXW, painted.width)\n",
    "                        caption = f\"{meta_row.get('title','')} â€” {meta.get('artist','')}\"\n",
    "                        ph.image(painted, caption=caption, width=w)\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /art-wrap\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /right-col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16066455",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09947e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EAR Backend API (FastAPI + Uvicorn)\n",
    "\n",
    "Exposes:\n",
    "- GET  /health     â†’ quick status check\n",
    "- POST /match      â†’ upload an image and get Top-K matched artworks\n",
    "\n",
    "It uses the PoseMatcher defined in backend/model/pose_matcher.py and the\n",
    "Settings from backend/config.py.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import logging\n",
    "from typing import List, Optional\n",
    "\n",
    "from fastapi import FastAPI, File, Form, HTTPException, UploadFile\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from PIL import Image\n",
    "\n",
    "from .config import Settings\n",
    "from .model.pose_matcher import PoseMatcher\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"uvicorn.error\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FastAPI app + CORS\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Embodied Aesthetic Reconstruction â€” Backend API\",\n",
    "    version=\"0.1.0\",\n",
    "    description=\"CLIP-based portrait â†’ artwork matching backend.\",\n",
    ")\n",
    "\n",
    "# Allow everything locally; you can tighten this later if needed.\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Global objects filled at startup\n",
    "settings: Optional[Settings] = None\n",
    "matcher: Optional[PoseMatcher] = None\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Startup / shutdown\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def _startup_load() -> None:\n",
    "    \"\"\"Load configuration + matcher once when the server starts.\"\"\"\n",
    "    global settings, matcher\n",
    "\n",
    "    logger.info(\"Loading Settings and PoseMatcherâ€¦\")\n",
    "    settings = Settings()\n",
    "    matcher = PoseMatcher(settings)\n",
    "    logger.info(\"Matcher loaded successfully.\")\n",
    "\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "def _shutdown_cleanup() -> None:\n",
    "    \"\"\"Optional cleanup (currently nothing special).\"\"\"\n",
    "    global matcher\n",
    "    matcher = None\n",
    "    logger.info(\"Matcher released.\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Simple health endpoint\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"device\": getattr(matcher, \"device\", \"unknown\") if matcher else \"unloaded\",\n",
    "        \"default_museum\": getattr(settings, \"default_museum\", None)\n",
    "        if settings\n",
    "        else None,\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Schemas for /match\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class MatchResult(BaseModel):\n",
    "    filename: str\n",
    "    title: Optional[str] = None\n",
    "    artist: Optional[str] = None\n",
    "    year: Optional[int] = None\n",
    "    score: float\n",
    "\n",
    "\n",
    "class MatchResponse(BaseModel):\n",
    "    museum: Optional[str]\n",
    "    topk: int\n",
    "    results: List[MatchResult]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Main matching endpoint\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@app.post(\"/match\", response_model=MatchResponse)\n",
    "async def match(\n",
    "    image: UploadFile = File(..., description=\"Uploaded portrait photo\"),\n",
    "    museum: Optional[str] = Form(None, description=\"Museum name, e.g. local / met\"),\n",
    "    topk: int = Form(3, description=\"Number of top results\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust matching endpoint:\n",
    "\n",
    "    - validates that the uploaded file looks like an image\n",
    "    - decodes it to a PIL image (RGB)\n",
    "    - calls PoseMatcher.match_pil(...)\n",
    "    - tries to back-fill missing title / artist / year from matcher.meta_by_filename\n",
    "    \"\"\"\n",
    "    # ----------------- basic validation -----------------\n",
    "    if topk <= 0:\n",
    "        topk = 3\n",
    "\n",
    "    if image.content_type is not None and not image.content_type.startswith(\"image/\"):\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=f\"Unsupported content type: {image.content_type}. Please upload an image file.\",\n",
    "        )\n",
    "\n",
    "    data = await image.read()\n",
    "    if not data:\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"Empty file received. Please take a photo again.\",\n",
    "        )\n",
    "\n",
    "    # ----------------- decode to PIL -----------------\n",
    "    try:\n",
    "        pil_img = Image.open(io.BytesIO(data)).convert(\"RGB\")\n",
    "    except Exception as exc:\n",
    "        logger.exception(\"Failed to decode uploaded image\")\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=f\"Invalid image file: {type(exc).__name__}\",\n",
    "        )\n",
    "\n",
    "    # ----------------- check matcher -----------------\n",
    "    global matcher\n",
    "    if matcher is None:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=\"Matcher is not loaded on the server.\",\n",
    "        )\n",
    "\n",
    "    effective_museum = museum or getattr(matcher, \"default_museum\", None) or \"local\"\n",
    "\n",
    "    # ----------------- run matcher -----------------\n",
    "    try:\n",
    "        if hasattr(matcher, \"match_pil\"):\n",
    "            raw_results = matcher.match_pil(\n",
    "                pil_img,\n",
    "                museum=effective_museum,\n",
    "                topk=topk,\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\"PoseMatcher has no method 'match_pil'\")\n",
    "    except ValueError as exc:\n",
    "        logger.exception(\"ValueError in matcher.match_pil\")\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=f\"Matching failed: {exc}\",\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        logger.exception(\"Unhandled error in matcher.match_pil\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Internal error during matching: {type(exc).__name__}: {exc}\",\n",
    "        )\n",
    "\n",
    "    # ----------------- normalize results structure -----------------\n",
    "    payload: List[MatchResult] = []\n",
    "\n",
    "    meta_store = getattr(matcher, \"meta_by_filename\", None)\n",
    "\n",
    "    def fill_from_meta(filename: str, title, artist, year):\n",
    "        \"\"\"If title/artist/year are missing, try to fill from matcher metadata.\"\"\"\n",
    "        nonlocal meta_store\n",
    "        if not isinstance(meta_store, dict):\n",
    "            return title, artist, year\n",
    "\n",
    "        meta = meta_store.get(filename) or meta_store.get(filename.strip()) or {}\n",
    "        if isinstance(meta, dict):\n",
    "            title = title or meta.get(\"title\") or meta.get(\"Title\")\n",
    "            artist = artist or meta.get(\"artist\") or meta.get(\"Artist\")\n",
    "            year = year or meta.get(\"year\") or meta.get(\"Year\")\n",
    "        else:\n",
    "            title = title or getattr(meta, \"title\", None)\n",
    "            artist = artist or getattr(meta, \"artist\", None)\n",
    "            year = year or getattr(meta, \"year\", None)\n",
    "        return title, artist, year\n",
    "\n",
    "    for r in raw_results or []:\n",
    "        if isinstance(r, dict):\n",
    "            filename = r.get(\"filename\") or r.get(\"file\") or \"\"\n",
    "            score = float(r.get(\"score\", 0.0))\n",
    "            title = r.get(\"title\")\n",
    "            artist = r.get(\"artist\")\n",
    "            year = r.get(\"year\")\n",
    "        else:\n",
    "            filename = getattr(r, \"filename\", \"\") or getattr(r, \"file\", \"\")\n",
    "            score = float(getattr(r, \"score\", 0.0))\n",
    "            title = getattr(r, \"title\", None)\n",
    "            artist = getattr(r, \"artist\", None)\n",
    "            year = getattr(r, \"year\", None)\n",
    "\n",
    "        title, artist, year = fill_from_meta(filename, title, artist, year)\n",
    "\n",
    "        # normalise year â†’ int or None\n",
    "        year_int: Optional[int]\n",
    "        try:\n",
    "            if isinstance(year, str) and year.strip():\n",
    "                year_int = int(year.split(\",\")[0].strip())\n",
    "            elif isinstance(year, (int, float)):\n",
    "                year_int = int(year)\n",
    "            else:\n",
    "                year_int = None\n",
    "        except Exception:\n",
    "            year_int = None\n",
    "\n",
    "        payload.append(\n",
    "            MatchResult(\n",
    "                filename=filename,\n",
    "                title=title,\n",
    "                artist=artist,\n",
    "                year=year_int,\n",
    "                score=float(score),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return MatchResponse(\n",
    "        museum=effective_museum,\n",
    "        topk=topk,\n",
    "        results=payload,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4a9e25",
   "metadata": {},
   "source": [
    "pose_matcher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c025e8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PoseMatcher\n",
    "\n",
    "A lightweight CLIP-based matcher that:\n",
    "\n",
    "- loads pre-computed embeddings from data/local/embeddings.npy\n",
    "- loads metadata from data/local/embeddings_meta.csv\n",
    "- computes an embedding for the query image\n",
    "- returns the Top-K most similar artworks with their metadata\n",
    "\n",
    "This version does NOT depend on YOLO pose directly; it only uses\n",
    "the CLIP image encoder. You already created the embeddings with:\n",
    "\n",
    "    python backend/tools/build_embeddings.py --museum local ...\n",
    "\n",
    "So this matcher just consumes those files.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from backend.config import Settings\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MatchItem:\n",
    "    filename: str\n",
    "    title: Optional[str]\n",
    "    artist: Optional[str]\n",
    "    year: Optional[int]\n",
    "    score: float\n",
    "\n",
    "\n",
    "class PoseMatcher:\n",
    "    def __init__(self, settings: Settings):\n",
    "        self.settings = settings\n",
    "\n",
    "        # --------- Resolve device ----------\n",
    "        if settings.device == \"cpu\":\n",
    "            device = \"cpu\"\n",
    "        elif settings.device == \"cuda\" and torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif settings.device == \"mps\" and torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "        else:\n",
    "            # \"auto\" or unsupported â†’ try CUDA, then MPS, then CPU\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda\"\n",
    "            elif torch.backends.mps.is_available():\n",
    "                device = \"mps\"\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        # --------- Load CLIP model ----------\n",
    "        # It must match the model used in build_embeddings.py\n",
    "        self.model, self.preprocess, _ = open_clip.create_model_and_transforms(\n",
    "            \"ViT-B-32\",\n",
    "            pretrained=\"openai\",\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "        # --------- Load stored embeddings ----------\n",
    "        emb_path = settings.embeddings_path\n",
    "        if not emb_path.exists():\n",
    "            raise FileNotFoundError(f\"Embeddings file not found: {emb_path}\")\n",
    "\n",
    "        # shape: (N, D)\n",
    "        emb_np = np.load(emb_path)\n",
    "        if isinstance(emb_np, np.lib.npyio.NpzFile):\n",
    "            # Just in case someone saved np.savez\n",
    "            # we expect a \"embeddings\" key\n",
    "            emb_np = emb_np[\"embeddings\"]\n",
    "\n",
    "        # Store as normalized torch tensor on device\n",
    "        emb = torch.from_numpy(emb_np).float()\n",
    "        emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        self.embeddings = emb.to(self.device)\n",
    "\n",
    "        # --------- Load metadata ----------\n",
    "        meta_path = settings.embeddings_meta_path\n",
    "        if not meta_path.exists():\n",
    "            raise FileNotFoundError(f\"Metadata CSV not found: {meta_path}\")\n",
    "\n",
    "        self.meta: List[Dict] = []\n",
    "        self.meta_by_filename: Dict[str, Dict] = {}\n",
    "\n",
    "        with meta_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            header = f.readline().strip().split(\",\")\n",
    "            # Simple CSV reader; no quoted commas expected here\n",
    "            for line in f:\n",
    "                parts = [p.strip() for p in line.strip().split(\",\")]\n",
    "                if not parts or not parts[0]:\n",
    "                    continue\n",
    "                row = dict(zip(header, parts))\n",
    "                filename = row.get(\"filename\") or row.get(\"file\") or \"\"\n",
    "                if not filename:\n",
    "                    continue\n",
    "\n",
    "                # Normalize year to int if possible\n",
    "                raw_year = row.get(\"year\") or row.get(\"Year\")\n",
    "                year_int: Optional[int] = None\n",
    "                if raw_year:\n",
    "                    try:\n",
    "                        year_int = int(str(raw_year).split(\",\")[0].strip())\n",
    "                    except Exception:\n",
    "                        year_int = None\n",
    "\n",
    "                meta_entry = {\n",
    "                    \"filename\": filename,\n",
    "                    \"title\": row.get(\"title\") or row.get(\"Title\"),\n",
    "                    \"artist\": row.get(\"artist\") or row.get(\"Artist\"),\n",
    "                    \"year\": year_int,\n",
    "                    # Keep raw dict as well in case we need extra columns later\n",
    "                    **row,\n",
    "                }\n",
    "                self.meta.append(meta_entry)\n",
    "                self.meta_by_filename[filename] = meta_entry\n",
    "\n",
    "        # For compatibility with the backend/main.py code\n",
    "        self.default_museum = settings.default_museum\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Core API used both by backend and Streamlit frontend\n",
    "    # ------------------------------------------------------------------\n",
    "    def _encode_image(self, image: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"Encode a PIL image into a normalized CLIP embedding.\"\"\"\n",
    "        img = self.preprocess(image).unsqueeze(0).to(self.device)  # (1, 3, H, W)\n",
    "        with torch.no_grad():\n",
    "            feat = self.model.encode_image(img)  # (1, D)\n",
    "        feat = feat / (feat.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        return feat  # (1, D)\n",
    "\n",
    "    def match_pil(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        museum: Optional[str] = None,\n",
    "        topk: int = 3,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Match a PIL image against the stored embeddings.\n",
    "\n",
    "        Returns a list of dicts:\n",
    "            { \"filename\": ..., \"title\": ..., \"artist\": ..., \"year\": ..., \"score\": ... }\n",
    "        \"\"\"\n",
    "        if topk <= 0:\n",
    "            topk = 3\n",
    "\n",
    "        # Encode query\n",
    "        query = self._encode_image(image)  # (1, D)\n",
    "\n",
    "        # Cosine similarity with all stored embeddings\n",
    "        # Because both are normalized, dot-product == cosine\n",
    "        sims = (self.embeddings @ query.T).squeeze(1)  # (N,)\n",
    "\n",
    "        # Top-K indices\n",
    "        k = min(topk, sims.shape[0])\n",
    "        scores, indices = torch.topk(sims, k=k, largest=True, sorted=True)\n",
    "\n",
    "        results: List[Dict] = []\n",
    "        for score, idx in zip(scores.tolist(), indices.tolist()):\n",
    "            # index into meta list; we assume same order as embeddings\n",
    "            if 0 <= idx < len(self.meta):\n",
    "                meta = self.meta[idx]\n",
    "            else:\n",
    "                meta = {}\n",
    "\n",
    "            filename = meta.get(\"filename\", \"\")\n",
    "            title = meta.get(\"title\")\n",
    "            artist = meta.get(\"artist\")\n",
    "            year = meta.get(\"year\")\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"filename\": filename,\n",
    "                    \"title\": title,\n",
    "                    \"artist\": artist,\n",
    "                    \"year\": year,\n",
    "                    \"score\": float(score),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da181e30",
   "metadata": {},
   "source": [
    "utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69926179",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# model/utils.py\n",
    "# -----------------------------------------------------------------------------\n",
    "# Low-level utilities for image I/O and vector math.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_image_from_bytes(image_bytes: bytes) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Read an image from raw bytes and convert to RGB PIL Image.\n",
    "    This function raises if the bytes are not a valid image.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(image_bytes))\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "\n",
    "def l2_normalize(x: np.ndarray, axis: int = 1, eps: float = 1e-10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    L2-normalize an array along the given axis.\n",
    "\n",
    "    Args:\n",
    "        x: array of shape (..., D) or (N, D) etc.\n",
    "        axis: axis to normalize across.\n",
    "        eps: numerical stability term.\n",
    "\n",
    "    Returns:\n",
    "        Array with unit L2 norm along `axis`.\n",
    "    \"\"\"\n",
    "    norm = np.linalg.norm(x, ord=2, axis=axis, keepdims=True)\n",
    "    return x / (norm + eps)\n",
    "\n",
    "\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between rows of `a` and a single vector `b`.\n",
    "\n",
    "    Assumes both `a` and `b` are already L2-normalized.\n",
    "    Then cosine similarity reduces to a dot product.\n",
    "\n",
    "    Args:\n",
    "        a: (N, D) matrix (database embeddings).\n",
    "        b: (D,) vector (query embedding).\n",
    "\n",
    "    Returns:\n",
    "        (N,) similarity scores.\n",
    "    \"\"\"\n",
    "    # If b has shape (D, 1), squeeze to (D,)\n",
    "    if b.ndim == 2 and b.shape[1] == 1:\n",
    "        b = b[:, 0]\n",
    "    return a @ b  # (N, D) Â· (D,) -> (N,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21bcbf",
   "metadata": {},
   "source": [
    "config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022023a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Global configuration for the EAR backend.\n",
    "\n",
    "This file defines a simple Settings model that knows where your\n",
    "data lives (embeddings, metadata CSV, images directory) and which\n",
    "device to use for inference.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "\n",
    "\n",
    "class Settings(BaseModel):\n",
    "    # Root folder that contains embeddings + metadata + images\n",
    "    data_root: Path = PROJECT_ROOT / \"data\" / \"local\"\n",
    "\n",
    "    # Default museum name used when the frontend does not specify one\n",
    "    default_museum: str = \"local\"\n",
    "\n",
    "    # Device: \"auto\", \"cpu\", \"cuda\", \"mps\"\n",
    "    device: str = \"auto\"\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def embeddings_path(self) -> Path:\n",
    "        return self.data_root / \"embeddings.npy\"\n",
    "\n",
    "    @property\n",
    "    def embeddings_meta_path(self) -> Path:\n",
    "        return self.data_root / \"embeddings_meta.csv\"\n",
    "\n",
    "    @property\n",
    "    def images_dir(self) -> Path:\n",
    "        return self.data_root / \"images\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3008b80",
   "metadata": {},
   "source": [
    "build_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be868c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Build CLIP embeddings for a museum folder.\n",
    "\n",
    "Layout (per museum):\n",
    "  data/<museum>/\n",
    "    images/                  # *.jpg / *.png ...\n",
    "    embeddings.npy           # (N, D)\n",
    "    embeddings_meta.csv      # at least: filename + optional columns from --meta\n",
    "\n",
    "Usage:\n",
    "  python backend/tools/build_embeddings.py --museum local --meta data/local/portrait_works.csv --overwrite\n",
    "  python backend/tools/build_embeddings.py --all --overwrite\n",
    "\n",
    "Deps:\n",
    "  pip install open_clip_torch torch pillow numpy pandas tqdm\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import open_clip\n",
    "\n",
    "\n",
    "# ------------------------- utils -------------------------\n",
    "\n",
    "def detect_device(device_override: Optional[str] = None) -> torch.device:\n",
    "    if device_override:\n",
    "        return torch.device(device_override)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def list_images(folder: str) -> List[str]:\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tiff\"}\n",
    "    files = []\n",
    "    for name in sorted(os.listdir(folder)):\n",
    "        p = os.path.join(folder, name)\n",
    "        if os.path.isfile(p) and os.path.splitext(name.lower())[1] in exts:\n",
    "            files.append(p)\n",
    "    return files\n",
    "\n",
    "\n",
    "def normalize_rows(x: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12\n",
    "    return x / n\n",
    "\n",
    "\n",
    "def load_external_meta(meta_csv_path: Optional[str]) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Robust CSV reader (BOM-safe, header-trim, filename normalization).\"\"\"\n",
    "    if not meta_csv_path:\n",
    "        return None\n",
    "    if not os.path.exists(meta_csv_path):\n",
    "        print(f\"[WARN] metadata CSV not found: {meta_csv_path}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(meta_csv_path, encoding=\"utf-8-sig\")\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    # accept 'filename' or common variants\n",
    "    if \"filename\" not in df.columns:\n",
    "        alt = None\n",
    "        for c in (\"image\", \"file\"):\n",
    "            if c in df.columns:\n",
    "                alt = c\n",
    "                break\n",
    "        if alt:\n",
    "            df = df.rename(columns={alt: \"filename\"})\n",
    "\n",
    "    if \"filename\" not in df.columns:\n",
    "        raise ValueError(\"External metadata CSV must contain a 'filename' (or 'image'/'file') column.\")\n",
    "\n",
    "    df[\"filename\"] = df[\"filename\"].map(lambda x: os.path.basename(str(x)).strip())\n",
    "    return df\n",
    "\n",
    "\n",
    "def dual_views(preprocess, pil: Image.Image):\n",
    "    \"\"\"full image + center crop (square) as a tiny augmentation ensemble.\"\"\"\n",
    "    # full\n",
    "    im_full = preprocess(pil.convert(\"RGB\"))\n",
    "    # center crop to min side (square), then preprocess again\n",
    "    w, h = pil.size\n",
    "    m = min(w, h)\n",
    "    left = (w - m) // 2\n",
    "    top = (h - m) // 2\n",
    "    pil_square = pil.crop((left, top, left + m, top + m))\n",
    "    im_square = preprocess(pil_square.convert(\"RGB\"))\n",
    "    return im_full, im_square\n",
    "\n",
    "\n",
    "# ------------------------- core -------------------------\n",
    "\n",
    "def build_for_museum(\n",
    "    museum: str,\n",
    "    data_root: str,\n",
    "    model_name: str = \"ViT-B-32\",\n",
    "    pretrained: str = \"openai\",\n",
    "    device: Optional[str] = None,\n",
    "    meta_csv: Optional[str] = None,\n",
    "    overwrite: bool = False,\n",
    "    use_dual_view: bool = True,\n",
    "):\n",
    "    out_dir = os.path.join(data_root, museum)\n",
    "    img_dir = os.path.join(out_dir, \"images\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(img_dir):\n",
    "        print(f\"[WARN] images folder not found: {img_dir}\")\n",
    "        return\n",
    "\n",
    "    files = list_images(img_dir)\n",
    "    if not files:\n",
    "        print(f\"[WARN] no images found in: {img_dir}\")\n",
    "        return\n",
    "\n",
    "    dev = detect_device(device)\n",
    "    print(f\"[INFO] museum={museum} | images={len(files)} | device={dev} | model={model_name}/{pretrained}\")\n",
    "\n",
    "    embed_path = os.path.join(out_dir, \"embeddings.npy\")\n",
    "    meta_path = os.path.join(out_dir, \"embeddings_meta.csv\")\n",
    "\n",
    "    if (os.path.exists(embed_path) or os.path.exists(meta_path)) and not overwrite:\n",
    "        print(f\"[SKIP] embeddings/meta already exist (use --overwrite to rebuild): {out_dir}\")\n",
    "        return\n",
    "\n",
    "    # model\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained, device=dev)\n",
    "    model.eval()\n",
    "\n",
    "    # encode\n",
    "    feats = []\n",
    "    for p in tqdm(files, desc=f\"[{museum}] progress\"):\n",
    "        try:\n",
    "            pil = Image.open(p)\n",
    "        except Exception:\n",
    "            print(f\"[WARN] failed to open image: {p}\")\n",
    "            continue\n",
    "\n",
    "        if use_dual_view:\n",
    "            im1, im2 = dual_views(preprocess, pil)\n",
    "            ims = torch.stack([im1, im2], dim=0).to(dev)\n",
    "        else:\n",
    "            ims = torch.stack([preprocess(pil.convert(\"RGB\"))], dim=0).to(dev)\n",
    "\n",
    "        with torch.no_grad(), torch.autocast(device_type=str(dev), enabled=(dev.type in [\"cuda\", \"mps\"])):\n",
    "            feat = model.encode_image(ims)  # (k, D)\n",
    "            feat = feat.float().cpu().numpy()\n",
    "            feat = feat.mean(axis=0, keepdims=True)  # simple average for dual view\n",
    "        feats.append(feat)\n",
    "\n",
    "    if not feats:\n",
    "        print(f\"[WARN] no features produced for: {museum}\")\n",
    "        return\n",
    "\n",
    "    E = np.concatenate(feats, axis=0)  # (N, D)\n",
    "    E = normalize_rows(E)\n",
    "    np.save(embed_path, E)\n",
    "    print(f\"[OK] {museum}: embeddings={E.shape} â†’ {embed_path}\")\n",
    "\n",
    "    # base meta (filename only)\n",
    "    meta_rows = [{\"filename\": os.path.basename(p)} for p in files]\n",
    "    base_df = pd.DataFrame(meta_rows)\n",
    "\n",
    "    # external meta\n",
    "    ext_df = None\n",
    "    try:\n",
    "        ext_df = load_external_meta(meta_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] failed to read external metadata: {e}\")\n",
    "\n",
    "    merged = base_df.merge(ext_df, on=\"filename\", how=\"left\") if ext_df is not None else base_df\n",
    "\n",
    "    # filename first\n",
    "    cols = [\"filename\"] + [c for c in merged.columns if c != \"filename\"]\n",
    "    merged = merged[cols]\n",
    "    merged.to_csv(meta_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[OK] {museum}: meta rows={len(merged)} â†’ {meta_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--museum\", type=str, help=\"Museum folder name under data/, e.g., local / met\")\n",
    "    parser.add_argument(\"--all\", action=\"store_true\", help=\"Build for all subfolders under data/\")\n",
    "    parser.add_argument(\"--data_root\", type=str, default=\"data\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"ViT-B-32\")\n",
    "    parser.add_argument(\"--pretrained\", type=str, default=\"openai\")\n",
    "    parser.add_argument(\"--device\", type=str, default=None, help=\"cuda / mps / cpu (auto if omitted)\")\n",
    "    parser.add_argument(\"--meta\", type=str, default=None, help=\"Optional external CSV to merge (must have filename)\")\n",
    "    parser.add_argument(\"--overwrite\", action=\"store_true\")\n",
    "    parser.add_argument(\"--no_dual\", action=\"store_true\", help=\"Disable dual-view averaging\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not args.museum and not args.all:\n",
    "        print(\"\\nPlease set --museum=NAME or use --all.\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if args.all:\n",
    "        for m in sorted(d for d in os.listdir(args.data_root)\n",
    "                        if os.path.isdir(os.path.join(args.data_root, d))):\n",
    "            build_for_museum(\n",
    "                museum=m,\n",
    "                data_root=args.data_root,\n",
    "                model_name=args.model,\n",
    "                pretrained=args.pretrained,\n",
    "                device=args.device,\n",
    "                meta_csv=args.meta,\n",
    "                overwrite=args.overwrite,\n",
    "                use_dual_view=not args.no_dual,\n",
    "            )\n",
    "    else:\n",
    "        build_for_museum(\n",
    "            museum=args.museum,\n",
    "            data_root=args.data_root,\n",
    "            model_name=args.model,\n",
    "            pretrained=args.pretrained,\n",
    "            device=args.device,\n",
    "            meta_csv=args.meta,\n",
    "            overwrite=args.overwrite,\n",
    "            use_dual_view=not args.no_dual,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e02a52",
   "metadata": {},
   "source": [
    "app_frontend.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d0a387",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_webrtc import (\n",
    "    WebRtcMode,\n",
    "    RTCConfiguration,\n",
    "    VideoProcessorBase,\n",
    "    webrtc_streamer,\n",
    ")\n",
    "\n",
    "# =================== è·¯å¾„ä¸å¸¸é‡ ===================\n",
    "\n",
    "FRONTEND_DIR = Path(__file__).resolve().parent\n",
    "ROOT_DIR = FRONTEND_DIR.parent\n",
    "\n",
    "# æœ¬åœ°æ•°æ®ï¼ˆç”¨äºæ ¹æ® filename æ‰¾ metaï¼‰\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"local\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "# å¯èƒ½å­˜åœ¨çš„ meta CSV\n",
    "META_CSV_CANDIDATES = [\n",
    "    DATA_DIR / \"portrait_works_enhanced_english.csv\",\n",
    "    DATA_DIR / \"portrait_works.csv\",\n",
    "]\n",
    "\n",
    "# åç«¯ APIï¼ˆéœ€è¦æ—¶ä½ å¯ä»¥æ”¹æˆ HuggingFace åœ°å€ï¼‰\n",
    "DEFAULT_API_URL = \"http://127.0.0.1:8000/match\"\n",
    "\n",
    "APP_TITLE = \"Embodied Aesthetic Reconstruction\"\n",
    "\n",
    "# YOLOv8-Pose æ¨¡å‹è·¯å¾„ï¼ˆæ”¾åœ¨ frontend ç›®å½•ä¸‹ï¼‰\n",
    "YOLO_MODEL_PATH = FRONTEND_DIR / \"yolov8n-pose.pt\"\n",
    "\n",
    "# WebRTC é…ç½®ï¼ˆSafari éœ€è¦ STUNï¼‰\n",
    "RTC_CONFIGURATION = RTCConfiguration(\n",
    "    {\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]}\n",
    ")\n",
    "\n",
    "# Stillness æ£€æµ‹ï¼ˆä¸ç­–å±•ç‰ˆä¸€è‡´ï¼‰\n",
    "STILLNESS_SEC = 3.5\n",
    "MAX_BUF_SEC = 5.0\n",
    "FPS_ASSUMED = 12\n",
    "MOTION_EPS_CXCY = 4.0\n",
    "MOTION_EPS_AREA = 0.03\n",
    "MIN_FACE_AREA = 0.06\n",
    "\n",
    "# é¢œè‰²\n",
    "YELLOW = (255, 235, 59)\n",
    "BLACK = (0, 0, 0)\n",
    "HOT_PINK = (255, 30, 180)\n",
    "\n",
    "# å±•ç¤ºåŒºåŸŸå®½åº¦\n",
    "RIGHT_IMG_MAXW = 900\n",
    "\n",
    "\n",
    "# =================== å°å·¥å…·å‡½æ•° ===================\n",
    "\n",
    "_META_CACHE: Optional[Dict[str, Dict]] = None\n",
    "\n",
    "\n",
    "def load_meta_mapping() -> Dict[str, Dict]:\n",
    "    \"\"\"åŠ è½½æœ¬åœ° CSVï¼ŒæŒ‰ filename å»ºç«‹ç´¢å¼•ã€‚\"\"\"\n",
    "    global _META_CACHE\n",
    "    if _META_CACHE is not None:\n",
    "        return _META_CACHE\n",
    "\n",
    "    import csv\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    for p in META_CSV_CANDIDATES:\n",
    "        if p.exists():\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                rows = list(csv.DictReader(f))\n",
    "            break\n",
    "\n",
    "    mapping: Dict[str, Dict] = {}\n",
    "    for r in rows:\n",
    "        fname = (\n",
    "            r.get(\"filename\")\n",
    "            or r.get(\"image_path\")\n",
    "            or r.get(\"path\")\n",
    "            or r.get(\"file\")\n",
    "        )\n",
    "        if fname:\n",
    "            mapping[str(fname)] = r\n",
    "\n",
    "    _META_CACHE = mapping\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def lookup_meta(filename: str) -> Dict:\n",
    "    mapping = load_meta_mapping()\n",
    "    return mapping.get(filename, {})\n",
    "\n",
    "\n",
    "def safe_open_image(p: Path) -> Optional[Image.Image]:\n",
    "    try:\n",
    "        return Image.open(p).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_image_path(filename: str) -> Optional[Path]:\n",
    "    \"\"\"æ ¹æ® filename åœ¨ IMAGES_DIR é‡Œæ‰¾åˆ°å›¾ç‰‡ã€‚\"\"\"\n",
    "    if not filename:\n",
    "        return None\n",
    "    p = Path(filename)\n",
    "    if not p.is_absolute():\n",
    "        p = IMAGES_DIR / p\n",
    "    return p if p.exists() else None\n",
    "\n",
    "\n",
    "def _load_font(size: int = 40) -> ImageFont.FreeTypeFont:\n",
    "    \"\"\"ä¼˜å…ˆç”¨ Courier / Courier Newï¼Œæ‰¾ä¸åˆ°å†é€€å› Arial / é»˜è®¤ã€‚\"\"\"\n",
    "    candidates = [\n",
    "        \"/Library/Fonts/Courier New.ttf\",\n",
    "        \"/System/Library/Fonts/Courier.dfont\",\n",
    "        \"/System/Library/Fonts/Supplemental/Courier New.ttf\",\n",
    "        \"/Library/Fonts/Arial.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                return ImageFont.truetype(p, size=size)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def draw_tiny_metrics_top_right(\n",
    "    im: Image.Image, lines: List[str], size: int = 16, margin: int = 10\n",
    ") -> Image.Image:\n",
    "    \"\"\"å³ä¸Šè§’ç²‰è‰²æ–‡å­—ï¼šå§¿æ€æŒ‡æ ‡ï¼ˆä¸ç­–å±•ç‰ˆä¸€è‡´ï¼‰ã€‚\"\"\"\n",
    "    if not lines:\n",
    "        return im\n",
    "    img = im.copy()\n",
    "    d = ImageDraw.Draw(img)\n",
    "    font = _load_font(size)\n",
    "\n",
    "    widths = []\n",
    "    for s in lines:\n",
    "        l, t, r, b = d.textbbox((0, 0), s, font=font)\n",
    "        widths.append(r - l)\n",
    "    wmax = max(widths) if widths else 0\n",
    "\n",
    "    x = img.width - margin - wmax\n",
    "    y = margin\n",
    "    for s in lines:\n",
    "        d.text((x, y), s, fill=HOT_PINK, font=font)\n",
    "        _, _, _, b = d.textbbox((0, 0), s, font=font)\n",
    "        y += int(b * 0.95)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _angle_deg(p1, p2):\n",
    "    if p1 is None or p2 is None:\n",
    "        return None\n",
    "    vx, vy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "    return float(np.degrees(np.arctan2(vy, vx)))\n",
    "\n",
    "\n",
    "def _elbow_angle(shoulder, elbow, wrist):\n",
    "    if None in (shoulder, elbow, wrist):\n",
    "        return None\n",
    "    v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]], float)\n",
    "    v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]], float)\n",
    "    n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if n1 < 1e-5 or n2 < 1e-5:\n",
    "        return None\n",
    "    cosv = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cosv)))\n",
    "\n",
    "\n",
    "def format_metrics(kps: Dict[int, Tuple[float, float] | None]) -> List[str]:\n",
    "    \"\"\"æŠŠå…³é”®ç‚¹å˜æˆå‡ è¡Œå°å­—ï¼ˆä¸ç­–å±•ç‰ˆåŒæ ·ç»“æ„ï¼‰ã€‚\"\"\"\n",
    "    le, re = kps.get(1), kps.get(2)\n",
    "    lsh, rsh = kps.get(5), kps.get(6)\n",
    "    lel, rel = kps.get(7), kps.get(8)\n",
    "    lwr, rwr = kps.get(9), kps.get(10)\n",
    "\n",
    "    fdeg = lambda v: \"â€”\" if v is None else f\"{v:+.1f}Â°\"\n",
    "    fpt = lambda p: \"(0, 0)\" if p is None else f\"({int(p[0])}, {int(p[1])})\"\n",
    "\n",
    "    return [\n",
    "        f\"Head tilt: {fdeg(_angle_deg(re, le))}\",\n",
    "        f\"Shoulder:  {fdeg(_angle_deg(rsh, lsh))}\",\n",
    "        f\"L elbow:   {fdeg(_elbow_angle(lsh, lel, lwr))}\",\n",
    "        f\"R elbow:   {fdeg(_elbow_angle(rsh, rel, rwr))}\",\n",
    "        f\"L wrist:   {fpt(lwr)}\",\n",
    "        f\"R wrist:   {fpt(rwr)}\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def overlay_right_labels(painting: Image.Image, meta: Dict) -> Image.Image:\n",
    "    \"\"\"\n",
    "    åœ¨ç”»ä½œä¸Šå åŠ  3 ä¸ªé»„è‰²çŸ©å½¢æ ‡ç­¾ï¼š\n",
    "    1. ä»·æ ¼ï¼ˆprice_text / auction_price_usdï¼‰\n",
    "    2. å¹´ä»½ï¼ˆyearï¼‰\n",
    "    3. è‰ºæœ¯å®¶ï¼ˆartistï¼‰\n",
    "    å­—ä½“ï¼šCourierï¼Œé»‘å­—ï¼Œé»„è‰²èƒŒæ™¯ï¼Œé¡ºåºå›ºå®šã€‚\n",
    "    \"\"\"\n",
    "    im = painting.convert(\"RGB\").copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    font_big = _load_font(44)\n",
    "    font_small = _load_font(36)\n",
    "\n",
    "    price = (\n",
    "        meta.get(\"price_text\")\n",
    "        or meta.get(\"auction_price_usd\")\n",
    "        or meta.get(\"price\")\n",
    "        or \"â€”\"\n",
    "    )\n",
    "    year = str(meta.get(\"year\") or \"â€”\")\n",
    "    artist = meta.get(\"artist\") or \"artist name\"\n",
    "\n",
    "    lines = [price, year, artist]\n",
    "    fonts = [font_big, font_small, font_big]\n",
    "\n",
    "    margin_x = 24\n",
    "    # ä»ç”»é¢ä¸­éƒ¨ç•¥é ä¸Šå¼€å§‹\n",
    "    y = int(im.height * 0.50)\n",
    "\n",
    "    for text, font in zip(lines, fonts):\n",
    "        # ç”¨ textbboxï¼Œé¿å… textsize æŠ¥é”™\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font)\n",
    "        w, h = r - l, b - t\n",
    "\n",
    "        pad_x, pad_y = 16, 10\n",
    "        box_w, box_h = w + 2 * pad_x, h + 2 * pad_y\n",
    "\n",
    "        x = margin_x\n",
    "        draw.rectangle([x, y, x + box_w, y + box_h], fill=YELLOW)\n",
    "        draw.text((x + pad_x, y + pad_y), text, fill=BLACK, font=font)\n",
    "\n",
    "        y += box_h + 10\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def force_rerun():\n",
    "    try:\n",
    "        st.rerun()\n",
    "    except Exception:\n",
    "        try:\n",
    "            st.experimental_rerun()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =================== YOLO è§†é¢‘å¤„ç†ï¼ˆå®Œå…¨æ²¿ç”¨ç­–å±•éª¨æ¶é£æ ¼ï¼‰ ===================\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "    HAS_YOLO = True\n",
    "except Exception:\n",
    "    HAS_YOLO = False\n",
    "\n",
    "\n",
    "class CuratorialProcessor(VideoProcessorBase):\n",
    "    \"\"\"\n",
    "    ä¸ç­–å±•ç‰ˆä¸€è‡´ï¼š\n",
    "    - ä½¿ç”¨ YOLOv8-Pose ç»˜åˆ¶è“è‰²æ¡† + ç»¿è‰²éª¨æ¶\n",
    "    - å³ä¸Šè§’ç²‰è‰²æ–‡æœ¬æ˜¾ç¤ºå§¿æ€æŒ‡æ ‡\n",
    "    - æ”¯æŒè‡ªåŠ¨é™æ­¢æŠ“æ‹ & æ‰‹åŠ¨æŠ“æ‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        if HAS_YOLO:\n",
    "            try:\n",
    "                local = YOLO_MODEL_PATH\n",
    "                self.model = YOLO(str(local if local.exists() else \"yolov8n-pose.pt\"))\n",
    "            except Exception:\n",
    "                self.model = None\n",
    "\n",
    "        maxlen = max(6, int(MAX_BUF_SEC * FPS_ASSUMED))\n",
    "        self.cx_buf = deque(maxlen=maxlen)\n",
    "        self.cy_buf = deque(maxlen=maxlen)\n",
    "        self.area_buf = deque(maxlen=maxlen)\n",
    "        self.last_stable_ts: Optional[float] = None\n",
    "\n",
    "        self.captured_img: Optional[Image.Image] = None\n",
    "        self.captured_ts: float = 0.0\n",
    "        self.captured_metrics: List[str] = []\n",
    "\n",
    "        self.latest_rgb: Optional[np.ndarray] = None\n",
    "        self.last_metrics_lines: List[str] = []\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _detect_bbox(self, rgb: np.ndarray) -> Optional[Tuple[int, int, int, int]]:\n",
    "        if not self.model:\n",
    "            return None\n",
    "        res = self.model.predict(rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "        if len(res) == 0 or res[0].boxes is None:\n",
    "            return None\n",
    "        b = res[0].boxes.xyxy\n",
    "        if b is None or len(b) == 0:\n",
    "            return None\n",
    "        b = b.cpu().numpy()\n",
    "        areas = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "        i = int(np.argmax(areas))\n",
    "        x1, y1, x2, y2 = b[i].astype(int).tolist()\n",
    "        return x1, y1, x2, y2\n",
    "\n",
    "    def _extract_keypoints(self, res) -> Dict[int, Tuple[float, float] | None]:\n",
    "        kps: Dict[int, Tuple[float, float] | None] = {}\n",
    "        try:\n",
    "            if res and res[0].keypoints is not None and len(res[0].keypoints) > 0:\n",
    "                xy = res[0].keypoints.xy[0].cpu().numpy()\n",
    "                for i in range(xy.shape[0]):\n",
    "                    kps[i] = (float(xy[i, 0]), float(xy[i, 1]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return kps\n",
    "\n",
    "    def _update_stillness(self, bbox, w, h) -> bool:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cx = 0.5 * (x1 + x2)\n",
    "        cy = 0.5 * (y1 + y2)\n",
    "        area = max(1.0, (x2 - x1) * (y2 - y1))\n",
    "        rel_area = area / float(w * h)\n",
    "\n",
    "        if rel_area < MIN_FACE_AREA:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        self.cx_buf.append(cx)\n",
    "        self.cy_buf.append(cy)\n",
    "        self.area_buf.append(rel_area)\n",
    "\n",
    "        need_len = int(STILLNESS_SEC * FPS_ASSUMED * 0.6)\n",
    "        if len(self.cx_buf) < max(3, need_len):\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        stdx = float(np.std(self.cx_buf))\n",
    "        stdy = float(np.std(self.cy_buf))\n",
    "        stda = float(np.std(self.area_buf))\n",
    "        stable_now = (\n",
    "            stdx < MOTION_EPS_CXCY\n",
    "            and stdy < MOTION_EPS_CXCY\n",
    "            and stda < MOTION_EPS_AREA\n",
    "        )\n",
    "\n",
    "        now = time.time()\n",
    "        if stable_now:\n",
    "            if self.last_stable_ts is None:\n",
    "                self.last_stable_ts = now\n",
    "            return (now - self.last_stable_ts) >= STILLNESS_SEC\n",
    "        else:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "    def _stamp_capture(self):\n",
    "        \"\"\"æŠŠå½“å‰å¸§ä¸å½“å‰æŒ‡æ ‡ä½œä¸ºä¸€æ¬¡æŠ“æ‹ã€‚\"\"\"\n",
    "        self.captured_img = Image.fromarray(self.latest_rgb)\n",
    "        self.captured_ts = time.time()\n",
    "        self.captured_metrics = list(self.last_metrics_lines)\n",
    "        self.last_stable_ts = None\n",
    "\n",
    "    def capture_now(self) -> bool:\n",
    "        if self.latest_rgb is None:\n",
    "            return False\n",
    "        with self.lock:\n",
    "            self._stamp_capture()\n",
    "        return True\n",
    "\n",
    "    def recv(self, frame):\n",
    "        import av\n",
    "\n",
    "        img_bgr = frame.to_ndarray(format=\"bgr24\")\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        h, w, _ = img_rgb.shape\n",
    "        self.latest_rgb = img_rgb\n",
    "\n",
    "        if self.model:\n",
    "            res = self.model.predict(img_rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "            plotted = res[0].plot()[:, :, ::-1]  # YOLO è‡ªå¸¦è“æ¡†+ç»¿éª¨æ¶\n",
    "            kps = self._extract_keypoints(res)\n",
    "            lines = format_metrics(kps)\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = lines\n",
    "            pil = Image.fromarray(plotted)\n",
    "            pil = draw_tiny_metrics_top_right(pil, lines, size=16, margin=10)\n",
    "            out_rgb = np.array(pil)\n",
    "        else:\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = [\"(pose model not available)\"]\n",
    "            out_rgb = img_rgb\n",
    "\n",
    "        bbox = self._detect_bbox(img_rgb) if self.model else None\n",
    "        if bbox and self._update_stillness(bbox, w, h):\n",
    "            with self.lock:\n",
    "                if time.time() - self.captured_ts > 0.35:\n",
    "                    self._stamp_capture()\n",
    "\n",
    "        out_bgr = out_rgb[:, :, ::-1]\n",
    "        return av.VideoFrame.from_ndarray(out_bgr, format=\"bgr24\")\n",
    "\n",
    "\n",
    "# =================== Streamlit é¡µé¢ï¼ˆç­–å±•å¸ƒå±€ï¼‰ ===================\n",
    "\n",
    "st.set_page_config(page_title=APP_TITLE, layout=\"wide\")\n",
    "\n",
    "# ç®€å• CSSï¼šä¸¤åˆ—ç»“æ„ï¼Œå·¦æ‘„åƒå¤´ç«–å±å±…ä¸­ï¼Œå³å›¾åƒåŒºåŸŸå›ºå®šå®½åº¦\n",
    "st.markdown(\n",
    "    f\"\"\"\n",
    "<style>\n",
    "section[data-testid=\"stSidebar\"] {{ display: none !important; }}\n",
    "header, footer, [data-testid=\"stToolbar\"] {{ visibility: hidden !important; }}\n",
    ".block-container {{ padding-top: 0.6rem; padding-bottom: 0.6rem; max-width: 1700px; }}\n",
    ".left-col .cam-wrap {{\n",
    "  position: relative;\n",
    "  height: 92vh;\n",
    "  width: 100%;\n",
    "  overflow: hidden;\n",
    "  border-radius: 12px;\n",
    "  background: #111;\n",
    "}}\n",
    ".left-col .cam-wrap video {{\n",
    "  height: 100% !important;\n",
    "  width: auto !important;\n",
    "  object-fit: cover !important;\n",
    "  border-radius: 12px !important;\n",
    "}}\n",
    ".right-col .art-wrap {{\n",
    "  position: relative;\n",
    "  height: 92vh;\n",
    "  max-width: {RIGHT_IMG_MAXW}px;\n",
    "  overflow: hidden;\n",
    "  margin: 0 auto;\n",
    "}}\n",
    ".right-col .art-wrap img {{\n",
    "  display: block;\n",
    "  width: 100% !important;\n",
    "  height: auto !important;\n",
    "}}\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "st.title(APP_TITLE)\n",
    "st.caption(\n",
    "    \"Hold still for ~3â€“5 seconds to auto-capture, or press the button to capture on demand. \"\n",
    "    \"Left: live with pose. Right: matched artwork with tiny pink metrics.\"\n",
    ")\n",
    "\n",
    "# å°è¯•è½»å¾®è‡ªåŠ¨åˆ·æ–°ï¼Œè®©è‡ªåŠ¨æŠ“æ‹æ—¶å³ä¾§è‡ªåŠ¨æ›´æ–°\n",
    "try:\n",
    "    st.autorefresh(interval=700, key=\"ear_auto\", limit=None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "left, right = st.columns([1, 1], gap=\"large\")\n",
    "\n",
    "if \"countdown_target\" not in st.session_state:\n",
    "    st.session_state[\"countdown_target\"] = None\n",
    "\n",
    "if \"last_match\" not in st.session_state:\n",
    "    st.session_state[\"last_match\"] = None  # ä¿å­˜æœ€åä¸€æ¬¡ API è¿”å›å†…å®¹\n",
    "if \"last_metrics\" not in st.session_state:\n",
    "    st.session_state[\"last_metrics\"] = []\n",
    "if \"last_ts\" not in st.session_state:\n",
    "    st.session_state[\"last_ts\"] = 0.0\n",
    "\n",
    "API_URL = DEFAULT_API_URL  # å¦‚æœä½ å¸Œæœ›åœ¨ UI é‡Œå¯ç¼–è¾‘ï¼Œä¹Ÿå¯ä»¥åšæˆ text_input\n",
    "\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"Live\")\n",
    "    st.markdown('<div class=\"left-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"cam-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ctx = webrtc_streamer(\n",
    "        key=\"ear-curatorial-final\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "        video_processor_factory=CuratorialProcessor,\n",
    "        async_processing=True,\n",
    "    )\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /cam-wrap\n",
    "    st.markdown(\"<div style='height:10px'></div>\", unsafe_allow_html=True)\n",
    "\n",
    "    c1, c2 = st.columns(2)\n",
    "    with c1:\n",
    "        if st.button(\"ğŸ“¸ Capture (wait 3s)\", use_container_width=True):\n",
    "            st.session_state[\"countdown_target\"] = time.time() + 3.0\n",
    "    with c2:\n",
    "        if st.button(\"âš¡ Instant Capture\", use_container_width=True):\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"No frame yet, try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "\n",
    "    # å€’è®¡æ—¶é€»è¾‘\n",
    "    if st.session_state[\"countdown_target\"]:\n",
    "        remain = st.session_state[\"countdown_target\"] - time.time()\n",
    "        if remain > 0:\n",
    "            st.info(f\"Capturing in {remain:.1f}sâ€¦ Please hold still.\")\n",
    "        else:\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"No frame yet.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "            st.session_state[\"countdown_target\"] = None\n",
    "            force_rerun()\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /left-col\n",
    "\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"Matched artwork\")\n",
    "    st.markdown('<div class=\"right-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"art-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ph = st.empty()\n",
    "\n",
    "    proc: Optional[CuratorialProcessor] = None\n",
    "    if ctx and ctx.video_processor:\n",
    "        proc = ctx.video_processor\n",
    "\n",
    "    if not proc:\n",
    "        ph.info(\"Initializing cameraâ€¦\")\n",
    "    else:\n",
    "        # ä»å¤„ç†å™¨è¯»å–æœ€æ–°æŠ“æ‹\n",
    "        with proc.lock:\n",
    "            cap_img = proc.captured_img\n",
    "            cap_ts = getattr(proc, \"captured_ts\", 0.0)\n",
    "            cap_metrics = list(getattr(proc, \"captured_metrics\", []))\n",
    "\n",
    "        if cap_img is not None and cap_ts > st.session_state[\"last_ts\"]:\n",
    "            # æ›´æ–°æœ¬åœ°æ—¶é—´æˆ³\n",
    "            st.session_state[\"last_ts\"] = cap_ts\n",
    "            st.session_state[\"last_metrics\"] = cap_metrics\n",
    "\n",
    "            # æŠŠæŠ“æ‹å›¾å‘ç»™åç«¯ /match\n",
    "            buf = io.BytesIO()\n",
    "            cap_img.save(buf, format=\"JPEG\")\n",
    "            buf.seek(0)\n",
    "\n",
    "            files = {\"image\": (\"frame.jpg\", buf.getvalue(), \"image/jpeg\")}\n",
    "            data = {\"museum\": \"local\", \"topk\": 3}\n",
    "\n",
    "            try:\n",
    "                resp = requests.post(API_URL, files=files, data=data, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                payload = resp.json()\n",
    "                st.session_state[\"last_match\"] = payload\n",
    "            except Exception as exc:\n",
    "                st.session_state[\"last_match\"] = {\"error\": str(exc)}\n",
    "\n",
    "            force_rerun()\n",
    "\n",
    "        payload = st.session_state.get(\"last_match\")\n",
    "\n",
    "        if not payload:\n",
    "            ph.info(\"Hold still or press capture to trigger matchingâ€¦\")\n",
    "        elif \"error\" in payload:\n",
    "            ph.error(f\"Error from backend: {payload['error']}\")\n",
    "        else:\n",
    "            results = payload.get(\"results\") or []\n",
    "            if not results:\n",
    "                ph.warning(\"No matches returned from backend.\")\n",
    "            else:\n",
    "                # åªå– Top-1ï¼Œå±•é™ˆæ•ˆæœæ›´å¹²å‡€\n",
    "                top = results[0]\n",
    "                filename = top.get(\"filename\") or top.get(\"file\")\n",
    "\n",
    "                img_path = ensure_image_path(filename or \"\")\n",
    "                if not img_path:\n",
    "                    ph.error(f\"Image file not found for: {filename}\")\n",
    "                else:\n",
    "                    painting = safe_open_image(img_path)\n",
    "                    if painting is None:\n",
    "                        ph.error(f\"Failed to open image: {img_path}\")\n",
    "                    else:\n",
    "                        # ä»æœ¬åœ° CSV æŸ¥è¡¥å…… meta\n",
    "                        meta_row = lookup_meta(str(filename))\n",
    "                        meta = {\n",
    "                            \"artist\": meta_row.get(\"artist\") or top.get(\"artist\") or \"artist name\",\n",
    "                            \"year\": meta_row.get(\"year\") or top.get(\"year\") or \"\",\n",
    "                            \"price_text\": meta_row.get(\"price_text\")\n",
    "                            or meta_row.get(\"auction_price_usd\")\n",
    "                            or \"\",\n",
    "                        }\n",
    "\n",
    "                        painted = overlay_right_labels(painting, meta)\n",
    "\n",
    "                        # æŠŠæŠ“æ‹æ—¶çš„å§¿æ€æŒ‡æ ‡å åˆ°å³ä¸Šè§’ï¼ˆç²‰è‰²æ–‡æœ¬ï¼‰\n",
    "                        metrics = st.session_state.get(\"last_metrics\") or []\n",
    "                        painted = draw_tiny_metrics_top_right(\n",
    "                            painted, metrics, size=16, margin=12\n",
    "                        )\n",
    "\n",
    "                        w = min(RIGHT_IMG_MAXW, painted.width)\n",
    "                        caption = f\"{meta_row.get('title','')} â€” {meta.get('artist','')}\"\n",
    "                        ph.image(painted, caption=caption, width=w)\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /art-wrap\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /right-col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196e902",
   "metadata": {},
   "source": [
    "build_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07443a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Build CLIP embeddings for a museum folder.\n",
    "\n",
    "Layout (per museum):\n",
    "  data/<museum>/\n",
    "    images/                  # *.jpg / *.png ...\n",
    "    embeddings.npy           # (N, D)\n",
    "    embeddings_meta.csv      # at least: filename + optional columns from --meta\n",
    "\n",
    "Usage:\n",
    "  python backend/tools/build_embeddings.py --museum local --meta data/local/portrait_works.csv --overwrite\n",
    "  python backend/tools/build_embeddings.py --all --overwrite\n",
    "\n",
    "Deps:\n",
    "  pip install open_clip_torch torch pillow numpy pandas tqdm\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import open_clip\n",
    "\n",
    "\n",
    "# ------------------------- utils -------------------------\n",
    "\n",
    "def detect_device(device_override: Optional[str] = None) -> torch.device:\n",
    "    if device_override:\n",
    "        return torch.device(device_override)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def list_images(folder: str) -> List[str]:\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tiff\"}\n",
    "    files = []\n",
    "    for name in sorted(os.listdir(folder)):\n",
    "        p = os.path.join(folder, name)\n",
    "        if os.path.isfile(p) and os.path.splitext(name.lower())[1] in exts:\n",
    "            files.append(p)\n",
    "    return files\n",
    "\n",
    "\n",
    "def normalize_rows(x: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12\n",
    "    return x / n\n",
    "\n",
    "\n",
    "def load_external_meta(meta_csv_path: Optional[str]) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Robust CSV reader (BOM-safe, header-trim, filename normalization).\"\"\"\n",
    "    if not meta_csv_path:\n",
    "        return None\n",
    "    if not os.path.exists(meta_csv_path):\n",
    "        print(f\"[WARN] metadata CSV not found: {meta_csv_path}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(meta_csv_path, encoding=\"utf-8-sig\")\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    # accept 'filename' or common variants\n",
    "    if \"filename\" not in df.columns:\n",
    "        alt = None\n",
    "        for c in (\"image\", \"file\"):\n",
    "            if c in df.columns:\n",
    "                alt = c\n",
    "                break\n",
    "        if alt:\n",
    "            df = df.rename(columns={alt: \"filename\"})\n",
    "\n",
    "    if \"filename\" not in df.columns:\n",
    "        raise ValueError(\"External metadata CSV must contain a 'filename' (or 'image'/'file') column.\")\n",
    "\n",
    "    df[\"filename\"] = df[\"filename\"].map(lambda x: os.path.basename(str(x)).strip())\n",
    "    return df\n",
    "\n",
    "\n",
    "def dual_views(preprocess, pil: Image.Image):\n",
    "    \"\"\"full image + center crop (square) as a tiny augmentation ensemble.\"\"\"\n",
    "    # full\n",
    "    im_full = preprocess(pil.convert(\"RGB\"))\n",
    "    # center crop to min side (square), then preprocess again\n",
    "    w, h = pil.size\n",
    "    m = min(w, h)\n",
    "    left = (w - m) // 2\n",
    "    top = (h - m) // 2\n",
    "    pil_square = pil.crop((left, top, left + m, top + m))\n",
    "    im_square = preprocess(pil_square.convert(\"RGB\"))\n",
    "    return im_full, im_square\n",
    "\n",
    "\n",
    "# ------------------------- core -------------------------\n",
    "\n",
    "def build_for_museum(\n",
    "    museum: str,\n",
    "    data_root: str,\n",
    "    model_name: str = \"ViT-B-32\",\n",
    "    pretrained: str = \"openai\",\n",
    "    device: Optional[str] = None,\n",
    "    meta_csv: Optional[str] = None,\n",
    "    overwrite: bool = False,\n",
    "    use_dual_view: bool = True,\n",
    "):\n",
    "    out_dir = os.path.join(data_root, museum)\n",
    "    img_dir = os.path.join(out_dir, \"images\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(img_dir):\n",
    "        print(f\"[WARN] images folder not found: {img_dir}\")\n",
    "        return\n",
    "\n",
    "    files = list_images(img_dir)\n",
    "    if not files:\n",
    "        print(f\"[WARN] no images found in: {img_dir}\")\n",
    "        return\n",
    "\n",
    "    dev = detect_device(device)\n",
    "    print(f\"[INFO] museum={museum} | images={len(files)} | device={dev} | model={model_name}/{pretrained}\")\n",
    "\n",
    "    embed_path = os.path.join(out_dir, \"embeddings.npy\")\n",
    "    meta_path = os.path.join(out_dir, \"embeddings_meta.csv\")\n",
    "\n",
    "    if (os.path.exists(embed_path) or os.path.exists(meta_path)) and not overwrite:\n",
    "        print(f\"[SKIP] embeddings/meta already exist (use --overwrite to rebuild): {out_dir}\")\n",
    "        return\n",
    "\n",
    "    # model\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained, device=dev)\n",
    "    model.eval()\n",
    "\n",
    "    # encode\n",
    "    feats = []\n",
    "    for p in tqdm(files, desc=f\"[{museum}] progress\"):\n",
    "        try:\n",
    "            pil = Image.open(p)\n",
    "        except Exception:\n",
    "            print(f\"[WARN] failed to open image: {p}\")\n",
    "            continue\n",
    "\n",
    "        if use_dual_view:\n",
    "            im1, im2 = dual_views(preprocess, pil)\n",
    "            ims = torch.stack([im1, im2], dim=0).to(dev)\n",
    "        else:\n",
    "            ims = torch.stack([preprocess(pil.convert(\"RGB\"))], dim=0).to(dev)\n",
    "\n",
    "        with torch.no_grad(), torch.autocast(device_type=str(dev), enabled=(dev.type in [\"cuda\", \"mps\"])):\n",
    "            feat = model.encode_image(ims)  # (k, D)\n",
    "            feat = feat.float().cpu().numpy()\n",
    "            feat = feat.mean(axis=0, keepdims=True)  # simple average for dual view\n",
    "        feats.append(feat)\n",
    "\n",
    "    if not feats:\n",
    "        print(f\"[WARN] no features produced for: {museum}\")\n",
    "        return\n",
    "\n",
    "    E = np.concatenate(feats, axis=0)  # (N, D)\n",
    "    E = normalize_rows(E)\n",
    "    np.save(embed_path, E)\n",
    "    print(f\"[OK] {museum}: embeddings={E.shape} â†’ {embed_path}\")\n",
    "\n",
    "    # base meta (filename only)\n",
    "    meta_rows = [{\"filename\": os.path.basename(p)} for p in files]\n",
    "    base_df = pd.DataFrame(meta_rows)\n",
    "\n",
    "    # external meta\n",
    "    ext_df = None\n",
    "    try:\n",
    "        ext_df = load_external_meta(meta_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] failed to read external metadata: {e}\")\n",
    "\n",
    "    merged = base_df.merge(ext_df, on=\"filename\", how=\"left\") if ext_df is not None else base_df\n",
    "\n",
    "    # filename first\n",
    "    cols = [\"filename\"] + [c for c in merged.columns if c != \"filename\"]\n",
    "    merged = merged[cols]\n",
    "    merged.to_csv(meta_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[OK] {museum}: meta rows={len(merged)} â†’ {meta_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--museum\", type=str, help=\"Museum folder name under data/, e.g., local / met\")\n",
    "    parser.add_argument(\"--all\", action=\"store_true\", help=\"Build for all subfolders under data/\")\n",
    "    parser.add_argument(\"--data_root\", type=str, default=\"data\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"ViT-B-32\")\n",
    "    parser.add_argument(\"--pretrained\", type=str, default=\"openai\")\n",
    "    parser.add_argument(\"--device\", type=str, default=None, help=\"cuda / mps / cpu (auto if omitted)\")\n",
    "    parser.add_argument(\"--meta\", type=str, default=None, help=\"Optional external CSV to merge (must have filename)\")\n",
    "    parser.add_argument(\"--overwrite\", action=\"store_true\")\n",
    "    parser.add_argument(\"--no_dual\", action=\"store_true\", help=\"Disable dual-view averaging\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not args.museum and not args.all:\n",
    "        print(\"\\nPlease set --museum=NAME or use --all.\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if args.all:\n",
    "        for m in sorted(d for d in os.listdir(args.data_root)\n",
    "                        if os.path.isdir(os.path.join(args.data_root, d))):\n",
    "            build_for_museum(\n",
    "                museum=m,\n",
    "                data_root=args.data_root,\n",
    "                model_name=args.model,\n",
    "                pretrained=args.pretrained,\n",
    "                device=args.device,\n",
    "                meta_csv=args.meta,\n",
    "                overwrite=args.overwrite,\n",
    "                use_dual_view=not args.no_dual,\n",
    "            )\n",
    "    else:\n",
    "        build_for_museum(\n",
    "            museum=args.museum,\n",
    "            data_root=args.data_root,\n",
    "            model_name=args.model,\n",
    "            pretrained=args.pretrained,\n",
    "            device=args.device,\n",
    "            meta_csv=args.meta,\n",
    "            overwrite=args.overwrite,\n",
    "            use_dual_view=not args.no_dual,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd97022",
   "metadata": {},
   "source": [
    "build_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d9da0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Build pose embeddings for a museum folder.\n",
    "\n",
    "For now this uses the same CLIP encoder as build_embeddings.py,\n",
    "so everything works out-of-the-box. Later you can replace it with a\n",
    "true pose encoder (e.g. from YOLO keypoints).\n",
    "\n",
    "Layout (per museum):\n",
    "  data/<museum>/\n",
    "    images/\n",
    "    pose_embeddings.npy      # (N, D_pose)  (same N as embeddings.npy)\n",
    "\n",
    "Usage example:\n",
    "  python backend/tools/build_pose_embeddings.py --museum mixed --data_root data\n",
    "\n",
    "Deps:\n",
    "  pip install open_clip_torch torch pillow numpy tqdm\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import open_clip\n",
    "\n",
    "\n",
    "def detect_device(device_override: Optional[str] = None) -> torch.device:\n",
    "    if device_override:\n",
    "        return torch.device(device_override)\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def list_images(folder: str) -> List[str]:\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tiff\"}\n",
    "    files = []\n",
    "    for name in sorted(os.listdir(folder)):\n",
    "        p = os.path.join(folder, name)\n",
    "        if os.path.isfile(p) and os.path.splitext(name.lower())[1] in exts:\n",
    "            files.append(p)\n",
    "    return files\n",
    "\n",
    "\n",
    "def normalize_rows(x: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12\n",
    "    return x / n\n",
    "\n",
    "\n",
    "def build_pose_for_museum(\n",
    "    museum: str,\n",
    "    data_root: str = \"data\",\n",
    "    model_name: str = \"ViT-B-32\",\n",
    "    pretrained: str = \"openai\",\n",
    "    device: Optional[str] = None,\n",
    "    overwrite: bool = False,\n",
    "):\n",
    "    base = os.path.join(data_root, museum)\n",
    "    img_dir = os.path.join(base, \"images\")\n",
    "    pose_path = os.path.join(base, \"pose_embeddings.npy\")\n",
    "\n",
    "    if not os.path.isdir(img_dir):\n",
    "        print(f\"[WARN] images folder not found: {img_dir}\")\n",
    "        return\n",
    "\n",
    "    if os.path.exists(pose_path) and not overwrite:\n",
    "        print(f\"[SKIP] {museum}: pose_embeddings already exists ({pose_path})\")\n",
    "        return\n",
    "\n",
    "    files = list_images(img_dir)\n",
    "    if not files:\n",
    "        print(f\"[WARN] no images in {img_dir}\")\n",
    "        return\n",
    "\n",
    "    dev = detect_device(device)\n",
    "    print(f\"[INFO] building pose embeddings for {museum} on {dev}â€¦\")\n",
    "\n",
    "    model, preprocess, _ = open_clip.create_model_and_transforms(\n",
    "        model_name,\n",
    "        pretrained=pretrained,\n",
    "        device=dev,\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    feats = []\n",
    "    for path in tqdm(files, desc=f\"{museum}\"):\n",
    "        try:\n",
    "            pil = Image.open(path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] failed to open {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        im = preprocess(pil).unsqueeze(0).to(dev)\n",
    "        with torch.no_grad(), torch.autocast(device_type=str(dev), enabled=(dev.type in [\"cuda\", \"mps\"])):\n",
    "            feat = model.encode_image(im)\n",
    "            feat = feat.float().cpu().numpy()\n",
    "        feats.append(feat)\n",
    "\n",
    "    if not feats:\n",
    "        print(f\"[WARN] no features produced for: {museum}\")\n",
    "        return\n",
    "\n",
    "    E = np.concatenate(feats, axis=0)  # (N, D)\n",
    "    E = normalize_rows(E)\n",
    "    np.save(pose_path, E)\n",
    "    print(f\"[OK] {museum}: pose_embeddings={E.shape} â†’ {pose_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--museum\", type=str, help=\"Museum folder name under data/, e.g. local / met / mixed\")\n",
    "    parser.add_argument(\"--data_root\", type=str, default=\"data\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"ViT-B-32\")\n",
    "    parser.add_argument(\"--pretrained\", type=str, default=\"openai\")\n",
    "    parser.add_argument(\"--device\", type=str, default=None)\n",
    "    parser.add_argument(\"--overwrite\", action=\"store_true\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not args.museum:\n",
    "        print(\"\\nPlease set --museum=NAME (e.g. --museum=mixed).\\n\")\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    build_pose_for_museum(\n",
    "        museum=args.museum,\n",
    "        data_root=args.data_root,\n",
    "        model_name=args.model,\n",
    "        pretrained=args.pretrained,\n",
    "        device=args.device,\n",
    "        overwrite=args.overwrite,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f14132",
   "metadata": {},
   "source": [
    "config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fa52b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Global configuration for the EAR backend.\n",
    "\n",
    "This file defines a simple Settings model that knows where your\n",
    "data lives (embeddings, metadata CSV, images directory) and which\n",
    "device to use for inference.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "\n",
    "\n",
    "class Settings(BaseModel):\n",
    "    # ä½¿ç”¨æ··åˆç´¢å¼•ï¼šlocal + met\n",
    "    data_root: Path = PROJECT_ROOT / \"data\" / \"mixed\"\n",
    "\n",
    "    # é»˜è®¤â€œé¦†â€åéšä¾¿å†™ï¼Œä½†ç»Ÿä¸€ä¸€ä¸‹\n",
    "    default_museum: str = \"mixed\"\n",
    "\n",
    "    # Device: \"auto\", \"cpu\", \"cuda\", \"mps\"\n",
    "    device: str = \"auto\"\n",
    "\n",
    "\n",
    "    # Device: \"auto\", \"cpu\", \"cuda\", \"mps\"\n",
    "    device: str = \"auto\"\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def embeddings_path(self) -> Path:\n",
    "        return self.data_root / \"embeddings.npy\"\n",
    "\n",
    "    @property\n",
    "    def embeddings_meta_path(self) -> Path:\n",
    "        return self.data_root / \"embeddings_meta.csv\"\n",
    "\n",
    "    @property\n",
    "    def images_dir(self) -> Path:\n",
    "        return self.data_root / \"images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca0011",
   "metadata": {},
   "source": [
    "app_frontend.py (in Chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54844745",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_webrtc import (\n",
    "    WebRtcMode,\n",
    "    RTCConfiguration,\n",
    "    VideoProcessorBase,\n",
    "    webrtc_streamer,\n",
    ")\n",
    "\n",
    "# =================== è·¯å¾„ä¸å¸¸é‡ ===================\n",
    "\n",
    "FRONTEND_DIR = Path(__file__).resolve().parent\n",
    "ROOT_DIR = FRONTEND_DIR.parent\n",
    "\n",
    "# ä½¿ç”¨ global mixed æ•°æ®ï¼ˆlocal + met åˆå¹¶åçš„å…¨å±€ç´¢å¼•ï¼‰\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"mixed\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "# å¯èƒ½å­˜åœ¨çš„ meta CSVï¼šä¼˜å…ˆ mixed çš„ embeddings_meta.csvï¼Œå†å›é€€åˆ° local çš„ csv\n",
    "META_CSV_CANDIDATES = [\n",
    "    DATA_DIR / \"embeddings_meta.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works_enhanced_english.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works.csv\",\n",
    "]\n",
    "\n",
    "# åç«¯ APIï¼ˆéœ€è¦æ—¶ä½ å¯ä»¥æ”¹æˆ HuggingFace åœ°å€ï¼‰\n",
    "DEFAULT_API_URL = \"http://127.0.0.1:8000/match\"\n",
    "\n",
    "APP_TITLE = \"Embodied Aesthetic Reconstruction\"\n",
    "\n",
    "# YOLOv8-Pose æ¨¡å‹è·¯å¾„ï¼ˆæ”¾åœ¨ frontend ç›®å½•ä¸‹ï¼‰\n",
    "YOLO_MODEL_PATH = FRONTEND_DIR / \"yolov8n-pose.pt\"\n",
    "\n",
    "# WebRTC é…ç½®ï¼ˆSafari éœ€è¦ STUNï¼‰\n",
    "RTC_CONFIGURATION = RTCConfiguration(\n",
    "    {\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]}\n",
    ")\n",
    "\n",
    "# Stillness æ£€æµ‹ï¼ˆä¸ç­–å±•ç‰ˆä¸€è‡´ï¼‰\n",
    "STILLNESS_SEC = 3.5\n",
    "MAX_BUF_SEC = 5.0\n",
    "FPS_ASSUMED = 12\n",
    "MOTION_EPS_CXCY = 4.0\n",
    "MOTION_EPS_AREA = 0.03\n",
    "MIN_FACE_AREA = 0.06\n",
    "\n",
    "# é¢œè‰²\n",
    "YELLOW = (255, 235, 59)\n",
    "BLACK = (0, 0, 0)\n",
    "HOT_PINK = (255, 30, 180)\n",
    "\n",
    "# å±•ç¤ºåŒºåŸŸå®½åº¦\n",
    "RIGHT_IMG_MAXW = 900\n",
    "\n",
    "\n",
    "# =================== å°å·¥å…·å‡½æ•° ===================\n",
    "\n",
    "_META_CACHE: Optional[Dict[str, Dict]] = None\n",
    "\n",
    "\n",
    "def load_meta_mapping() -> Dict[str, Dict]:\n",
    "    \"\"\"åŠ è½½ CSVï¼ŒæŒ‰ filename å»ºç«‹ç´¢å¼•ã€‚\"\"\"\n",
    "    global _META_CACHE\n",
    "    if _META_CACHE is not None:\n",
    "        return _META_CACHE\n",
    "\n",
    "    import csv\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    for p in META_CSV_CANDIDATES:\n",
    "        if p.exists():\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                rows = list(csv.DictReader(f))\n",
    "            break\n",
    "\n",
    "    mapping: Dict[str, Dict] = {}\n",
    "    for r in rows:\n",
    "        fname = (\n",
    "            r.get(\"filename\")\n",
    "            or r.get(\"image_path\")\n",
    "            or r.get(\"path\")\n",
    "            or r.get(\"file\")\n",
    "        )\n",
    "        if fname:\n",
    "            mapping[str(fname)] = r\n",
    "\n",
    "    _META_CACHE = mapping\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def lookup_meta(filename: str) -> Dict:\n",
    "    mapping = load_meta_mapping()\n",
    "    return mapping.get(filename, {})\n",
    "\n",
    "\n",
    "def safe_open_image(p: Path) -> Optional[Image.Image]:\n",
    "    try:\n",
    "        return Image.open(p).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_image_path(filename: str) -> Optional[Path]:\n",
    "    \"\"\"æ ¹æ® filename åœ¨ IMAGES_DIR é‡Œæ‰¾åˆ°å›¾ç‰‡ã€‚\"\"\"\n",
    "    if not filename:\n",
    "        return None\n",
    "    p = Path(filename)\n",
    "    if not p.is_absolute():\n",
    "        p = IMAGES_DIR / p\n",
    "    return p if p.exists() else None\n",
    "\n",
    "\n",
    "def _load_font(size: int = 40) -> ImageFont.FreeTypeFont:\n",
    "    \"\"\"ä¼˜å…ˆç”¨ Courier / Courier Newï¼Œæ‰¾ä¸åˆ°å†é€€å› Arial / é»˜è®¤ã€‚\"\"\"\n",
    "    candidates = [\n",
    "        \"/Library/Fonts/Courier New.ttf\",\n",
    "        \"/System/Library/Fonts/Courier.dfont\",\n",
    "        \"/System/Library/Fonts/Supplemental/Courier New.ttf\",\n",
    "        \"/Library/Fonts/Arial.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                return ImageFont.truetype(p, size=size)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def draw_tiny_metrics_top_right(\n",
    "    im: Image.Image, lines: List[str], size: int = 16, margin: int = 10\n",
    ") -> Image.Image:\n",
    "    \"\"\"å³ä¸Šè§’ç²‰è‰²æ–‡å­—ï¼šå§¿æ€æŒ‡æ ‡ï¼ˆä¸ç­–å±•ç‰ˆä¸€è‡´ï¼‰ã€‚\"\"\"\n",
    "    if not lines:\n",
    "        return im\n",
    "    img = im.copy()\n",
    "    d = ImageDraw.Draw(img)\n",
    "    font = _load_font(size)\n",
    "\n",
    "    widths = []\n",
    "    for s in lines:\n",
    "        l, t, r, b = d.textbbox((0, 0), s, font=font)\n",
    "        widths.append(r - l)\n",
    "    wmax = max(widths) if widths else 0\n",
    "\n",
    "    x = img.width - margin - wmax\n",
    "    y = margin\n",
    "    for s in lines:\n",
    "        d.text((x, y), s, fill=HOT_PINK, font=font)\n",
    "        _, _, _, b = d.textbbox((0, 0), s, font=font)\n",
    "        y += int(b * 0.95)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _angle_deg(p1, p2):\n",
    "    if p1 is None or p2 is None:\n",
    "        return None\n",
    "    vx, vy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "    return float(np.degrees(np.arctan2(vy, vx)))\n",
    "\n",
    "\n",
    "def _elbow_angle(shoulder, elbow, wrist):\n",
    "    if None in (shoulder, elbow, wrist):\n",
    "        return None\n",
    "    v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]], float)\n",
    "    v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]], float)\n",
    "    n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if n1 < 1e-5 or n2 < 1e-5:\n",
    "        return None\n",
    "    cosv = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cosv)))\n",
    "\n",
    "\n",
    "def format_metrics(kps: Dict[int, Tuple[float, float] | None]) -> List[str]:\n",
    "    \"\"\"æŠŠå…³é”®ç‚¹å˜æˆå‡ è¡Œå°å­—ï¼ˆä¸ç­–å±•ç‰ˆåŒæ ·ç»“æ„ï¼‰ã€‚\"\"\"\n",
    "    le, re = kps.get(1), kps.get(2)\n",
    "    lsh, rsh = kps.get(5), kps.get(6)\n",
    "    lel, rel = kps.get(7), kps.get(8)\n",
    "    lwr, rwr = kps.get(9), kps.get(10)\n",
    "\n",
    "    fdeg = lambda v: \"â€”\" if v is None else f\"{v:+.1f}Â°\"\n",
    "    fpt = lambda p: \"(0, 0)\" if p is None else f\"({int(p[0])}, {int(p[1])})\"\n",
    "\n",
    "    return [\n",
    "        f\"Head tilt: {fdeg(_angle_deg(re, le))}\",\n",
    "        f\"Shoulder:  {fdeg(_angle_deg(rsh, lsh))}\",\n",
    "        f\"L elbow:   {fdeg(_elbow_angle(lsh, lel, lwr))}\",\n",
    "        f\"R elbow:   {fdeg(_elbow_angle(rsh, rel, rwr))}\",\n",
    "        f\"L wrist:   {fpt(lwr)}\",\n",
    "        f\"R wrist:   {fpt(rwr)}\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def overlay_right_labels(painting: Image.Image, meta: Dict) -> Image.Image:\n",
    "    \"\"\"\n",
    "    åœ¨ç”»ä½œä¸Šå åŠ  3 ä¸ªé»„è‰²çŸ©å½¢æ ‡ç­¾ï¼š\n",
    "    1. ä»·æ ¼ï¼ˆprice_text / auction_price_usdï¼‰\n",
    "    2. å¹´ä»½ï¼ˆyearï¼‰\n",
    "    3. è‰ºæœ¯å®¶ï¼ˆartistï¼‰\n",
    "    å­—ä½“ï¼šCourierï¼Œé»‘å­—ï¼Œé»„è‰²èƒŒæ™¯ï¼Œé¡ºåºå›ºå®šã€‚\n",
    "    \"\"\"\n",
    "    im = painting.convert(\"RGB\").copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    font_big = _load_font(44)\n",
    "    font_small = _load_font(36)\n",
    "\n",
    "    price = (\n",
    "        meta.get(\"price_text\")\n",
    "        or meta.get(\"auction_price_usd\")\n",
    "        or meta.get(\"price\")\n",
    "        or \"â€”\"\n",
    "    )\n",
    "    year = str(meta.get(\"year\") or \"â€”\")\n",
    "    artist = meta.get(\"artist\") or \"artist name\"\n",
    "\n",
    "    lines = [price, year, artist]\n",
    "    fonts = [font_big, font_small, font_big]\n",
    "\n",
    "    margin_x = 24\n",
    "    # ä»ç”»é¢ä¸­éƒ¨ç•¥é ä¸Šå¼€å§‹\n",
    "    y = int(im.height * 0.50)\n",
    "\n",
    "    for text, font in zip(lines, fonts):\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font)\n",
    "        w, h = r - l, b - t\n",
    "\n",
    "        pad_x, pad_y = 16, 10\n",
    "        box_w, box_h = w + 2 * pad_x, h + 2 * pad_y\n",
    "\n",
    "        x = margin_x\n",
    "        draw.rectangle([x, y, x + box_w, y + box_h], fill=YELLOW)\n",
    "        draw.text((x + pad_x, y + pad_y), text, fill=BLACK, font=font)\n",
    "\n",
    "        y += box_h + 10\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def force_rerun():\n",
    "    \"\"\"å…¼å®¹æ–°æ—§ Streamlit çš„ rerunã€‚\"\"\"\n",
    "    try:\n",
    "        st.rerun()\n",
    "    except Exception:\n",
    "        try:\n",
    "            st.experimental_rerun()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =================== YOLO è§†é¢‘å¤„ç†ï¼ˆæ²¿ç”¨ç­–å±•éª¨æ¶é£æ ¼ï¼‰ ===================\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "    HAS_YOLO = True\n",
    "except Exception:\n",
    "    HAS_YOLO = False\n",
    "\n",
    "\n",
    "class CuratorialProcessor(VideoProcessorBase):\n",
    "    \"\"\"\n",
    "    ä¸ç­–å±•ç‰ˆä¸€è‡´ï¼š\n",
    "    - ä½¿ç”¨ YOLOv8-Pose ç»˜åˆ¶è“è‰²æ¡† + ç»¿è‰²éª¨æ¶\n",
    "    - å³ä¸Šè§’ç²‰è‰²æ–‡æœ¬æ˜¾ç¤ºå§¿æ€æŒ‡æ ‡\n",
    "    - æ”¯æŒè‡ªåŠ¨é™æ­¢æŠ“æ‹ & æ‰‹åŠ¨æŠ“æ‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        if HAS_YOLO:\n",
    "            try:\n",
    "                local = YOLO_MODEL_PATH\n",
    "                self.model = YOLO(str(local if local.exists() else \"yolov8n-pose.pt\"))\n",
    "            except Exception:\n",
    "                self.model = None\n",
    "\n",
    "        maxlen = max(6, int(MAX_BUF_SEC * FPS_ASSUMED))\n",
    "        self.cx_buf = deque(maxlen=maxlen)\n",
    "        self.cy_buf = deque(maxlen=maxlen)\n",
    "        self.area_buf = deque(maxlen=maxlen)\n",
    "        self.last_stable_ts: Optional[float] = None\n",
    "\n",
    "        self.captured_img: Optional[Image.Image] = None\n",
    "        self.captured_ts: float = 0.0\n",
    "        self.captured_metrics: List[str] = []\n",
    "\n",
    "        self.latest_rgb: Optional[np.ndarray] = None\n",
    "        self.last_metrics_lines: List[str] = []\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _detect_bbox(self, rgb: np.ndarray) -> Optional[Tuple[int, int, int, int]]:\n",
    "        if not self.model:\n",
    "            return None\n",
    "        res = self.model.predict(rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "        if len(res) == 0 or res[0].boxes is None:\n",
    "            return None\n",
    "        b = res[0].boxes.xyxy\n",
    "        if b is None or len(b) == 0:\n",
    "            return None\n",
    "        b = b.cpu().numpy()\n",
    "        areas = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "        i = int(np.argmax(areas))\n",
    "        x1, y1, x2, y2 = b[i].astype(int).tolist()\n",
    "        return x1, y1, x2, y2\n",
    "\n",
    "    def _extract_keypoints(self, res) -> Dict[int, Tuple[float, float] | None]:\n",
    "        kps: Dict[int, Tuple[float, float] | None] = {}\n",
    "        try:\n",
    "            if res and res[0].keypoints is not None and len(res[0].keypoints) > 0:\n",
    "                xy = res[0].keypoints.xy[0].cpu().numpy()\n",
    "                for i in range(xy.shape[0]):\n",
    "                    kps[i] = (float(xy[i, 0]), float(xy[i, 1]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return kps\n",
    "\n",
    "    def _update_stillness(self, bbox, w, h) -> bool:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cx = 0.5 * (x1 + x2)\n",
    "        cy = 0.5 * (y1 + y2)\n",
    "        area = max(1.0, (x2 - x1) * (y2 - y1))\n",
    "        rel_area = area / float(w * h)\n",
    "\n",
    "        if rel_area < MIN_FACE_AREA:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        self.cx_buf.append(cx)\n",
    "        self.cy_buf.append(cy)\n",
    "        self.area_buf.append(rel_area)\n",
    "\n",
    "        need_len = int(STILLNESS_SEC * FPS_ASSUMED * 0.6)\n",
    "        if len(self.cx_buf) < max(3, need_len):\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        stdx = float(np.std(self.cx_buf))\n",
    "        stdy = float(np.std(self.cy_buf))\n",
    "        stda = float(np.std(self.area_buf))\n",
    "        stable_now = (\n",
    "            stdx < MOTION_EPS_CXCY\n",
    "            and stdy < MOTION_EPS_CXCY\n",
    "            and stda < MOTION_EPS_AREA\n",
    "        )\n",
    "\n",
    "        now = time.time()\n",
    "        if stable_now:\n",
    "            if self.last_stable_ts is None:\n",
    "                self.last_stable_ts = now\n",
    "            return (now - self.last_stable_ts) >= STILLNESS_SEC\n",
    "        else:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "    def _stamp_capture(self):\n",
    "        \"\"\"æŠŠå½“å‰å¸§ä¸å½“å‰æŒ‡æ ‡ä½œä¸ºä¸€æ¬¡æŠ“æ‹ã€‚\"\"\"\n",
    "        self.captured_img = Image.fromarray(self.latest_rgb)\n",
    "        self.captured_ts = time.time()\n",
    "        self.captured_metrics = list(self.last_metrics_lines)\n",
    "        self.last_stable_ts = None\n",
    "\n",
    "    def capture_now(self) -> bool:\n",
    "        if self.latest_rgb is None:\n",
    "            return False\n",
    "        with self.lock:\n",
    "            self._stamp_capture()\n",
    "        return True\n",
    "\n",
    "    def recv(self, frame):\n",
    "        import av\n",
    "\n",
    "        img_bgr = frame.to_ndarray(format=\"bgr24\")\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        h, w, _ = img_rgb.shape\n",
    "        self.latest_rgb = img_rgb\n",
    "\n",
    "        if self.model:\n",
    "            res = self.model.predict(img_rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "            plotted = res[0].plot()[:, :, ::-1]  # YOLO è‡ªå¸¦è“æ¡†+ç»¿éª¨æ¶\n",
    "            kps = self._extract_keypoints(res)\n",
    "            lines = format_metrics(kps)\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = lines\n",
    "            pil = Image.fromarray(plotted)\n",
    "            pil = draw_tiny_metrics_top_right(pil, lines, size=16, margin=10)\n",
    "            out_rgb = np.array(pil)\n",
    "        else:\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = [\"(pose model not available)\"]\n",
    "            out_rgb = img_rgb\n",
    "\n",
    "        bbox = self._detect_bbox(img_rgb) if self.model else None\n",
    "        if bbox and self._update_stillness(bbox, w, h):\n",
    "            with self.lock:\n",
    "                if time.time() - self.captured_ts > 0.35:\n",
    "                    self._stamp_capture()\n",
    "\n",
    "        out_bgr = out_rgb[:, :, ::-1]\n",
    "        return av.VideoFrame.from_ndarray(out_bgr, format=\"bgr24\")\n",
    "\n",
    "\n",
    "# =================== Streamlit é¡µé¢ï¼ˆç­–å±•å¸ƒå±€ï¼‰ ===================\n",
    "\n",
    "st.set_page_config(page_title=APP_TITLE, layout=\"wide\")\n",
    "\n",
    "# ç®€å• CSSï¼šä¸¤åˆ—ç»“æ„ï¼Œå·¦æ‘„åƒå¤´ç«–å±å±…ä¸­ï¼Œå³å›¾åƒåŒºåŸŸå›ºå®šå®½åº¦\n",
    "st.markdown(\n",
    "    f\"\"\"\n",
    "<style>\n",
    "section[data-testid=\"stSidebar\"] {{ display: none !important; }}\n",
    "header, footer, [data-testid=\"stToolbar\"] {{ visibility: hidden !important; }}\n",
    ".block-container {{ padding-top: 0.6rem; padding-bottom: 0.6rem; max-width: 1700px; }}\n",
    ".left-col .cam-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;\n",
    "  width: 100%;\n",
    "  overflow: hidden;\n",
    "  border-radius: 12px;\n",
    "  background: #111;\n",
    "}}\n",
    ".left-col .cam-wrap video {{\n",
    "  height: 100% !important;\n",
    "  width: auto !important;\n",
    "  object-fit: cover !important;\n",
    "  border-radius: 12px !important;\n",
    "}}\n",
    ".right-col .art-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;\n",
    "  max-width: {RIGHT_IMG_MAXW}px;\n",
    "  overflow: hidden;\n",
    "  margin: 0 auto;\n",
    "}}\n",
    ".right-col .art-wrap img {{\n",
    "  display: block;\n",
    "  width: 100% !important;\n",
    "  height: 100% !important;\n",
    "  object-fit: cover !important;\n",
    "}}\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "st.title(APP_TITLE)\n",
    "st.caption(\n",
    "    \"Hold still for ~3â€“5 seconds to auto-capture, or press the button to capture on demand. \"\n",
    "    \"Left: live with pose. Right: matched artwork with tiny pink metrics.\"\n",
    ")\n",
    "\n",
    "# è½»å¾®è‡ªåŠ¨åˆ·æ–°ï¼Œè®©è‡ªåŠ¨æŠ“æ‹æ—¶å³ä¾§è‡ªåŠ¨æ›´æ–°\n",
    "try:\n",
    "    st.autorefresh(interval=700, key=\"ear_auto\", limit=None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "left, right = st.columns([1, 1], gap=\"large\")\n",
    "\n",
    "if \"countdown_target\" not in st.session_state:\n",
    "    st.session_state[\"countdown_target\"] = None\n",
    "\n",
    "if \"last_match\" not in st.session_state:\n",
    "    st.session_state[\"last_match\"] = None  # ä¿å­˜æœ€åä¸€æ¬¡ API è¿”å›å†…å®¹\n",
    "if \"last_metrics\" not in st.session_state:\n",
    "    st.session_state[\"last_metrics\"] = []\n",
    "if \"last_ts\" not in st.session_state:\n",
    "    st.session_state[\"last_ts\"] = 0.0\n",
    "\n",
    "API_URL = DEFAULT_API_URL  # å¦‚æœä½ å¸Œæœ›åœ¨ UI é‡Œå¯ç¼–è¾‘ï¼Œä¹Ÿå¯ä»¥åšæˆ text_input\n",
    "\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"Live\")\n",
    "    st.markdown('<div class=\"left-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"cam-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ctx = webrtc_streamer(\n",
    "        key=\"ear-curatorial-final\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "        video_processor_factory=CuratorialProcessor,\n",
    "        async_processing=True,\n",
    "    )\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /cam-wrap\n",
    "    st.markdown(\"<div style='height:10px'></div>\", unsafe_allow_html=True)\n",
    "\n",
    "    c1, c2 = st.columns(2)\n",
    "    with c1:\n",
    "        if st.button(\"ğŸ“¸ Capture (wait 3s)\", use_container_width=True):\n",
    "            st.session_state[\"countdown_target\"] = time.time() + 3.0\n",
    "    with c2:\n",
    "        if st.button(\"âš¡ Instant Capture\", use_container_width=True):\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\n",
    "                    \"Captured.\" if ok else \"No frame yet, try again.\",\n",
    "                    icon=\"âœ…\" if ok else \"âš ï¸\",\n",
    "                )\n",
    "\n",
    "    # å€’è®¡æ—¶é€»è¾‘\n",
    "    if st.session_state[\"countdown_target\"]:\n",
    "        remain = st.session_state[\"countdown_target\"] - time.time()\n",
    "        if remain > 0:\n",
    "            st.info(f\"Capturing in {remain:.1f}sâ€¦ Please hold still.\")\n",
    "        else:\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\n",
    "                    \"Captured.\" if ok else \"No frame yet.\",\n",
    "                    icon=\"âœ…\" if ok else \"âš ï¸\",\n",
    "                )\n",
    "            st.session_state[\"countdown_target\"] = None\n",
    "            force_rerun()\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /left-col\n",
    "\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"Matched artwork\")\n",
    "    st.markdown('<div class=\"right-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"art-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ph = st.empty()\n",
    "\n",
    "    proc: Optional[CuratorialProcessor] = None\n",
    "    if ctx and ctx.video_processor:\n",
    "        proc = ctx.video_processor\n",
    "\n",
    "    if not proc:\n",
    "        ph.info(\"Initializing cameraâ€¦\")\n",
    "    else:\n",
    "        # ä»å¤„ç†å™¨è¯»å–æœ€æ–°æŠ“æ‹\n",
    "        with proc.lock:\n",
    "            cap_img = proc.captured_img\n",
    "            cap_ts = getattr(proc, \"captured_ts\", 0.0)\n",
    "            cap_metrics = list(getattr(proc, \"captured_metrics\", []))\n",
    "\n",
    "        if cap_img is not None and cap_ts > st.session_state[\"last_ts\"]:\n",
    "            # æ›´æ–°æœ¬åœ°æ—¶é—´æˆ³\n",
    "            st.session_state[\"last_ts\"] = cap_ts\n",
    "            st.session_state[\"last_metrics\"] = cap_metrics\n",
    "\n",
    "            # æŠŠæŠ“æ‹å›¾å‘ç»™åç«¯ /match\n",
    "            buf = io.BytesIO()\n",
    "            cap_img.save(buf, format=\"JPEG\")\n",
    "            buf.seek(0)\n",
    "\n",
    "            files = {\"image\": (\"frame.jpg\", buf.getvalue(), \"image/jpeg\")}\n",
    "            # ä½¿ç”¨ global mixed indexï¼Œä¸€æ¬¡å– Top-3ï¼Œä½†åªå±•ç¤º Top-1\n",
    "            data = {\"museum\": \"mixed\", \"topk\": 3}\n",
    "\n",
    "            try:\n",
    "                resp = requests.post(API_URL, files=files, data=data, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                payload = resp.json()\n",
    "                st.session_state[\"last_match\"] = payload\n",
    "            except Exception as exc:\n",
    "                st.session_state[\"last_match\"] = {\"error\": str(exc)}\n",
    "\n",
    "            force_rerun()\n",
    "\n",
    "        payload = st.session_state.get(\"last_match\")\n",
    "\n",
    "        if not payload:\n",
    "            ph.info(\"Hold still or press capture to trigger matchingâ€¦\")\n",
    "        elif \"error\" in payload:\n",
    "            ph.error(f\"Error from backend: {payload['error']}\")\n",
    "        else:\n",
    "            results = payload.get(\"results\") or []\n",
    "            if not results:\n",
    "                ph.warning(\"No matches returned from backend.\")\n",
    "            else:\n",
    "                # åªå– Top-1ï¼Œå±•é™ˆæ•ˆæœæ›´å¹²å‡€\n",
    "                top = results[0]\n",
    "                filename = top.get(\"filename\") or top.get(\"file\")\n",
    "\n",
    "                img_path = ensure_image_path(filename or \"\")\n",
    "                if not img_path:\n",
    "                    ph.error(f\"Image file not found for: {filename}\")\n",
    "                else:\n",
    "                    painting = safe_open_image(img_path)\n",
    "                    if painting is None:\n",
    "                        ph.error(f\"Failed to open image: {img_path}\")\n",
    "                    else:\n",
    "                        # ä» CSV æŸ¥è¡¥å…… meta\n",
    "                        meta_row = lookup_meta(str(filename))\n",
    "                        meta = {\n",
    "                            \"artist\": meta_row.get(\"artist\")\n",
    "                            or top.get(\"artist\")\n",
    "                            or \"artist name\",\n",
    "                            \"year\": meta_row.get(\"year\") or top.get(\"year\") or \"\",\n",
    "                            \"price_text\": meta_row.get(\"price_text\")\n",
    "                            or meta_row.get(\"auction_price_usd\")\n",
    "                            or \"\",\n",
    "                        }\n",
    "\n",
    "                        painted = overlay_right_labels(painting, meta)\n",
    "\n",
    "                        # æŠŠæŠ“æ‹æ—¶çš„å§¿æ€æŒ‡æ ‡å åˆ°å³ä¸Šè§’ï¼ˆç²‰è‰²æ–‡æœ¬ï¼‰\n",
    "                        metrics = st.session_state.get(\"last_metrics\") or []\n",
    "                        painted = draw_tiny_metrics_top_right(\n",
    "                            painted, metrics, size=16, margin=12\n",
    "                        )\n",
    "\n",
    "                        w = min(RIGHT_IMG_MAXW, painted.width)\n",
    "                        caption = f\"{meta_row.get('title','')} â€” {meta.get('artist','')}\"\n",
    "                        ph.image(painted, caption=caption, width=w)\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /art-wrap\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)  # /right-col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5ebd4",
   "metadata": {},
   "source": [
    "app_fronted.py(in English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51baaa1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_webrtc import (\n",
    "    WebRtcMode,\n",
    "    RTCConfiguration,\n",
    "    VideoProcessorBase,\n",
    "    webrtc_streamer,\n",
    ")\n",
    "\n",
    "# =================== PATHS & CONSTANTS ===================\n",
    "\n",
    "FRONTEND_DIR = Path(__file__).resolve().parent\n",
    "ROOT_DIR = FRONTEND_DIR.parent\n",
    "\n",
    "# Use GLOBAL MIXED INDEX (local + met merged)\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"mixed\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "# Meta CSV candidates (fallback to local)\n",
    "META_CSV_CANDIDATES = [\n",
    "    DATA_DIR / \"embeddings_meta.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works_enhanced_english.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works.csv\",\n",
    "]\n",
    "\n",
    "# Backend API\n",
    "DEFAULT_API_URL = \"http://127.0.0.1:8000/match\"\n",
    "\n",
    "APP_TITLE = \"Embodied Aesthetic Reconstruction\"\n",
    "\n",
    "# YOLO model path\n",
    "YOLO_MODEL_PATH = FRONTEND_DIR / \"yolov8n-pose.pt\"\n",
    "\n",
    "# WebRTC ICE config (Safari requires STUN)\n",
    "RTC_CONFIGURATION = RTCConfiguration(\n",
    "    {\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]}\n",
    ")\n",
    "\n",
    "# Stillness & motion detection\n",
    "STILLNESS_SEC = 3.5\n",
    "MAX_BUF_SEC = 5.0\n",
    "FPS_ASSUMED = 12\n",
    "MOTION_EPS_CXCY = 4.0\n",
    "MOTION_EPS_AREA = 0.03\n",
    "MIN_FACE_AREA = 0.06\n",
    "\n",
    "# Colors\n",
    "YELLOW = (255, 235, 59)\n",
    "BLACK = (0, 0, 0)\n",
    "HOT_PINK = (255, 30, 180)\n",
    "\n",
    "RIGHT_IMG_MAXW = 900\n",
    "\n",
    "\n",
    "# =================== UTILITIES ===================\n",
    "\n",
    "_META_CACHE: Optional[Dict[str, Dict]] = None\n",
    "\n",
    "\n",
    "def load_meta_mapping() -> Dict[str, Dict]:\n",
    "    \"\"\"Load CSV â†’ filename â†’ metadata mapping.\"\"\"\n",
    "    global _META_CACHE\n",
    "    if _META_CACHE is not None:\n",
    "        return _META_CACHE\n",
    "\n",
    "    import csv\n",
    "\n",
    "    rows = []\n",
    "    for p in META_CSV_CANDIDATES:\n",
    "        if p.exists():\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                rows = list(csv.DictReader(f))\n",
    "            break\n",
    "\n",
    "    mapping = {}\n",
    "    for r in rows:\n",
    "        fname = (\n",
    "            r.get(\"filename\")\n",
    "            or r.get(\"image_path\")\n",
    "            or r.get(\"path\")\n",
    "            or r.get(\"file\")\n",
    "        )\n",
    "        if fname:\n",
    "            mapping[str(fname)] = r\n",
    "\n",
    "    _META_CACHE = mapping\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def lookup_meta(filename: str) -> Dict:\n",
    "    mapping = load_meta_mapping()\n",
    "    return mapping.get(filename, {})\n",
    "\n",
    "\n",
    "def safe_open_image(p: Path) -> Optional[Image.Image]:\n",
    "    try:\n",
    "        return Image.open(p).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_image_path(filename: str) -> Optional[Path]:\n",
    "    if not filename:\n",
    "        return None\n",
    "    p = Path(filename)\n",
    "    if not p.is_absolute():\n",
    "        p = IMAGES_DIR / p\n",
    "    return p if p.exists() else None\n",
    "\n",
    "\n",
    "def _load_font(size: int = 40):\n",
    "    \"\"\"Try Courier â†’ Arial â†’ default.\"\"\"\n",
    "    candidates = [\n",
    "        \"/Library/Fonts/Courier New.ttf\",\n",
    "        \"/System/Library/Fonts/Courier.dfont\",\n",
    "        \"/System/Library/Fonts/Supplemental/Courier New.ttf\",\n",
    "        \"/Library/Fonts/Arial.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                return ImageFont.truetype(p, size=size)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def draw_tiny_metrics_top_right(im: Image.Image, lines: List[str], size=16, margin=10):\n",
    "    \"\"\"Small pink pose metrics in top-right corner of image.\"\"\"\n",
    "    if not lines:\n",
    "        return im\n",
    "    img = im.copy()\n",
    "    d = ImageDraw.Draw(img)\n",
    "    font = _load_font(size)\n",
    "\n",
    "    widths = []\n",
    "    for s in lines:\n",
    "        l, t, r, b = d.textbbox((0, 0), s, font=font)\n",
    "        widths.append(r - l)\n",
    "    wmax = max(widths) if widths else 0\n",
    "\n",
    "    x = img.width - margin - wmax\n",
    "    y = margin\n",
    "\n",
    "    for s in lines:\n",
    "        d.text((x, y), s, fill=HOT_PINK, font=font)\n",
    "        _, _, _, b = d.textbbox((0, 0), s, font=font)\n",
    "        y += int(b * 0.95)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def _angle_deg(p1, p2):\n",
    "    if p1 is None or p2 is None:\n",
    "        return None\n",
    "    vx, vy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "    return float(np.degrees(np.arctan2(vy, vx)))\n",
    "\n",
    "\n",
    "def _elbow_angle(shoulder, elbow, wrist):\n",
    "    if None in (shoulder, elbow, wrist):\n",
    "        return None\n",
    "    v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]], float)\n",
    "    v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]], float)\n",
    "    n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if n1 < 1e-5 or n2 < 1e-5:\n",
    "        return None\n",
    "    cosv = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cosv)))\n",
    "\n",
    "\n",
    "def format_metrics(kps: Dict[int, Tuple[float, float] | None]) -> List[str]:\n",
    "    le, re = kps.get(1), kps.get(2)\n",
    "    lsh, rsh = kps.get(5), kps.get(6)\n",
    "    lel, rel = kps.get(7), kps.get(8)\n",
    "    lwr, rwr = kps.get(9), kps.get(10)\n",
    "\n",
    "    fdeg = lambda v: \"â€”\" if v is None else f\"{v:+.1f}Â°\"\n",
    "    fpt = lambda p: \"(0, 0)\" if p is None else f\"({int(p[0])}, {int(p[1])})\"\n",
    "\n",
    "    return [\n",
    "        f\"Head tilt: {fdeg(_angle_deg(re, le))}\",\n",
    "        f\"Shoulder:  {fdeg(_angle_deg(rsh, lsh))}\",\n",
    "        f\"L elbow:   {fdeg(_elbow_angle(lsh, lel, lwr))}\",\n",
    "        f\"R elbow:   {fdeg(_elbow_angle(rsh, rel, rwr))}\",\n",
    "        f\"L wrist:   {fpt(lwr)}\",\n",
    "        f\"R wrist:   {fpt(rwr)}\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def overlay_right_labels(painting: Image.Image, meta: Dict) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Exhibition-style yellow labels:\n",
    "    1. Price\n",
    "    2. Year\n",
    "    3. Artist\n",
    "    \"\"\"\n",
    "    im = painting.convert(\"RGB\").copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    font_big = _load_font(44)\n",
    "    font_small = _load_font(36)\n",
    "\n",
    "    price = (\n",
    "        meta.get(\"price_text\")\n",
    "        or meta.get(\"auction_price_usd\")\n",
    "        or meta.get(\"price\")\n",
    "        or \"â€”\"\n",
    "    )\n",
    "    year = str(meta.get(\"year\") or \"â€”\")\n",
    "    artist = meta.get(\"artist\") or \"artist name\"\n",
    "\n",
    "    lines = [price, year, artist]\n",
    "    fonts = [font_big, font_small, font_big]\n",
    "\n",
    "    margin_x = 24\n",
    "    y = int(im.height * 0.50)\n",
    "\n",
    "    for text, font in zip(lines, fonts):\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font)\n",
    "        w, h = r - l, b - t\n",
    "\n",
    "        pad_x, pad_y = 16, 10\n",
    "        box_w, box_h = w + pad_x * 2, h + pad_y * 2\n",
    "\n",
    "        x = margin_x\n",
    "        draw.rectangle([x, y, x + box_w, y + box_h], fill=YELLOW)\n",
    "        draw.text((x + pad_x, y + pad_y), text, fill=BLACK, font=font)\n",
    "\n",
    "        y += box_h + 10\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def force_rerun():\n",
    "    \"\"\"Compatible with new/old versions of Streamlit.\"\"\"\n",
    "    try:\n",
    "        st.rerun()\n",
    "    except Exception:\n",
    "        try:\n",
    "            st.experimental_rerun()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =================== YOLO VIDEO PROCESSOR ===================\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    HAS_YOLO = True\n",
    "except Exception:\n",
    "    HAS_YOLO = False\n",
    "\n",
    "\n",
    "class CuratorialProcessor(VideoProcessorBase):\n",
    "    \"\"\"\n",
    "    YOLOv8-Pose:\n",
    "    - blue bbox\n",
    "    - green skeleton\n",
    "    - pink tiny pose metrics\n",
    "    - stillness-based auto-capture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        if HAS_YOLO:\n",
    "            try:\n",
    "                local = YOLO_MODEL_PATH\n",
    "                self.model = YOLO(str(local if local.exists() else \"yolov8n-pose.pt\"))\n",
    "            except Exception:\n",
    "                self.model = None\n",
    "\n",
    "        maxlen = max(6, int(MAX_BUF_SEC * FPS_ASSUMED))\n",
    "        self.cx_buf = deque(maxlen=maxlen)\n",
    "        self.cy_buf = deque(maxlen=maxlen)\n",
    "        self.area_buf = deque(maxlen=maxlen)\n",
    "        self.last_stable_ts: Optional[float] = None\n",
    "\n",
    "        self.captured_img: Optional[Image.Image] = None\n",
    "        self.captured_ts: float = 0.0\n",
    "        self.captured_metrics: List[str] = []\n",
    "\n",
    "        self.latest_rgb: Optional[np.ndarray] = None\n",
    "        self.last_metrics_lines: List[str] = []\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _detect_bbox(self, rgb):\n",
    "        if not self.model:\n",
    "            return None\n",
    "        res = self.model.predict(rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "        if len(res) == 0 or res[0].boxes is None:\n",
    "            return None\n",
    "        b = res[0].boxes.xyxy\n",
    "        if b is None or len(b) == 0:\n",
    "            return None\n",
    "        b = b.cpu().numpy()\n",
    "        areas = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "        i = int(np.argmax(areas))\n",
    "        return tuple(b[i].astype(int).tolist())\n",
    "\n",
    "    def _extract_keypoints(self, res):\n",
    "        kps = {}\n",
    "        try:\n",
    "            if res and res[0].keypoints is not None and len(res[0].keypoints) > 0:\n",
    "                xy = res[0].keypoints.xy[0].cpu().numpy()\n",
    "                for i in range(xy.shape[0]):\n",
    "                    kps[i] = (float(xy[i, 0]), float(xy[i, 1]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return kps\n",
    "\n",
    "    def _update_stillness(self, bbox, w, h) -> bool:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cx = 0.5 * (x1 + x2)\n",
    "        cy = 0.5 * (y1 + y2)\n",
    "        area = max(1.0, (x2 - x1) * (y2 - y1))\n",
    "        rel_area = area / (w * h)\n",
    "\n",
    "        if rel_area < MIN_FACE_AREA:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        self.cx_buf.append(cx)\n",
    "        self.cy_buf.append(cy)\n",
    "        self.area_buf.append(rel_area)\n",
    "\n",
    "        need_len = int(STILLNESS_SEC * FPS_ASSUMED * 0.6)\n",
    "        if len(self.cx_buf) < max(3, need_len):\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        stdx = float(np.std(self.cx_buf))\n",
    "        stdy = float(np.std(self.cy_buf))\n",
    "        stda = float(np.std(self.area_buf))\n",
    "\n",
    "        stable = (\n",
    "            stdx < MOTION_EPS_CXCY\n",
    "            and stdy < MOTION_EPS_CXCY\n",
    "            and stda < MOTION_EPS_AREA\n",
    "        )\n",
    "\n",
    "        now = time.time()\n",
    "        if stable:\n",
    "            if self.last_stable_ts is None:\n",
    "                self.last_stable_ts = now\n",
    "            return (now - self.last_stable_ts) >= STILLNESS_SEC\n",
    "        else:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "    def _stamp_capture(self):\n",
    "        self.captured_img = Image.fromarray(self.latest_rgb)\n",
    "        self.captured_ts = time.time()\n",
    "        self.captured_metrics = list(self.last_metrics_lines)\n",
    "        self.last_stable_ts = None\n",
    "\n",
    "    def capture_now(self) -> bool:\n",
    "        if self.latest_rgb is None:\n",
    "            return False\n",
    "        with self.lock:\n",
    "            self._stamp_capture()\n",
    "        return True\n",
    "\n",
    "    def recv(self, frame):\n",
    "        import av\n",
    "\n",
    "        img_bgr = frame.to_ndarray(format=\"bgr24\")\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        h, w, _ = img_rgb.shape\n",
    "        self.latest_rgb = img_rgb\n",
    "\n",
    "        if self.model:\n",
    "            res = self.model.predict(img_rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "            plotted = res[0].plot()[:, :, ::-1]\n",
    "            kps = self._extract_keypoints(res)\n",
    "            lines = format_metrics(kps)\n",
    "\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = lines\n",
    "\n",
    "            pil = Image.fromarray(plotted)\n",
    "            pil = draw_tiny_metrics_top_right(pil, lines, size=16, margin=10)\n",
    "            out_rgb = np.array(pil)\n",
    "        else:\n",
    "            out_rgb = img_rgb\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = [\"(pose model unavailable)\"]\n",
    "\n",
    "        bbox = self._detect_bbox(img_rgb) if self.model else None\n",
    "        if bbox and self._update_stillness(bbox, w, h):\n",
    "            with self.lock:\n",
    "                if time.time() - self.captured_ts > 0.35:\n",
    "                    self._stamp_capture()\n",
    "\n",
    "        out_bgr = out_rgb[:, :, ::-1]\n",
    "        return av.VideoFrame.from_ndarray(out_bgr, format=\"bgr24\")\n",
    "\n",
    "\n",
    "# =================== PAGE LAYOUT ===================\n",
    "\n",
    "st.set_page_config(page_title=APP_TITLE, layout=\"wide\")\n",
    "\n",
    "st.markdown(\n",
    "    f\"\"\"\n",
    "<style>\n",
    "section[data-testid=\"stSidebar\"] {{ display: none !important; }}\n",
    "header, footer, [data-testid=\"stToolbar\"] {{ visibility: hidden !important; }}\n",
    ".block-container {{ padding-top: 0.6rem; padding-bottom: 0.6rem; max-width: 1700px; }}\n",
    "\n",
    ".left-col .cam-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;               /* was 92vh */\n",
    "  width: 100%;\n",
    "  overflow: hidden;\n",
    "  border-radius: 12px;\n",
    "  background: #111;\n",
    "}}\n",
    ".left-col .cam-wrap video {{\n",
    "  height: 100% !important;\n",
    "  width: auto !important;\n",
    "  object-fit: cover !important;\n",
    "  border-radius: 12px !important;\n",
    "}}\n",
    "\n",
    ".right-col .art-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;               /* was 92vh */\n",
    "  max-width: {RIGHT_IMG_MAXW}px;\n",
    "  overflow: hidden;\n",
    "  margin: 0 auto;\n",
    "}}\n",
    ".right-col .art-wrap img {{\n",
    "  display: block;\n",
    "  width: 100% !important;\n",
    "  height: 100% !important;\n",
    "  object-fit: cover !important;\n",
    "}}\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "st.title(APP_TITLE)\n",
    "st.caption(\n",
    "    \"Hold still for ~3â€“5 seconds to auto-capture, or press the button to capture manually. \"\n",
    "    \"Left: live video with pose. Right: matched artwork with tiny pink metrics.\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    st.autorefresh(interval=700, key=\"auto\", limit=None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "left, right = st.columns([1, 1], gap=\"large\")\n",
    "\n",
    "if \"countdown_target\" not in st.session_state:\n",
    "    st.session_state[\"countdown_target\"] = None\n",
    "if \"last_match\" not in st.session_state:\n",
    "    st.session_state[\"last_match\"] = None\n",
    "if \"last_metrics\" not in st.session_state:\n",
    "    st.session_state[\"last_metrics\"] = []\n",
    "if \"last_ts\" not in st.session_state:\n",
    "    st.session_state[\"last_ts\"] = 0.0\n",
    "\n",
    "API_URL = DEFAULT_API_URL\n",
    "\n",
    "\n",
    "# =================== LEFT PANEL ===================\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"Live\")\n",
    "    st.markdown('<div class=\"left-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"cam-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ctx = webrtc_streamer(\n",
    "        key=\"ear-curatorial-final\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "        video_processor_factory=CuratorialProcessor,\n",
    "        async_processing=True,\n",
    "    )\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"<div style='height:10px'></div>\", unsafe_allow_html=True)\n",
    "\n",
    "    c1, c2 = st.columns(2)\n",
    "\n",
    "    with c1:\n",
    "        if st.button(\"ğŸ“¸ Capture (wait 3s)\", use_container_width=True):\n",
    "            st.session_state[\"countdown_target\"] = time.time() + 3.0\n",
    "\n",
    "    with c2:\n",
    "        if st.button(\"âš¡ Instant Capture\", use_container_width=True):\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"Try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "\n",
    "    if st.session_state[\"countdown_target\"]:\n",
    "        remain = st.session_state[\"countdown_target\"] - time.time()\n",
    "        if remain > 0:\n",
    "            st.info(f\"Capturing in {remain:.1f}sâ€¦ Hold still.\")\n",
    "        else:\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"Try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "            st.session_state[\"countdown_target\"] = None\n",
    "            force_rerun()\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# =================== RIGHT PANEL ===================\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"Matched artwork\")\n",
    "    st.markdown('<div class=\"right-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"art-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ph = st.empty()\n",
    "\n",
    "    proc: Optional[CuratorialProcessor] = None\n",
    "    if ctx and ctx.video_processor:\n",
    "        proc = ctx.video_processor\n",
    "\n",
    "    if not proc:\n",
    "        ph.info(\"Initializing cameraâ€¦\")\n",
    "    else:\n",
    "        with proc.lock:\n",
    "            cap_img = proc.captured_img\n",
    "            cap_ts = getattr(proc, \"captured_ts\", 0.0)\n",
    "            cap_metrics = list(getattr(proc, \"captured_metrics\", []))\n",
    "\n",
    "        if cap_img is not None and cap_ts > st.session_state[\"last_ts\"]:\n",
    "            st.session_state[\"last_ts\"] = cap_ts\n",
    "            st.session_state[\"last_metrics\"] = cap_metrics\n",
    "\n",
    "            buf = io.BytesIO()\n",
    "            cap_img.save(buf, format=\"JPEG\")\n",
    "            buf.seek(0)\n",
    "\n",
    "            files = {\"image\": (\"frame.jpg\", buf.getvalue(), \"image/jpeg\")}\n",
    "            data = {\"museum\": \"mixed\", \"topk\": 3}\n",
    "\n",
    "            try:\n",
    "                resp = requests.post(API_URL, files=files, data=data, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                st.session_state[\"last_match\"] = resp.json()\n",
    "            except Exception as exc:\n",
    "                st.session_state[\"last_match\"] = {\"error\": str(exc)}\n",
    "\n",
    "            force_rerun()\n",
    "\n",
    "        payload = st.session_state.get(\"last_match\")\n",
    "\n",
    "        if not payload:\n",
    "            ph.info(\"Hold still or press capture to matchâ€¦\")\n",
    "        elif \"error\" in payload:\n",
    "            ph.error(f\"Backend error: {payload['error']}\")\n",
    "        else:\n",
    "            results = payload.get(\"results\") or []\n",
    "            if not results:\n",
    "                ph.warning(\"No matches returned.\")\n",
    "            else:\n",
    "                top = results[0]\n",
    "                filename = top.get(\"filename\") or top.get(\"file\")\n",
    "\n",
    "                img_path = ensure_image_path(filename or \"\")\n",
    "                if not img_path:\n",
    "                    ph.error(f\"Image not found: {filename}\")\n",
    "                else:\n",
    "                    painting = safe_open_image(img_path)\n",
    "                    if painting is None:\n",
    "                        ph.error(f\"Failed to open: {img_path}\")\n",
    "                    else:\n",
    "                        meta_row = lookup_meta(str(filename))\n",
    "                        meta = {\n",
    "                            \"artist\": meta_row.get(\"artist\")\n",
    "                            or top.get(\"artist\")\n",
    "                            or \"artist name\",\n",
    "                            \"year\": meta_row.get(\"year\") or top.get(\"year\") or \"\",\n",
    "                            \"price_text\": meta_row.get(\"price_text\")\n",
    "                            or meta_row.get(\"auction_price_usd\")\n",
    "                            or \"\",\n",
    "                        }\n",
    "\n",
    "                        painted = overlay_right_labels(painting, meta)\n",
    "\n",
    "                        metrics = st.session_state.get(\"last_metrics\") or []\n",
    "                        painted = draw_tiny_metrics_top_right(painted, metrics, size=16)\n",
    "\n",
    "                        w = min(RIGHT_IMG_MAXW, painted.width)\n",
    "                        caption = f\"{meta_row.get('title','')} â€” {meta.get('artist','')}\"\n",
    "                        ph.image(painted, caption=caption, width=w)\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022239d1",
   "metadata": {},
   "source": [
    "679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee82a08e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_webrtc import (\n",
    "    WebRtcMode,\n",
    "    RTCConfiguration,\n",
    "    VideoProcessorBase,\n",
    "    webrtc_streamer,\n",
    ")\n",
    "\n",
    "# =================== PATHS & CONSTANTS ===================\n",
    "\n",
    "FRONTEND_DIR = Path(__file__).resolve().parent\n",
    "ROOT_DIR = FRONTEND_DIR.parent\n",
    "\n",
    "# Use GLOBAL MIXED INDEX (local + met merged)\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"mixed\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "# Meta CSV candidates (fallback to local if needed)\n",
    "META_CSV_CANDIDATES = [\n",
    "    DATA_DIR / \"embeddings_meta.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works_enhanced_english.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works.csv\",\n",
    "]\n",
    "\n",
    "DEFAULT_API_URL = \"http://127.0.0.1:8000/match\"\n",
    "APP_TITLE = \"Embodied Aesthetic Reconstruction\"\n",
    "\n",
    "YOLO_MODEL_PATH = FRONTEND_DIR / \"yolov8n-pose.pt\"\n",
    "\n",
    "RTC_CONFIGURATION = RTCConfiguration(\n",
    "    {\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]}\n",
    ")\n",
    "\n",
    "STILLNESS_SEC = 3.5\n",
    "MAX_BUF_SEC = 5.0\n",
    "FPS_ASSUMED = 12\n",
    "MOTION_EPS_CXCY = 4.0\n",
    "MOTION_EPS_AREA = 0.03\n",
    "MIN_FACE_AREA = 0.06\n",
    "\n",
    "YELLOW = (255, 235, 59)\n",
    "BLACK = (0, 0, 0)\n",
    "HOT_PINK = (255, 30, 180)\n",
    "\n",
    "RIGHT_IMG_MAXW = 900\n",
    "\n",
    "# Diversity / randomness control\n",
    "MAX_RECENT = 6          # how many artworks to remember\n",
    "PRIMARY_TOP_K = 3       # \"very best\" range\n",
    "TAIL_TOP_K = 10         # search in top-10 in total\n",
    "TAIL_RANDOM_PROB = 0.3  # 30% chance to look into 4â€“10 first\n",
    "\n",
    "\n",
    "# =================== UTILITIES ===================\n",
    "\n",
    "_META_CACHE: Optional[Dict[str, Dict]] = None\n",
    "\n",
    "\n",
    "def load_meta_mapping() -> Dict[str, Dict]:\n",
    "    \"\"\"Load CSV â†’ filename â†’ metadata mapping.\"\"\"\n",
    "    global _META_CACHE\n",
    "    if _META_CACHE is not None:\n",
    "        return _META_CACHE\n",
    "\n",
    "    import csv\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    for p in META_CSV_CANDIDATES:\n",
    "        if p.exists():\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                rows = list(csv.DictReader(f))\n",
    "            break\n",
    "\n",
    "    mapping: Dict[str, Dict] = {}\n",
    "    for r in rows:\n",
    "        fname = (\n",
    "            r.get(\"filename\")\n",
    "            or r.get(\"image_path\")\n",
    "            or r.get(\"path\")\n",
    "            or r.get(\"file\")\n",
    "        )\n",
    "        if fname:\n",
    "            mapping[str(fname)] = r\n",
    "\n",
    "    _META_CACHE = mapping\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def lookup_meta(filename: str) -> Dict:\n",
    "    mapping = load_meta_mapping()\n",
    "    return mapping.get(filename, {})\n",
    "\n",
    "\n",
    "def safe_open_image(p: Path) -> Optional[Image.Image]:\n",
    "    try:\n",
    "        return Image.open(p).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_image_path(filename: str) -> Optional[Path]:\n",
    "    if not filename:\n",
    "        return None\n",
    "    p = Path(filename)\n",
    "    if not p.is_absolute():\n",
    "        p = IMAGES_DIR / p\n",
    "    return p if p.exists() else None\n",
    "\n",
    "\n",
    "def _load_font(size: int = 40):\n",
    "    \"\"\"Try Courier â†’ Arial â†’ default.\"\"\"\n",
    "    candidates = [\n",
    "        \"/Library/Fonts/Courier New.ttf\",\n",
    "        \"/System/Library/Fonts/Courier.dfont\",\n",
    "        \"/System/Library/Fonts/Supplemental/Courier New.ttf\",\n",
    "        \"/Library/Fonts/Arial.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                return ImageFont.truetype(p, size=size)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def draw_tiny_metrics_top_right(\n",
    "    im: Image.Image, lines: List[str], size: int = 16, margin: int = 10\n",
    ") -> Image.Image:\n",
    "    \"\"\"Small pink pose metrics in top-right corner of the image.\"\"\"\n",
    "    if not lines:\n",
    "        return im\n",
    "    img = im.copy()\n",
    "    d = ImageDraw.Draw(img)\n",
    "    font = _load_font(size)\n",
    "\n",
    "    widths = []\n",
    "    for s in lines:\n",
    "        l, t, r, b = d.textbbox((0, 0), s, font=font)\n",
    "        widths.append(r - l)\n",
    "    wmax = max(widths) if widths else 0\n",
    "\n",
    "    x = img.width - margin - wmax\n",
    "    y = margin\n",
    "    for s in lines:\n",
    "        d.text((x, y), s, fill=HOT_PINK, font=font)\n",
    "        _, _, _, b = d.textbbox((0, 0), s, font=font)\n",
    "        y += int(b * 0.95)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _angle_deg(p1, p2):\n",
    "    if p1 is None or p2 is None:\n",
    "        return None\n",
    "    vx, vy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "    return float(np.degrees(np.arctan2(vy, vx)))\n",
    "\n",
    "\n",
    "def _elbow_angle(shoulder, elbow, wrist):\n",
    "    if None in (shoulder, elbow, wrist):\n",
    "        return None\n",
    "    v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]], float)\n",
    "    v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]], float)\n",
    "    n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if n1 < 1e-5 or n2 < 1e-5:\n",
    "        return None\n",
    "    cosv = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cosv)))\n",
    "\n",
    "\n",
    "def format_metrics(kps: Dict[int, Tuple[float, float] | None]) -> List[str]:\n",
    "    le, re = kps.get(1), kps.get(2)\n",
    "    lsh, rsh = kps.get(5), kps.get(6)\n",
    "    lel, rel = kps.get(7), kps.get(8)\n",
    "    lwr, rwr = kps.get(9), kps.get(10)\n",
    "\n",
    "    fdeg = lambda v: \"â€”\" if v is None else f\"{v:+.1f}Â°\"\n",
    "    fpt = lambda p: \"(0, 0)\" if p is None else f\"({int(p[0])}, {int(p[1])})\"\n",
    "\n",
    "    return [\n",
    "        f\"Head tilt: {fdeg(_angle_deg(re, le))}\",\n",
    "        f\"Shoulder:  {fdeg(_angle_deg(rsh, lsh))}\",\n",
    "        f\"L elbow:   {fdeg(_elbow_angle(lsh, lel, lwr))}\",\n",
    "        f\"R elbow:   {fdeg(_elbow_angle(rsh, rel, rwr))}\",\n",
    "        f\"L wrist:   {fpt(lwr)}\",\n",
    "        f\"R wrist:   {fpt(rwr)}\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def overlay_right_labels(painting: Image.Image, meta: Dict) -> Image.Image:\n",
    "    \"\"\"Yellow label stack: price, year, artist.\"\"\"\n",
    "    im = painting.convert(\"RGB\").copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    font_big = _load_font(44)\n",
    "    font_small = _load_font(36)\n",
    "\n",
    "    price = (\n",
    "        meta.get(\"price_text\")\n",
    "        or meta.get(\"auction_price_usd\")\n",
    "        or meta.get(\"price\")\n",
    "        or \"â€”\"\n",
    "    )\n",
    "    year = str(meta.get(\"year\") or \"â€”\")\n",
    "    artist = meta.get(\"artist\") or \"artist name\"\n",
    "\n",
    "    lines = [price, year, artist]\n",
    "    fonts = [font_big, font_small, font_big]\n",
    "\n",
    "    margin_x = 24\n",
    "    y = int(im.height * 0.50)\n",
    "\n",
    "    for text, font in zip(lines, fonts):\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font)\n",
    "        w, h = r - l, b - t\n",
    "\n",
    "        pad_x, pad_y = 16, 10\n",
    "        box_w, box_h = w + 2 * pad_x, h + 2 * pad_y\n",
    "\n",
    "        x = margin_x\n",
    "        draw.rectangle([x, y, x + box_w, y + box_h], fill=YELLOW)\n",
    "        draw.text((x + pad_x, y + pad_y), text, fill=BLACK, font=font)\n",
    "\n",
    "        y += box_h + 10\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def force_rerun():\n",
    "    \"\"\"Compatible with new/old Streamlit.\"\"\"\n",
    "    try:\n",
    "        st.rerun()\n",
    "    except Exception:\n",
    "        try:\n",
    "            st.experimental_rerun()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =================== YOLO VIDEO PROCESSOR ===================\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "    HAS_YOLO = True\n",
    "except Exception:\n",
    "    HAS_YOLO = False\n",
    "\n",
    "\n",
    "class CuratorialProcessor(VideoProcessorBase):\n",
    "    \"\"\"\n",
    "    YOLOv8-Pose:\n",
    "    - blue bbox\n",
    "    - green skeleton\n",
    "    - pink tiny pose metrics\n",
    "    - stillness-based auto capture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        if HAS_YOLO:\n",
    "            try:\n",
    "                local = YOLO_MODEL_PATH\n",
    "                self.model = YOLO(str(local if local.exists() else \"yolov8n-pose.pt\"))\n",
    "            except Exception:\n",
    "                self.model = None\n",
    "\n",
    "        maxlen = max(6, int(MAX_BUF_SEC * FPS_ASSUMED))\n",
    "        self.cx_buf = deque(maxlen=maxlen)\n",
    "        self.cy_buf = deque(maxlen=maxlen)\n",
    "        self.area_buf = deque(maxlen=maxlen)\n",
    "        self.last_stable_ts: Optional[float] = None\n",
    "\n",
    "        self.captured_img: Optional[Image.Image] = None\n",
    "        self.captured_ts: float = 0.0\n",
    "        self.captured_metrics: List[str] = []\n",
    "\n",
    "        self.latest_rgb: Optional[np.ndarray] = None\n",
    "        self.last_metrics_lines: List[str] = []\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _detect_bbox(self, rgb: np.ndarray) -> Optional[Tuple[int, int, int, int]]:\n",
    "        if not self.model:\n",
    "            return None\n",
    "        res = self.model.predict(rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "        if len(res) == 0 or res[0].boxes is None:\n",
    "            return None\n",
    "        b = res[0].boxes.xyxy\n",
    "        if b is None or len(b) == 0:\n",
    "            return None\n",
    "        b = b.cpu().numpy()\n",
    "        areas = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "        i = int(np.argmax(areas))\n",
    "        x1, y1, x2, y2 = b[i].astype(int).tolist()\n",
    "        return x1, y1, x2, y2\n",
    "\n",
    "    def _extract_keypoints(self, res) -> Dict[int, Tuple[float, float] | None]:\n",
    "        kps: Dict[int, Tuple[float, float] | None] = {}\n",
    "        try:\n",
    "            if res and res[0].keypoints is not None and len(res[0].keypoints) > 0:\n",
    "                xy = res[0].keypoints.xy[0].cpu().numpy()\n",
    "                for i in range(xy.shape[0]):\n",
    "                    kps[i] = (float(xy[i, 0]), float(xy[i, 1]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return kps\n",
    "\n",
    "    def _update_stillness(self, bbox, w, h) -> bool:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cx = 0.5 * (x1 + x2)\n",
    "        cy = 0.5 * (y1 + y2)\n",
    "        area = max(1.0, (x2 - x1) * (y2 - y1))\n",
    "        rel_area = area / float(w * h)\n",
    "\n",
    "        if rel_area < MIN_FACE_AREA:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        self.cx_buf.append(cx)\n",
    "        self.cy_buf.append(cy)\n",
    "        self.area_buf.append(rel_area)\n",
    "\n",
    "        need_len = int(STILLNESS_SEC * FPS_ASSUMED * 0.6)\n",
    "        if len(self.cx_buf) < max(3, need_len):\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        stdx = float(np.std(self.cx_buf))\n",
    "        stdy = float(np.std(self.cy_buf))\n",
    "        stda = float(np.std(self.area_buf))\n",
    "        stable_now = (\n",
    "            stdx < MOTION_EPS_CXCY\n",
    "            and stdy < MOTION_EPS_CXCY\n",
    "            and stda < MOTION_EPS_AREA\n",
    "        )\n",
    "\n",
    "        now = time.time()\n",
    "        if stable_now:\n",
    "            if self.last_stable_ts is None:\n",
    "                self.last_stable_ts = now\n",
    "            return (now - self.last_stable_ts) >= STILLNESS_SEC\n",
    "        else:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "    def _stamp_capture(self):\n",
    "        self.captured_img = Image.fromarray(self.latest_rgb)\n",
    "        self.captured_ts = time.time()\n",
    "        self.captured_metrics = list(self.last_metrics_lines)\n",
    "        self.last_stable_ts = None\n",
    "\n",
    "    def capture_now(self) -> bool:\n",
    "        if self.latest_rgb is None:\n",
    "            return False\n",
    "        with self.lock:\n",
    "            self._stamp_capture()\n",
    "        return True\n",
    "\n",
    "    def recv(self, frame):\n",
    "        import av\n",
    "\n",
    "        img_bgr = frame.to_ndarray(format=\"bgr24\")\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        h, w, _ = img_rgb.shape\n",
    "        self.latest_rgb = img_rgb\n",
    "\n",
    "        if self.model:\n",
    "            res = self.model.predict(img_rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "            plotted = res[0].plot()[:, :, ::-1]\n",
    "            kps = self._extract_keypoints(res)\n",
    "            lines = format_metrics(kps)\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = lines\n",
    "            pil = Image.fromarray(plotted)\n",
    "            pil = draw_tiny_metrics_top_right(pil, lines, size=16, margin=10)\n",
    "            out_rgb = np.array(pil)\n",
    "        else:\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = [\"(pose model unavailable)\"]\n",
    "            out_rgb = img_rgb\n",
    "\n",
    "        bbox = self._detect_bbox(img_rgb) if self.model else None\n",
    "        if bbox and self._update_stillness(bbox, w, h):\n",
    "            with self.lock:\n",
    "                if time.time() - self.captured_ts > 0.35:\n",
    "                    self._stamp_capture()\n",
    "\n",
    "        out_bgr = out_rgb[:, :, ::-1]\n",
    "        return av.VideoFrame.from_ndarray(out_bgr, format=\"bgr24\")\n",
    "\n",
    "\n",
    "# =================== PAGE LAYOUT ===================\n",
    "\n",
    "st.set_page_config(page_title=APP_TITLE, layout=\"wide\")\n",
    "\n",
    "st.markdown(\n",
    "    f\"\"\"\n",
    "<style>\n",
    "section[data-testid=\"stSidebar\"] {{ display: none !important; }}\n",
    "header, footer, [data-testid=\"stToolbar\"] {{ visibility: hidden !important; }}\n",
    "\n",
    "body {{\n",
    "  background-color: #f5f5f7;\n",
    "}}\n",
    ".block-container {{\n",
    "  padding-top: 0.6rem;\n",
    "  padding-bottom: 0.6rem;\n",
    "  max-width: 1700px;\n",
    "}}\n",
    "\n",
    "h1 {{\n",
    "  font-family: -apple-system, BlinkMacSystemFont, \"SF Pro Display\", system-ui, sans-serif;\n",
    "  letter-spacing: 0.04em;\n",
    "  font-weight: 700;\n",
    "}}\n",
    "\n",
    ".left-col .cam-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;\n",
    "  width: 100%;\n",
    "  overflow: hidden;\n",
    "  border-radius: 18px;\n",
    "  background: #111;\n",
    "  box-shadow: 0 18px 45px rgba(0,0,0,0.3);\n",
    "}}\n",
    ".left-col .cam-wrap video {{\n",
    "  height: 100% !important;\n",
    "  width: auto !important;\n",
    "  object-fit: cover !important;\n",
    "  border-radius: 18px !important;\n",
    "}}\n",
    "\n",
    ".right-col .art-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;\n",
    "  max-width: {RIGHT_IMG_MAXW}px;\n",
    "  overflow: hidden;\n",
    "  margin: 0 auto;\n",
    "  border-radius: 18px;\n",
    "  background: #050505;\n",
    "  box-shadow: 0 18px 45px rgba(0,0,0,0.35);\n",
    "}}\n",
    ".right-col .art-wrap img {{\n",
    "  display: block;\n",
    "  width: 100% !important;\n",
    "  height: 100% !important;\n",
    "  object-fit: cover !important;\n",
    "}}\n",
    "\n",
    "button[kind=\"secondary\"] {{\n",
    "  border-radius: 999px !important;\n",
    "}}\n",
    "\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "st.title(APP_TITLE)\n",
    "st.caption(\n",
    "    \"Hold still for ~3â€“5 seconds to auto-capture, or press the button to capture manually. \"\n",
    "    \"Left: live video with pose. Right: matched artwork with tiny pink pose metrics.\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    st.autorefresh(interval=700, key=\"ear_auto\", limit=None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "left, right = st.columns([1, 1], gap=\"large\")\n",
    "\n",
    "if \"countdown_target\" not in st.session_state:\n",
    "    st.session_state[\"countdown_target\"] = None\n",
    "if \"last_match\" not in st.session_state:\n",
    "    st.session_state[\"last_match\"] = None\n",
    "if \"last_metrics\" not in st.session_state:\n",
    "    st.session_state[\"last_metrics\"] = []\n",
    "if \"last_ts\" not in st.session_state:\n",
    "    st.session_state[\"last_ts\"] = 0.0\n",
    "if \"recent_files\" not in st.session_state:\n",
    "    st.session_state[\"recent_files\"] = []  # type: List[str]\n",
    "\n",
    "API_URL = DEFAULT_API_URL\n",
    "\n",
    "\n",
    "# =================== LEFT PANEL ===================\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"Live\")\n",
    "    st.markdown('<div class=\"left-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"cam-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ctx = webrtc_streamer(\n",
    "        key=\"ear-curatorial-final\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "        video_processor_factory=CuratorialProcessor,\n",
    "        async_processing=True,\n",
    "    )\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"<div style='height:10px'></div>\", unsafe_allow_html=True)\n",
    "\n",
    "    c1, c2 = st.columns(2)\n",
    "\n",
    "    with c1:\n",
    "        if st.button(\"ğŸ“¸ Capture (wait 3s)\", use_container_width=True):\n",
    "            st.session_state[\"countdown_target\"] = time.time() + 3.0\n",
    "\n",
    "    with c2:\n",
    "        if st.button(\"âš¡ Instant Capture\", use_container_width=True):\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"Try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "\n",
    "    if st.session_state[\"countdown_target\"]:\n",
    "        remain = st.session_state[\"countdown_target\"] - time.time()\n",
    "        if remain > 0:\n",
    "            st.info(f\"Capturing in {remain:.1f}sâ€¦ Hold still.\")\n",
    "        else:\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"Try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "            st.session_state[\"countdown_target\"] = None\n",
    "            force_rerun()\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# =================== RIGHT PANEL ===================\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"Matched artwork\")\n",
    "    st.markdown('<div class=\"right-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"art-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ph = st.empty()\n",
    "\n",
    "    proc: Optional[CuratorialProcessor] = None\n",
    "    if ctx and ctx.video_processor:\n",
    "        proc = ctx.video_processor\n",
    "\n",
    "    if not proc:\n",
    "        ph.info(\"Initializing cameraâ€¦\")\n",
    "    else:\n",
    "        with proc.lock:\n",
    "            cap_img = proc.captured_img\n",
    "            cap_ts = getattr(proc, \"captured_ts\", 0.0)\n",
    "            cap_metrics = list(getattr(proc, \"captured_metrics\", []))\n",
    "\n",
    "        if cap_img is not None and cap_ts > st.session_state[\"last_ts\"]:\n",
    "            st.session_state[\"last_ts\"] = cap_ts\n",
    "            st.session_state[\"last_metrics\"] = cap_metrics\n",
    "\n",
    "            buf = io.BytesIO()\n",
    "            cap_img.save(buf, format=\"JPEG\")\n",
    "            buf.seek(0)\n",
    "\n",
    "            files = {\"image\": (\"frame.jpg\", buf.getvalue(), \"image/jpeg\")}\n",
    "            # Ask backend for Top-10, we'll do diversity selection here\n",
    "            data = {\"museum\": \"mixed\", \"topk\": TAIL_TOP_K}\n",
    "\n",
    "            try:\n",
    "                resp = requests.post(API_URL, files=files, data=data, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                st.session_state[\"last_match\"] = resp.json()\n",
    "            except Exception as exc:\n",
    "                st.session_state[\"last_match\"] = {\"error\": str(exc)}\n",
    "\n",
    "            force_rerun()\n",
    "\n",
    "        payload = st.session_state.get(\"last_match\")\n",
    "\n",
    "        if not payload:\n",
    "            ph.info(\"Hold still or press capture to matchâ€¦\")\n",
    "        elif \"error\" in payload:\n",
    "            ph.error(f\"Backend error: {payload['error']}\")\n",
    "        else:\n",
    "            results = payload.get(\"results\") or []\n",
    "            if not results:\n",
    "                ph.warning(\"No matches returned.\")\n",
    "            else:\n",
    "                recent = st.session_state.get(\"recent_files\", [])\n",
    "\n",
    "                # ---------- diversity + mild randomness ----------\n",
    "                # Decide whether to start looking from the tail (ranks 4â€“10)\n",
    "                use_tail_first = random.random() < TAIL_RANDOM_PROB and len(results) > PRIMARY_TOP_K\n",
    "                primary_slice = results[:PRIMARY_TOP_K]\n",
    "                tail_slice = results[PRIMARY_TOP_K:TAIL_TOP_K]\n",
    "\n",
    "                ordered_candidates = (\n",
    "                    (tail_slice + primary_slice) if use_tail_first else (primary_slice + tail_slice)\n",
    "                )\n",
    "\n",
    "                chosen = None\n",
    "                chosen_fname = None\n",
    "\n",
    "                for r in ordered_candidates:\n",
    "                    fname = (\n",
    "                        r.get(\"filename\")\n",
    "                        or r.get(\"file\")\n",
    "                        or r.get(\"image_path\")\n",
    "                        or r.get(\"path\")\n",
    "                    )\n",
    "                    if not fname:\n",
    "                        continue\n",
    "                    if fname not in recent:\n",
    "                        chosen = r\n",
    "                        chosen_fname = fname\n",
    "                        break\n",
    "\n",
    "                # Fallback: if all candidates are \"recent\", use strict Top-1\n",
    "                if chosen is None:\n",
    "                    chosen = results[0]\n",
    "                    chosen_fname = (\n",
    "                        chosen.get(\"filename\")\n",
    "                        or chosen.get(\"file\")\n",
    "                        or chosen.get(\"image_path\")\n",
    "                        or chosen.get(\"path\")\n",
    "                    )\n",
    "\n",
    "                # Update recent memory\n",
    "                if chosen_fname:\n",
    "                    if chosen_fname in recent:\n",
    "                        recent.remove(chosen_fname)\n",
    "                    recent.insert(0, chosen_fname)\n",
    "                    del recent[MAX_RECENT:]\n",
    "                    st.session_state[\"recent_files\"] = recent\n",
    "\n",
    "                filename = chosen_fname\n",
    "\n",
    "                img_path = ensure_image_path(filename or \"\")\n",
    "                if not img_path:\n",
    "                    ph.error(f\"Image not found: {filename}\")\n",
    "                else:\n",
    "                    painting = safe_open_image(img_path)\n",
    "                    if painting is None:\n",
    "                        ph.error(f\"Failed to open: {img_path}\")\n",
    "                    else:\n",
    "                        meta_row = lookup_meta(str(filename))\n",
    "                        meta = {\n",
    "                            \"artist\": meta_row.get(\"artist\")\n",
    "                            or chosen.get(\"artist\")\n",
    "                            or \"artist name\",\n",
    "                            \"year\": meta_row.get(\"year\") or chosen.get(\"year\") or \"\",\n",
    "                            \"price_text\": meta_row.get(\"price_text\")\n",
    "                            or meta_row.get(\"auction_price_usd\")\n",
    "                            or \"\",\n",
    "                        }\n",
    "\n",
    "                        painted = overlay_right_labels(painting, meta)\n",
    "                        metrics = st.session_state.get(\"last_metrics\") or []\n",
    "                        painted = draw_tiny_metrics_top_right(painted, metrics, size=16, margin=12)\n",
    "\n",
    "                        w = min(RIGHT_IMG_MAXW, painted.width)\n",
    "                        caption = f\"{meta_row.get('title','')} â€” {meta.get('artist','')}\"\n",
    "                        ph.image(painted, caption=caption, width=w)\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7490258",
   "metadata": {},
   "source": [
    "pose_matcher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42932093",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PoseMatcher\n",
    "\n",
    "CLIP-based matcher with optional *pose embeddings* re-weighting:\n",
    "\n",
    "- loads CLIP embeddings from data/.../embeddings.npy\n",
    "- loads optional pose embeddings from data/.../pose_embeddings.npy\n",
    "- loads metadata from data/.../embeddings_meta.csv\n",
    "- computes query CLIP (and pose) embeddings for the uploaded image\n",
    "- mixes scores: (1 - pose_weight) * clip + pose_weight * pose\n",
    "- returns Top-K matched artworks with their metadata\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from backend.config import Settings\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MatchItem:\n",
    "    filename: str\n",
    "    title: Optional[str]\n",
    "    artist: Optional[str]\n",
    "    year: Optional[int]\n",
    "    score: float\n",
    "\n",
    "\n",
    "class PoseMatcher:\n",
    "    def __init__(self, settings: Settings):\n",
    "        self.settings = settings\n",
    "\n",
    "        # --------- Resolve device ----------\n",
    "        if settings.device == \"cpu\":\n",
    "            device = \"cpu\"\n",
    "        elif settings.device == \"cuda\" and torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif settings.device == \"mps\" and torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "        else:\n",
    "            # \"auto\" or unsupported â†’ try CUDA, then MPS, then CPU\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda\"\n",
    "            elif torch.backends.mps.is_available():\n",
    "                device = \"mps\"\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        # --------- Load CLIP model ----------\n",
    "        # Must match build_embeddings.py\n",
    "        self.model, self.preprocess, _ = open_clip.create_model_and_transforms(\n",
    "            \"ViT-B-32\",\n",
    "            pretrained=\"openai\",\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "        # --------- Load CLIP embeddings ----------\n",
    "        emb_path = settings.embeddings_path\n",
    "        if not emb_path.exists():\n",
    "            raise FileNotFoundError(f\"Embeddings file not found: {emb_path}\")\n",
    "\n",
    "        emb_np = np.load(emb_path)\n",
    "        if isinstance(emb_np, np.lib.npyio.NpzFile):\n",
    "            emb_np = emb_np[\"embeddings\"]\n",
    "\n",
    "        emb = torch.from_numpy(emb_np).float()\n",
    "        emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        self.embeddings = emb.to(self.device)  # (N, D_clip)\n",
    "\n",
    "        # --------- Load pose embeddings (optional) ----------\n",
    "        self.pose_embeddings: Optional[torch.Tensor] = None\n",
    "        self.pose_weight: float = float(settings.pose_weight)\n",
    "\n",
    "        pose_path: Path = settings.pose_embeddings_path\n",
    "        if pose_path.exists():\n",
    "            pose_np = np.load(pose_path)\n",
    "            # basic sanity check: same number of rows as CLIP embeddings\n",
    "            if pose_np.shape[0] == emb_np.shape[0]:\n",
    "                pose = torch.from_numpy(pose_np).float()\n",
    "                pose = pose / (pose.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "                self.pose_embeddings = pose.to(self.device)  # (N, D_pose)\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[WARN] pose_embeddings shape {pose_np.shape} \"\n",
    "                    f\"does not match embeddings {emb_np.shape}; ignoring pose features.\"\n",
    "                )\n",
    "        else:\n",
    "            # Not an error: we simply fall back to CLIP-only\n",
    "            print(f\"[INFO] pose_embeddings not found at {pose_path}, using CLIP-only scores.\")\n",
    "\n",
    "        # --------- Load metadata ----------\n",
    "        meta_path = settings.embeddings_meta_path\n",
    "        if not meta_path.exists():\n",
    "            raise FileNotFoundError(f\"Metadata CSV not found: {meta_path}\")\n",
    "\n",
    "        self.meta: List[Dict] = []\n",
    "        self.meta_by_filename: Dict[str, Dict] = {}\n",
    "\n",
    "        with meta_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            header = f.readline().strip().split(\",\")\n",
    "            for line in f:\n",
    "                parts = [p.strip() for p in line.strip().split(\",\")]\n",
    "                if not parts or not parts[0]:\n",
    "                    continue\n",
    "                row = dict(zip(header, parts))\n",
    "                filename = row.get(\"filename\") or row.get(\"file\") or \"\"\n",
    "                if not filename:\n",
    "                    continue\n",
    "\n",
    "                # Normalize year to int if possible\n",
    "                raw_year = row.get(\"year\") or row.get(\"Year\")\n",
    "                year_int: Optional[int] = None\n",
    "                if raw_year:\n",
    "                    try:\n",
    "                        year_int = int(str(raw_year).split(\",\")[0].strip())\n",
    "                    except Exception:\n",
    "                        year_int = None\n",
    "\n",
    "                meta_entry = {\n",
    "                    \"filename\": filename,\n",
    "                    \"title\": row.get(\"title\") or row.get(\"Title\"),\n",
    "                    \"artist\": row.get(\"artist\") or row.get(\"Artist\"),\n",
    "                    \"year\": year_int,\n",
    "                    # keep all raw columns in case we need them later\n",
    "                    **row,\n",
    "                }\n",
    "                self.meta.append(meta_entry)\n",
    "                self.meta_by_filename[filename] = meta_entry\n",
    "\n",
    "        # For compatibility with backend.main\n",
    "        self.default_museum = settings.default_museum\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Encoding helpers\n",
    "    # ------------------------------------------------------------------\n",
    "    def _encode_image(self, image: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode a PIL image into a normalized CLIP embedding (1, D_clip).\n",
    "        \"\"\"\n",
    "        img = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            feat = self.model.encode_image(img)\n",
    "        feat = feat / (feat.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        return feat  # (1, D_clip)\n",
    "\n",
    "    def _encode_pose(self, image: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode pose-style features for the image.\n",
    "\n",
    "        For now this is implemented as the same CLIP encoder\n",
    "        (so that everything runs out-of-the-box). Later you can\n",
    "        replace this with a dedicated pose encoder or a vector\n",
    "        built from YOLO keypoints.\n",
    "\n",
    "        Shape: (1, D_pose) â€” must match pose_embeddings' last dim.\n",
    "        \"\"\"\n",
    "        # Placeholder implementation = CLIP features again\n",
    "        return self._encode_image(image)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Public APIs\n",
    "    # ------------------------------------------------------------------\n",
    "    def match_pil(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        museum: Optional[str] = None,\n",
    "        topk: int = 3,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Match a PIL image against the stored embeddings.\n",
    "\n",
    "        Returns a list of dicts:\n",
    "            { \"filename\": ..., \"title\": ..., \"artist\": ..., \"year\": ..., \"score\": ... }\n",
    "        \"\"\"\n",
    "        if topk <= 0:\n",
    "            topk = 3\n",
    "\n",
    "        # ---- CLIP similarity ----\n",
    "        q_clip = self._encode_image(image)  # (1, D_clip)\n",
    "        sims_clip = (self.embeddings @ q_clip.T).squeeze(1)  # (N,)\n",
    "\n",
    "        # ---- pose similarity (optional) ----\n",
    "        if self.pose_embeddings is not None and self.pose_weight > 0.0:\n",
    "            q_pose = self._encode_pose(image)  # (1, D_pose)\n",
    "            # if dims mismatch, fall back to CLIP-only\n",
    "            if q_pose.shape[1] == self.pose_embeddings.shape[1]:\n",
    "                sims_pose = (self.pose_embeddings @ q_pose.T).squeeze(1)\n",
    "                alpha = float(self.pose_weight)\n",
    "                sims = (1.0 - alpha) * sims_clip + alpha * sims_pose\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[WARN] pose dim mismatch: query={q_pose.shape}, \"\n",
    "                    f\"embeddings={self.pose_embeddings.shape}; using CLIP-only.\"\n",
    "                )\n",
    "                sims = sims_clip\n",
    "        else:\n",
    "            sims = sims_clip\n",
    "\n",
    "        # Top-K indices\n",
    "        k = min(topk, sims.shape[0])\n",
    "        scores, indices = torch.topk(sims, k=k, largest=True, sorted=True)\n",
    "\n",
    "        results: List[Dict] = []\n",
    "        for score, idx in zip(scores.tolist(), indices.tolist()):\n",
    "            if 0 <= idx < len(self.meta):\n",
    "                meta = self.meta[idx]\n",
    "            else:\n",
    "                meta = {}\n",
    "\n",
    "            filename = meta.get(\"filename\", \"\")\n",
    "            title = meta.get(\"title\")\n",
    "            artist = meta.get(\"artist\")\n",
    "            year = meta.get(\"year\")\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"filename\": filename,\n",
    "                    \"title\": title,\n",
    "                    \"artist\": artist,\n",
    "                    \"year\": year,\n",
    "                    \"score\": float(score),\n",
    "                }\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    def match_image_bytes(\n",
    "        self,\n",
    "        image_bytes: bytes,\n",
    "        museum: Optional[str] = None,\n",
    "        topk: int = 3,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Convenience wrapper used by the purely local Streamlit curatorial app.\n",
    "        \"\"\"\n",
    "        from io import BytesIO\n",
    "\n",
    "        pil = Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n",
    "        return self.match_pil(pil, museum=museum, topk=topk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c579f",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b137149",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EAR Backend API (FastAPI + Uvicorn)\n",
    "\n",
    "Exposes:\n",
    "- GET  /health     â†’ quick status check\n",
    "- POST /match      â†’ upload an image and get Top-K matched artworks\n",
    "\n",
    "It uses the PoseMatcher defined in backend/model/pose_matcher.py and the\n",
    "Settings from backend/config.py.\n",
    "\n",
    "Default index: data/mixed/ (global mixed index: local + met).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import logging\n",
    "from typing import List, Optional\n",
    "\n",
    "from fastapi import FastAPI, File, Form, HTTPException, UploadFile\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from PIL import Image\n",
    "\n",
    "from .config import Settings\n",
    "from .model.pose_matcher import PoseMatcher\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"uvicorn.error\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FastAPI app + CORS\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Embodied Aesthetic Reconstruction â€” Backend API\",\n",
    "    version=\"0.2.0\",\n",
    "    description=\"CLIP + pose based portrait â†’ artwork matching backend.\",\n",
    ")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # you can tighten this later\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Global objects filled at startup\n",
    "settings: Optional[Settings] = None\n",
    "matcher: Optional[PoseMatcher] = None\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Startup / shutdown\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def _startup_load() -> None:\n",
    "    \"\"\"Load configuration + matcher once when the server starts.\"\"\"\n",
    "    global settings, matcher\n",
    "\n",
    "    logger.info(\"Loading Settings and PoseMatcherâ€¦\")\n",
    "    settings = Settings()\n",
    "    matcher = PoseMatcher(settings)\n",
    "    logger.info(\"Matcher loaded successfully.\")\n",
    "\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "def _shutdown_cleanup() -> None:\n",
    "    \"\"\"Optional cleanup (currently nothing special).\"\"\"\n",
    "    global matcher\n",
    "    matcher = None\n",
    "    logger.info(\"Matcher released.\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Simple health endpoint\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"device\": getattr(matcher, \"device\", \"unknown\") if matcher else \"unloaded\",\n",
    "        \"default_museum\": getattr(settings, \"default_museum\", None)\n",
    "        if settings\n",
    "        else None,\n",
    "        \"pose_weight\": getattr(settings, \"pose_weight\", None) if settings else None,\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Schemas for /match\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class MatchResult(BaseModel):\n",
    "    filename: str\n",
    "    title: Optional[str] = None\n",
    "    artist: Optional[str] = None\n",
    "    year: Optional[int] = None\n",
    "    score: float\n",
    "\n",
    "\n",
    "class MatchResponse(BaseModel):\n",
    "    museum: Optional[str]\n",
    "    topk: int\n",
    "    results: List[MatchResult]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Main matching endpoint\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@app.post(\"/match\", response_model=MatchResponse)\n",
    "async def match(\n",
    "    image: UploadFile = File(..., description=\"Uploaded portrait photo\"),\n",
    "    museum: Optional[str] = Form(None, description=\"Museum name (kept for compatibility)\"),\n",
    "    topk: int = Form(3, description=\"Number of top results\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust matching endpoint:\n",
    "\n",
    "    - validates that the uploaded file looks like an image\n",
    "    - decodes it to a PIL image (RGB)\n",
    "    - calls PoseMatcher.match_pil(...)\n",
    "    - tries to back-fill missing title / artist / year from matcher.meta_by_filename\n",
    "    \"\"\"\n",
    "    if topk <= 0:\n",
    "        topk = 3\n",
    "\n",
    "    if image.content_type is not None and not image.content_type.startswith(\"image/\"):\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=f\"Unsupported content type: {image.content_type}. Please upload an image file.\",\n",
    "        )\n",
    "\n",
    "    data = await image.read()\n",
    "    if not data:\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"Empty file received. Please take a photo again.\",\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        pil_img = Image.open(io.BytesIO(data)).convert(\"RGB\")\n",
    "    except Exception as exc:\n",
    "        logger.exception(\"Failed to decode uploaded image\")\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=f\"Invalid image file: {type(exc).__name__}\",\n",
    "        )\n",
    "\n",
    "    global matcher\n",
    "    if matcher is None:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=\"Matcher is not loaded on the server.\",\n",
    "        )\n",
    "\n",
    "    effective_museum = museum or getattr(matcher, \"default_museum\", None) or \"mixed\"\n",
    "\n",
    "    try:\n",
    "        if hasattr(matcher, \"match_pil\"):\n",
    "            raw_results = matcher.match_pil(\n",
    "                pil_img,\n",
    "                museum=effective_museum,\n",
    "                topk=topk,\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\"PoseMatcher has no method 'match_pil'\")\n",
    "    except ValueError as exc:\n",
    "        logger.exception(\"ValueError in matcher.match_pil\")\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=f\"Matching failed: {exc}\",\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        logger.exception(\"Unhandled error in matcher.match_pil\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Internal error during matching: {type(exc).__name__}: {exc}\",\n",
    "        )\n",
    "\n",
    "    payload: List[MatchResult] = []\n",
    "\n",
    "    meta_store = getattr(matcher, \"meta_by_filename\", None)\n",
    "\n",
    "    def fill_from_meta(filename: str, title, artist, year):\n",
    "        \"\"\"If title/artist/year are missing, try to fill from matcher metadata.\"\"\"\n",
    "        nonlocal meta_store\n",
    "        if not isinstance(meta_store, dict):\n",
    "            return title, artist, year\n",
    "\n",
    "        meta = meta_store.get(filename) or meta_store.get(filename.strip()) or {}\n",
    "        if isinstance(meta, dict):\n",
    "            title = title or meta.get(\"title\") or meta.get(\"Title\")\n",
    "            artist = artist or meta.get(\"artist\") or meta.get(\"Artist\")\n",
    "            year = year or meta.get(\"year\") or meta.get(\"Year\")\n",
    "        else:\n",
    "            title = title or getattr(meta, \"title\", None)\n",
    "            artist = artist or getattr(meta, \"artist\", None)\n",
    "            year = year or getattr(meta, \"year\", None)\n",
    "        return title, artist, year\n",
    "\n",
    "    for r in raw_results or []:\n",
    "        if isinstance(r, dict):\n",
    "            filename = r.get(\"filename\") or r.get(\"file\") or \"\"\n",
    "            score = float(r.get(\"score\", 0.0))\n",
    "            title = r.get(\"title\")\n",
    "            artist = r.get(\"artist\")\n",
    "            year = r.get(\"year\")\n",
    "        else:\n",
    "            filename = getattr(r, \"filename\", \"\") or getattr(r, \"file\", \"\")\n",
    "            score = float(getattr(r, \"score\", 0.0))\n",
    "            title = getattr(r, \"title\", None)\n",
    "            artist = getattr(r, \"artist\", None)\n",
    "            year = getattr(r, \"year\", None)\n",
    "\n",
    "        title, artist, year = fill_from_meta(filename, title, artist, year)\n",
    "\n",
    "        # normalise year â†’ int or None\n",
    "        try:\n",
    "            if isinstance(year, str) and year.strip():\n",
    "                year_int = int(year.split(\",\")[0].strip())\n",
    "            elif isinstance(year, (int, float)):\n",
    "                year_int = int(year)\n",
    "            else:\n",
    "                year_int = None\n",
    "        except Exception:\n",
    "            year_int = None\n",
    "\n",
    "        payload.append(\n",
    "            MatchResult(\n",
    "                filename=filename,\n",
    "                title=title,\n",
    "                artist=artist,\n",
    "                year=year_int,\n",
    "                score=score,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return MatchResponse(museum=effective_museum, topk=topk, results=payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc1b590",
   "metadata": {},
   "source": [
    "pose_matcher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d009df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Hybrid Pose+CLIP Matcher\n",
    "------------------------\n",
    "This module performs a hybrid similarity search over a museum dataset.\n",
    "\n",
    "Data folder layout (per museum):\n",
    "    data/<museum>/\n",
    "        embeddings.npy          # (N, D_clip)\n",
    "        pose_embeddings.npy     # (N, D_pose)   â† optional\n",
    "        embeddings_meta.csv     # filenames + metadata\n",
    "\n",
    "Final score:\n",
    "    final = (1 - POSE_WEIGHT) * clip_sim + (POSE_WEIGHT) * pose_sim\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "from .utils import normalize_rows, cosine_similarity_matrix\n",
    "\n",
    "\n",
    "class PoseMatcher:\n",
    "    \"\"\"\n",
    "    Hybrid matcher combining CLIP embeddings + Pose embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        museum_dir: Path,\n",
    "        pose_weight: float = 0.35,        # You can tune this\n",
    "        topk_default: int = 3,\n",
    "    ):\n",
    "        self.museum_dir = museum_dir\n",
    "        self.pose_weight = float(pose_weight)\n",
    "        self.topk_default = int(topk_default)\n",
    "\n",
    "        # Load embeddings\n",
    "        emb_path = museum_dir / \"embeddings.npy\"\n",
    "        meta_path = museum_dir / \"embeddings_meta.csv\"\n",
    "        pose_path = museum_dir / \"pose_embeddings.npy\"\n",
    "\n",
    "        if not emb_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing embeddings: {emb_path}\")\n",
    "\n",
    "        self.clip_emb = np.load(emb_path).astype(\"float32\")\n",
    "        self.clip_emb = normalize_rows(self.clip_emb)\n",
    "\n",
    "        # Metadata\n",
    "        self.meta = self._load_meta_rows(meta_path)\n",
    "\n",
    "        # Pose embeddings (optional)\n",
    "        if pose_path.exists():\n",
    "            print(f\"[INFO] Pose embeddings found at {pose_path}\")\n",
    "            self.pose_emb = np.load(pose_path).astype(\"float32\")\n",
    "            self.pose_emb = normalize_rows(self.pose_emb)\n",
    "            self.has_pose = True\n",
    "        else:\n",
    "            print(f\"[INFO] No pose_embeddings.npy â†’ using CLIP-only mode\")\n",
    "            self.pose_emb = None\n",
    "            self.has_pose = False\n",
    "\n",
    "        # Basic sanity checks\n",
    "        if len(self.meta) != len(self.clip_emb):\n",
    "            raise ValueError(\n",
    "                f\"Metadata length {len(self.meta)} != embeddings length {len(self.clip_emb)}\"\n",
    "            )\n",
    "        if self.has_pose and len(self.pose_emb) != len(self.clip_emb):\n",
    "            raise ValueError(\n",
    "                f\"Pose embeddings length {len(self.pose_emb)} != CLIP embeddings length {len(self.clip_emb)}\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"[OK] Loaded museum='{museum_dir.name}' \"\n",
    "            f\"â†’ N={len(self.clip_emb)}, pose={self.has_pose}\"\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    def _load_meta_rows(self, path: Path) -> List[Dict]:\n",
    "        import csv\n",
    "\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Metadata CSV not found: {path}\")\n",
    "\n",
    "        rows = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for row in csv.DictReader(f):\n",
    "                rows.append(row)\n",
    "        return rows\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    def encode_query_image(self, pil, clip_model, preprocess, device):\n",
    "        \"\"\"\n",
    "        Encode query image using CLIP image encoder.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "\n",
    "        x = preprocess(pil).unsqueeze(0).to(device)\n",
    "        with torch.no_grad(), torch.autocast(device_type=str(device), enabled=(device.type in [\"cuda\", \"mps\"])):\n",
    "            feat = clip_model.encode_image(x)\n",
    "        feat = feat.float().cpu().numpy()\n",
    "        feat = normalize_rows(feat)\n",
    "        return feat  # (1, D)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    def compute_pose_for_query(self, kp_vec: Optional[np.ndarray]) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        You pass in a (D_pose,) vector computed from YOLO keypoints.\n",
    "        This function simply normalizes it.\n",
    "        \"\"\"\n",
    "        if kp_vec is None or not self.has_pose:\n",
    "            return None\n",
    "        v = kp_vec.astype(\"float32\")[None, :]\n",
    "        v = normalize_rows(v)\n",
    "        return v  # (1, D_pose)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    def match(\n",
    "        self,\n",
    "        clip_query_vec: np.ndarray,\n",
    "        pose_query_vec: Optional[np.ndarray] = None,\n",
    "        topk: Optional[int] = None,\n",
    "    ) -> List[Dict]:\n",
    "\n",
    "        if topk is None:\n",
    "            topk = self.topk_default\n",
    "\n",
    "        # --- 1) CLIP similarity\n",
    "        sim_clip = (self.clip_emb @ clip_query_vec.T).reshape(-1)\n",
    "\n",
    "        # --- 2) Pose similarity (if enabled)\n",
    "        if self.has_pose and pose_query_vec is not None:\n",
    "            sim_pose = (self.pose_emb @ pose_query_vec.T).reshape(-1)\n",
    "        else:\n",
    "            sim_pose = None\n",
    "\n",
    "        # --- 3) Hybrid scoring\n",
    "        if sim_pose is not None:\n",
    "            w = self.pose_weight\n",
    "            final = (1.0 - w) * sim_clip + w * sim_pose\n",
    "        else:\n",
    "            final = sim_clip\n",
    "\n",
    "        # --- 4) Sort\n",
    "        idx = np.argsort(final)[::-1][:topk]\n",
    "\n",
    "        # --- 5) Pack results\n",
    "        out = []\n",
    "        for i in idx:\n",
    "            row = self.meta[i]\n",
    "            out.append(\n",
    "                dict(\n",
    "                    filename=row.get(\"filename\") or row.get(\"file\"),\n",
    "                    score=float(final[i]),\n",
    "                    clip=float(sim_clip[i]),\n",
    "                    pose=float(sim_pose[i]) if sim_pose is not None else None,\n",
    "                    meta=row,\n",
    "                )\n",
    "            )\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb50b6",
   "metadata": {},
   "source": [
    "app_frontend.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101aa646",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_webrtc import (\n",
    "    WebRtcMode,\n",
    "    RTCConfiguration,\n",
    "    VideoProcessorBase,\n",
    "    webrtc_streamer,\n",
    ")\n",
    "\n",
    "# =================== PATHS & CONSTANTS ===================\n",
    "\n",
    "FRONTEND_DIR = Path(__file__).resolve().parent\n",
    "ROOT_DIR = FRONTEND_DIR.parent\n",
    "\n",
    "# Use GLOBAL MIXED INDEX (local + met merged)\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"mixed\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "# Meta CSV candidates (fallback to local if needed)\n",
    "META_CSV_CANDIDATES = [\n",
    "    DATA_DIR / \"embeddings_meta.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works_enhanced_english.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works.csv\",\n",
    "]\n",
    "\n",
    "DEFAULT_API_URL = \"http://127.0.0.1:8000/match\"\n",
    "APP_TITLE = \"Embodied Aesthetic Reconstruction\"\n",
    "\n",
    "YOLO_MODEL_PATH = FRONTEND_DIR / \"yolov8n-pose.pt\"\n",
    "\n",
    "RTC_CONFIGURATION = RTCConfiguration(\n",
    "    {\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]}\n",
    ")\n",
    "\n",
    "STILLNESS_SEC = 3.5\n",
    "MAX_BUF_SEC = 5.0\n",
    "FPS_ASSUMED = 12\n",
    "MOTION_EPS_CXCY = 4.0\n",
    "MOTION_EPS_AREA = 0.03\n",
    "MIN_FACE_AREA = 0.06\n",
    "\n",
    "YELLOW = (255, 235, 59)\n",
    "BLACK = (0, 0, 0)\n",
    "HOT_PINK = (255, 30, 180)\n",
    "\n",
    "RIGHT_IMG_MAXW = 900\n",
    "\n",
    "# Diversity / randomness control\n",
    "MAX_RECENT = 6          # how many artworks to remember\n",
    "PRIMARY_TOP_K = 3       # \"very best\" range\n",
    "TAIL_TOP_K = 10         # search in top-10 in total\n",
    "TAIL_RANDOM_PROB = 0.3  # 30% chance to look into 4â€“10 first\n",
    "\n",
    "\n",
    "# =================== UTILITIES ===================\n",
    "\n",
    "_META_CACHE: Optional[Dict[str, Dict]] = None\n",
    "\n",
    "\n",
    "def load_meta_mapping() -> Dict[str, Dict]:\n",
    "    \"\"\"Load CSV â†’ filename â†’ metadata mapping.\"\"\"\n",
    "    global _META_CACHE\n",
    "    if _META_CACHE is not None:\n",
    "        return _META_CACHE\n",
    "\n",
    "    import csv\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    for p in META_CSV_CANDIDATES:\n",
    "        if p.exists():\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                rows = list(csv.DictReader(f))\n",
    "            break\n",
    "\n",
    "    mapping: Dict[str, Dict] = {}\n",
    "    for r in rows:\n",
    "        fname = (\n",
    "            r.get(\"filename\")\n",
    "            or r.get(\"image_path\")\n",
    "            or r.get(\"path\")\n",
    "            or r.get(\"file\")\n",
    "        )\n",
    "        if fname:\n",
    "            mapping[str(fname)] = r\n",
    "\n",
    "    _META_CACHE = mapping\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def lookup_meta(filename: str) -> Dict:\n",
    "    mapping = load_meta_mapping()\n",
    "    return mapping.get(filename, {})\n",
    "\n",
    "\n",
    "def safe_open_image(p: Path) -> Optional[Image.Image]:\n",
    "    try:\n",
    "        return Image.open(p).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_image_path(filename: str) -> Optional[Path]:\n",
    "    if not filename:\n",
    "        return None\n",
    "    p = Path(filename)\n",
    "    if not p.is_absolute():\n",
    "        p = IMAGES_DIR / p\n",
    "    return p if p.exists() else None\n",
    "\n",
    "\n",
    "def _load_font(size: int = 40):\n",
    "    \"\"\"Try Courier â†’ Arial â†’ default.\"\"\"\n",
    "    candidates = [\n",
    "        \"/Library/Fonts/Courier New.ttf\",\n",
    "        \"/System/Library/Fonts/Courier.dfont\",\n",
    "        \"/System/Library/Fonts/Supplemental/Courier New.ttf\",\n",
    "        \"/Library/Fonts/Arial.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                return ImageFont.truetype(p, size=size)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def draw_tiny_metrics_top_right(\n",
    "    im: Image.Image, lines: List[str], size: int = 16, margin: int = 10\n",
    ") -> Image.Image:\n",
    "    \"\"\"Small pink pose metrics in top-right corner of the image.\"\"\"\n",
    "    if not lines:\n",
    "        return im\n",
    "    img = im.copy()\n",
    "    d = ImageDraw.Draw(img)\n",
    "    font = _load_font(size)\n",
    "\n",
    "    widths = []\n",
    "    for s in lines:\n",
    "        l, t, r, b = d.textbbox((0, 0), s, font=font)\n",
    "        widths.append(r - l)\n",
    "    wmax = max(widths) if widths else 0\n",
    "\n",
    "    x = img.width - margin - wmax\n",
    "    y = margin\n",
    "    for s in lines:\n",
    "        d.text((x, y), s, fill=HOT_PINK, font=font)\n",
    "        _, _, _, b = d.textbbox((0, 0), s, font=font)\n",
    "        y += int(b * 0.95)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _angle_deg(p1, p2):\n",
    "    if p1 is None or p2 is None:\n",
    "        return None\n",
    "    vx, vy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "    return float(np.degrees(np.arctan2(vy, vx)))\n",
    "\n",
    "\n",
    "def _elbow_angle(shoulder, elbow, wrist):\n",
    "    if None in (shoulder, elbow, wrist):\n",
    "        return None\n",
    "    v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]], float)\n",
    "    v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]], float)\n",
    "    n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if n1 < 1e-5 or n2 < 1e-5:\n",
    "        return None\n",
    "    cosv = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cosv)))\n",
    "\n",
    "\n",
    "def format_metrics(kps: Dict[int, Tuple[float, float] | None]) -> List[str]:\n",
    "    le, re = kps.get(1), kps.get(2)\n",
    "    lsh, rsh = kps.get(5), kps.get(6)\n",
    "    lel, rel = kps.get(7), kps.get(8)\n",
    "    lwr, rwr = kps.get(9), kps.get(10)\n",
    "\n",
    "    fdeg = lambda v: \"â€”\" if v is None else f\"{v:+.1f}Â°\"\n",
    "    fpt = lambda p: \"(0, 0)\" if p is None else f\"({int(p[0])}, {int(p[1])})\"\n",
    "\n",
    "    return [\n",
    "        f\"Head tilt: {fdeg(_angle_deg(re, le))}\",\n",
    "        f\"Shoulder:  {fdeg(_angle_deg(rsh, lsh))}\",\n",
    "        f\"L elbow:   {fdeg(_elbow_angle(lsh, lel, lwr))}\",\n",
    "        f\"R elbow:   {fdeg(_elbow_angle(rsh, rel, rwr))}\",\n",
    "        f\"L wrist:   {fpt(lwr)}\",\n",
    "        f\"R wrist:   {fpt(rwr)}\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def overlay_right_labels(painting: Image.Image, meta: Dict) -> Image.Image:\n",
    "    \"\"\"Yellow label stack: price, year, artist.\"\"\"\n",
    "    im = painting.convert(\"RGB\").copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    font_big = _load_font(44)\n",
    "    font_small = _load_font(36)\n",
    "\n",
    "    price = (\n",
    "        meta.get(\"price_text\")\n",
    "        or meta.get(\"auction_price_usd\")\n",
    "        or meta.get(\"price\")\n",
    "        or \"â€”\"\n",
    "    )\n",
    "    year = str(meta.get(\"year\") or \"â€”\")\n",
    "    artist = meta.get(\"artist\") or \"artist name\"\n",
    "\n",
    "    lines = [price, year, artist]\n",
    "    fonts = [font_big, font_small, font_big]\n",
    "\n",
    "    margin_x = 24\n",
    "    y = int(im.height * 0.50)\n",
    "\n",
    "    for text, font in zip(lines, fonts):\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font)\n",
    "        w, h = r - l, b - t\n",
    "\n",
    "        pad_x, pad_y = 16, 10\n",
    "        box_w, box_h = w + 2 * pad_x, h + 2 * pad_y\n",
    "\n",
    "        x = margin_x\n",
    "        draw.rectangle([x, y, x + box_w, y + box_h], fill=YELLOW)\n",
    "        draw.text((x + pad_x, y + pad_y), text, fill=BLACK, font=font)\n",
    "\n",
    "        y += box_h + 10\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def force_rerun():\n",
    "    \"\"\"Compatible with new/old Streamlit.\"\"\"\n",
    "    try:\n",
    "        st.rerun()\n",
    "    except Exception:\n",
    "        try:\n",
    "            st.experimental_rerun()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =================== YOLO VIDEO PROCESSOR ===================\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "    HAS_YOLO = True\n",
    "except Exception:\n",
    "    HAS_YOLO = False\n",
    "\n",
    "\n",
    "class CuratorialProcessor(VideoProcessorBase):\n",
    "    \"\"\"\n",
    "    YOLOv8-Pose:\n",
    "    - blue bbox\n",
    "    - green skeleton\n",
    "    - pink tiny pose metrics\n",
    "    - stillness-based auto capture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        if HAS_YOLO:\n",
    "            try:\n",
    "                local = YOLO_MODEL_PATH\n",
    "                self.model = YOLO(str(local if local.exists() else \"yolov8n-pose.pt\"))\n",
    "            except Exception:\n",
    "                self.model = None\n",
    "\n",
    "        maxlen = max(6, int(MAX_BUF_SEC * FPS_ASSUMED))\n",
    "        self.cx_buf = deque(maxlen=maxlen)\n",
    "        self.cy_buf = deque(maxlen=maxlen)\n",
    "        self.area_buf = deque(maxlen=maxlen)\n",
    "        self.last_stable_ts: Optional[float] = None\n",
    "\n",
    "        self.captured_img: Optional[Image.Image] = None\n",
    "        self.captured_ts: float = 0.0\n",
    "        self.captured_metrics: List[str] = []\n",
    "\n",
    "        self.latest_rgb: Optional[np.ndarray] = None\n",
    "        self.last_metrics_lines: List[str] = []\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _detect_bbox(self, rgb: np.ndarray) -> Optional[Tuple[int, int, int, int]]:\n",
    "        if not self.model:\n",
    "            return None\n",
    "        res = self.model.predict(rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "        if len(res) == 0 or res[0].boxes is None:\n",
    "            return None\n",
    "        b = res[0].boxes.xyxy\n",
    "        if b is None or len(b) == 0:\n",
    "            return None\n",
    "        b = b.cpu().numpy()\n",
    "        areas = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "        i = int(np.argmax(areas))\n",
    "        x1, y1, x2, y2 = b[i].astype(int).tolist()\n",
    "        return x1, y1, x2, y2\n",
    "\n",
    "    def _extract_keypoints(self, res) -> Dict[int, Tuple[float, float] | None]:\n",
    "        kps: Dict[int, Tuple[float, float] | None] = {}\n",
    "        try:\n",
    "            if res and res[0].keypoints is not None and len(res[0].keypoints) > 0:\n",
    "                xy = res[0].keypoints.xy[0].cpu().numpy()\n",
    "                for i in range(xy.shape[0]):\n",
    "                    kps[i] = (float(xy[i, 0]), float(xy[i, 1]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return kps\n",
    "\n",
    "    def _update_stillness(self, bbox, w, h) -> bool:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cx = 0.5 * (x1 + x2)\n",
    "        cy = 0.5 * (y1 + y2)\n",
    "        area = max(1.0, (x2 - x1) * (y2 - y1))\n",
    "        rel_area = area / float(w * h)\n",
    "\n",
    "        if rel_area < MIN_FACE_AREA:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        self.cx_buf.append(cx)\n",
    "        self.cy_buf.append(cy)\n",
    "        self.area_buf.append(rel_area)\n",
    "\n",
    "        need_len = int(STILLNESS_SEC * FPS_ASSUMED * 0.6)\n",
    "        if len(self.cx_buf) < max(3, need_len):\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        stdx = float(np.std(self.cx_buf))\n",
    "        stdy = float(np.std(self.cy_buf))\n",
    "        stda = float(np.std(self.area_buf))\n",
    "        stable_now = (\n",
    "            stdx < MOTION_EPS_CXCY\n",
    "            and stdy < MOTION_EPS_CXCY\n",
    "            and stda < MOTION_EPS_AREA\n",
    "        )\n",
    "\n",
    "        now = time.time()\n",
    "        if stable_now:\n",
    "            if self.last_stable_ts is None:\n",
    "                self.last_stable_ts = now\n",
    "            return (now - self.last_stable_ts) >= STILLNESS_SEC\n",
    "        else:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "    def _stamp_capture(self):\n",
    "        self.captured_img = Image.fromarray(self.latest_rgb)\n",
    "        self.captured_ts = time.time()\n",
    "        self.captured_metrics = list(self.last_metrics_lines)\n",
    "        self.last_stable_ts = None\n",
    "\n",
    "    def capture_now(self) -> bool:\n",
    "        if self.latest_rgb is None:\n",
    "            return False\n",
    "        with self.lock:\n",
    "            self._stamp_capture()\n",
    "        return True\n",
    "\n",
    "    def recv(self, frame):\n",
    "        import av\n",
    "\n",
    "        img_bgr = frame.to_ndarray(format=\"bgr24\")\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        h, w, _ = img_rgb.shape\n",
    "        self.latest_rgb = img_rgb\n",
    "\n",
    "        if self.model:\n",
    "            res = self.model.predict(img_rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "            plotted = res[0].plot()[:, :, ::-1]\n",
    "            kps = self._extract_keypoints(res)\n",
    "            lines = format_metrics(kps)\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = lines\n",
    "            pil = Image.fromarray(plotted)\n",
    "            pil = draw_tiny_metrics_top_right(pil, lines, size=16, margin=10)\n",
    "            out_rgb = np.array(pil)\n",
    "        else:\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = [\"(pose model unavailable)\"]\n",
    "            out_rgb = img_rgb\n",
    "\n",
    "        bbox = self._detect_bbox(img_rgb) if self.model else None\n",
    "        if bbox and self._update_stillness(bbox, w, h):\n",
    "            with self.lock:\n",
    "                if time.time() - self.captured_ts > 0.35:\n",
    "                    self._stamp_capture()\n",
    "\n",
    "        out_bgr = out_rgb[:, :, ::-1]\n",
    "        return av.VideoFrame.from_ndarray(out_bgr, format=\"bgr24\")\n",
    "\n",
    "\n",
    "# =================== PAGE LAYOUT ===================\n",
    "\n",
    "st.set_page_config(page_title=APP_TITLE, layout=\"wide\")\n",
    "\n",
    "st.markdown(\n",
    "    f\"\"\"\n",
    "<style>\n",
    "section[data-testid=\"stSidebar\"] {{ display: none !important; }}\n",
    "header, footer, [data-testid=\"stToolbar\"] {{ visibility: hidden !important; }}\n",
    "\n",
    "body {{\n",
    "  background-color: #f5f5f7;\n",
    "}}\n",
    ".block-container {{\n",
    "  padding-top: 0.6rem;\n",
    "  padding-bottom: 0.6rem;\n",
    "  max-width: 1700px;\n",
    "}}\n",
    "\n",
    "h1 {{\n",
    "  font-family: -apple-system, BlinkMacSystemFont, \"SF Pro Display\", system-ui, sans-serif;\n",
    "  letter-spacing: 0.04em;\n",
    "  font-weight: 700;\n",
    "}}\n",
    "\n",
    ".left-col .cam-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;\n",
    "  width: 100%;\n",
    "  overflow: hidden;\n",
    "  border-radius: 18px;\n",
    "  background: #111;\n",
    "  box-shadow: 0 18px 45px rgba(0,0,0,0.3);\n",
    "}}\n",
    ".left-col .cam-wrap video {{\n",
    "  height: 100% !important;\n",
    "  width: auto !important;\n",
    "  object-fit: cover !important;\n",
    "  border-radius: 18px !important;\n",
    "}}\n",
    "\n",
    ".right-col .art-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;\n",
    "  max-width: {RIGHT_IMG_MAXW}px;\n",
    "  overflow: hidden;\n",
    "  margin: 0 auto;\n",
    "  border-radius: 18px;\n",
    "  background: #050505;\n",
    "  box-shadow: 0 18px 45px rgba(0,0,0,0.35);\n",
    "}}\n",
    ".right-col .art-wrap img {{\n",
    "  display: block;\n",
    "  width: 100% !important;\n",
    "  height: 100% !important;\n",
    "  object-fit: cover !important;\n",
    "}}\n",
    "\n",
    "button[kind=\"secondary\"] {{\n",
    "  border-radius: 999px !important;\n",
    "}}\n",
    "\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "st.title(APP_TITLE)\n",
    "st.caption(\n",
    "    \"Hold still for ~3â€“5 seconds to auto-capture, or press the button to capture manually. \"\n",
    "    \"Left: live video with pose. Right: matched artwork with tiny pink pose metrics.\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    st.autorefresh(interval=700, key=\"ear_auto\", limit=None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "left, right = st.columns([1, 1], gap=\"large\")\n",
    "\n",
    "if \"countdown_target\" not in st.session_state:\n",
    "    st.session_state[\"countdown_target\"] = None\n",
    "if \"last_match\" not in st.session_state:\n",
    "    st.session_state[\"last_match\"] = None\n",
    "if \"last_metrics\" not in st.session_state:\n",
    "    st.session_state[\"last_metrics\"] = []\n",
    "if \"last_ts\" not in st.session_state:\n",
    "    st.session_state[\"last_ts\"] = 0.0\n",
    "if \"recent_files\" not in st.session_state:\n",
    "    st.session_state[\"recent_files\"] = []  # type: List[str]\n",
    "\n",
    "API_URL = DEFAULT_API_URL\n",
    "\n",
    "\n",
    "# =================== LEFT PANEL ===================\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"Live\")\n",
    "    st.markdown('<div class=\"left-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"cam-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ctx = webrtc_streamer(\n",
    "        key=\"ear-curatorial-final\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "        video_processor_factory=CuratorialProcessor,\n",
    "        async_processing=True,\n",
    "    )\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"<div style='height:10px'></div>\", unsafe_allow_html=True)\n",
    "\n",
    "    c1, c2 = st.columns(2)\n",
    "\n",
    "    with c1:\n",
    "        if st.button(\"ğŸ“¸ Capture (wait 3s)\", use_container_width=True):\n",
    "            st.session_state[\"countdown_target\"] = time.time() + 3.0\n",
    "\n",
    "    with c2:\n",
    "        if st.button(\"âš¡ Instant Capture\", use_container_width=True):\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"Try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "\n",
    "    if st.session_state[\"countdown_target\"]:\n",
    "        remain = st.session_state[\"countdown_target\"] - time.time()\n",
    "        if remain > 0:\n",
    "            st.info(f\"Capturing in {remain:.1f}sâ€¦ Hold still.\")\n",
    "        else:\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"Try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "            st.session_state[\"countdown_target\"] = None\n",
    "            force_rerun()\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# =================== RIGHT PANEL ===================\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"Matched artwork\")\n",
    "    st.markdown('<div class=\"right-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"art-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ph = st.empty()\n",
    "\n",
    "    proc: Optional[CuratorialProcessor] = None\n",
    "    if ctx and ctx.video_processor:\n",
    "        proc = ctx.video_processor\n",
    "\n",
    "    if not proc:\n",
    "        ph.info(\"Initializing cameraâ€¦\")\n",
    "    else:\n",
    "        with proc.lock:\n",
    "            cap_img = proc.captured_img\n",
    "            cap_ts = getattr(proc, \"captured_ts\", 0.0)\n",
    "            cap_metrics = list(getattr(proc, \"captured_metrics\", []))\n",
    "\n",
    "        if cap_img is not None and cap_ts > st.session_state[\"last_ts\"]:\n",
    "            st.session_state[\"last_ts\"] = cap_ts\n",
    "            st.session_state[\"last_metrics\"] = cap_metrics\n",
    "\n",
    "            buf = io.BytesIO()\n",
    "            cap_img.save(buf, format=\"JPEG\")\n",
    "            buf.seek(0)\n",
    "\n",
    "            files = {\"image\": (\"frame.jpg\", buf.getvalue(), \"image/jpeg\")}\n",
    "            # Ask backend for Top-10, we'll do diversity selection here\n",
    "            data = {\"museum\": \"mixed\", \"topk\": TAIL_TOP_K}\n",
    "\n",
    "            try:\n",
    "                resp = requests.post(API_URL, files=files, data=data, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                st.session_state[\"last_match\"] = resp.json()\n",
    "            except Exception as exc:\n",
    "                st.session_state[\"last_match\"] = {\"error\": str(exc)}\n",
    "\n",
    "            force_rerun()\n",
    "\n",
    "        payload = st.session_state.get(\"last_match\")\n",
    "\n",
    "        if not payload:\n",
    "            ph.info(\"Hold still or press capture to matchâ€¦\")\n",
    "        elif \"error\" in payload:\n",
    "            ph.error(f\"Backend error: {payload['error']}\")\n",
    "        else:\n",
    "            results = payload.get(\"results\") or []\n",
    "            if not results:\n",
    "                ph.warning(\"No matches returned.\")\n",
    "            else:\n",
    "                recent = st.session_state.get(\"recent_files\", [])\n",
    "\n",
    "                # ---------- diversity + mild randomness ----------\n",
    "                # Decide whether to start looking from the tail (ranks 4â€“10)\n",
    "                use_tail_first = random.random() < TAIL_RANDOM_PROB and len(results) > PRIMARY_TOP_K\n",
    "                primary_slice = results[:PRIMARY_TOP_K]\n",
    "                tail_slice = results[PRIMARY_TOP_K:TAIL_TOP_K]\n",
    "\n",
    "                ordered_candidates = (\n",
    "                    (tail_slice + primary_slice) if use_tail_first else (primary_slice + tail_slice)\n",
    "                )\n",
    "\n",
    "                chosen = None\n",
    "                chosen_fname = None\n",
    "\n",
    "                for r in ordered_candidates:\n",
    "                    fname = (\n",
    "                        r.get(\"filename\")\n",
    "                        or r.get(\"file\")\n",
    "                        or r.get(\"image_path\")\n",
    "                        or r.get(\"path\")\n",
    "                    )\n",
    "                    if not fname:\n",
    "                        continue\n",
    "                    if fname not in recent:\n",
    "                        chosen = r\n",
    "                        chosen_fname = fname\n",
    "                        break\n",
    "\n",
    "                # Fallback: if all candidates are \"recent\", use strict Top-1\n",
    "                if chosen is None:\n",
    "                    chosen = results[0]\n",
    "                    chosen_fname = (\n",
    "                        chosen.get(\"filename\")\n",
    "                        or chosen.get(\"file\")\n",
    "                        or chosen.get(\"image_path\")\n",
    "                        or chosen.get(\"path\")\n",
    "                    )\n",
    "\n",
    "                # Update recent memory\n",
    "                if chosen_fname:\n",
    "                    if chosen_fname in recent:\n",
    "                        recent.remove(chosen_fname)\n",
    "                    recent.insert(0, chosen_fname)\n",
    "                    del recent[MAX_RECENT:]\n",
    "                    st.session_state[\"recent_files\"] = recent\n",
    "\n",
    "                filename = chosen_fname\n",
    "\n",
    "                img_path = ensure_image_path(filename or \"\")\n",
    "                if not img_path:\n",
    "                    ph.error(f\"Image not found: {filename}\")\n",
    "                else:\n",
    "                    painting = safe_open_image(img_path)\n",
    "                    if painting is None:\n",
    "                        ph.error(f\"Failed to open: {img_path}\")\n",
    "                    else:\n",
    "                        meta_row = lookup_meta(str(filename))\n",
    "                        meta = {\n",
    "                            \"artist\": meta_row.get(\"artist\")\n",
    "                            or chosen.get(\"artist\")\n",
    "                            or \"artist name\",\n",
    "                            \"year\": meta_row.get(\"year\") or chosen.get(\"year\") or \"\",\n",
    "                            \"price_text\": meta_row.get(\"price_text\")\n",
    "                            or meta_row.get(\"auction_price_usd\")\n",
    "                            or \"\",\n",
    "                        }\n",
    "\n",
    "                        painted = overlay_right_labels(painting, meta)\n",
    "                        metrics = st.session_state.get(\"last_metrics\") or []\n",
    "                        painted = draw_tiny_metrics_top_right(painted, metrics, size=16, margin=12)\n",
    "\n",
    "                        w = min(RIGHT_IMG_MAXW, painted.width)\n",
    "                        caption = f\"{meta_row.get('title','')} â€” {meta.get('artist','')}\"\n",
    "                        ph.image(painted, caption=caption, width=w)\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893c904",
   "metadata": {},
   "source": [
    "app_frontend.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fab6d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_webrtc import (\n",
    "    WebRtcMode,\n",
    "    RTCConfiguration,\n",
    "    VideoProcessorBase,\n",
    "    webrtc_streamer,\n",
    ")\n",
    "\n",
    "# =================== PATHS & CONSTANTS ===================\n",
    "\n",
    "FRONTEND_DIR = Path(__file__).resolve().parent\n",
    "ROOT_DIR = FRONTEND_DIR.parent\n",
    "\n",
    "# Use GLOBAL MIXED INDEX (local + met + aic merged)\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"mixed\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "# All museum image roots (search order)\n",
    "MUSEUM_IMAGE_DIRS = [\n",
    "    DATA_DIR / \"images\",                    # mixed\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"images\",\n",
    "    ROOT_DIR / \"data\" / \"met\" / \"images\",\n",
    "    ROOT_DIR / \"data\" / \"aic\" / \"images\",\n",
    "]\n",
    "\n",
    "# Meta CSV candidates (fallback to local if needed)\n",
    "META_CSV_CANDIDATES = [\n",
    "    DATA_DIR / \"embeddings_meta.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works_enhanced_english.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works.csv\",\n",
    "]\n",
    "\n",
    "DEFAULT_API_URL = \"http://127.0.0.1:8000/match\"\n",
    "APP_TITLE = \"Embodied Aesthetic Reconstruction\"\n",
    "\n",
    "YOLO_MODEL_PATH = FRONTEND_DIR / \"yolov8n-pose.pt\"\n",
    "\n",
    "RTC_CONFIGURATION = RTCConfiguration(\n",
    "    {\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]}\n",
    ")\n",
    "\n",
    "STILLNESS_SEC = 3.5\n",
    "MAX_BUF_SEC = 5.0\n",
    "FPS_ASSUMED = 12\n",
    "MOTION_EPS_CXCY = 4.0\n",
    "MOTION_EPS_AREA = 0.03\n",
    "MIN_FACE_AREA = 0.06\n",
    "\n",
    "YELLOW = (255, 235, 59)\n",
    "BLACK = (0, 0, 0)\n",
    "HOT_PINK = (255, 30, 180)\n",
    "\n",
    "RIGHT_IMG_MAXW = 900\n",
    "\n",
    "# Diversity / randomness control\n",
    "MAX_RECENT = 6          # how many artworks to remember\n",
    "PRIMARY_TOP_K = 3       # \"very best\" range\n",
    "TAIL_TOP_K = 10         # search in top-10 in total\n",
    "TAIL_RANDOM_PROB = 0.3  # 30% chance to look into 4â€“10 first\n",
    "\n",
    "\n",
    "# =================== UTILITIES ===================\n",
    "\n",
    "_META_CACHE: Optional[Dict[str, Dict]] = None\n",
    "\n",
    "\n",
    "def load_meta_mapping() -> Dict[str, Dict]:\n",
    "    \"\"\"Load CSV â†’ filename â†’ metadata mapping.\"\"\"\n",
    "    global _META_CACHE\n",
    "    if _META_CACHE is not None:\n",
    "        return _META_CACHE\n",
    "\n",
    "    import csv\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    for p in META_CSV_CANDIDATES:\n",
    "        if p.exists():\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                rows = list(csv.DictReader(f))\n",
    "            break\n",
    "\n",
    "    mapping: Dict[str, Dict] = {}\n",
    "    for r in rows:\n",
    "        fname = (\n",
    "            r.get(\"filename\")\n",
    "            or r.get(\"image_path\")\n",
    "            or r.get(\"path\")\n",
    "            or r.get(\"file\")\n",
    "        )\n",
    "        if fname:\n",
    "            mapping[str(fname)] = r\n",
    "\n",
    "    _META_CACHE = mapping\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def lookup_meta(filename: str) -> Dict:\n",
    "    mapping = load_meta_mapping()\n",
    "    return mapping.get(filename, {})\n",
    "\n",
    "\n",
    "def safe_open_image(p: Path) -> Optional[Image.Image]:\n",
    "    try:\n",
    "        return Image.open(p).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_image_path(filename: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Resolve an artwork filename to a real image path.\n",
    "\n",
    "    Strategy:\n",
    "      1) If it's an absolute existing path â†’ return.\n",
    "      2) If it has subdirectories, try relative to project root / data.\n",
    "      3) Otherwise, search by basename in all known museum image dirs.\n",
    "    \"\"\"\n",
    "    if not filename:\n",
    "        return None\n",
    "\n",
    "    p = Path(filename)\n",
    "\n",
    "    # 1) absolute path already\n",
    "    if p.is_absolute() and p.exists():\n",
    "        return p\n",
    "\n",
    "    candidates: List[Path] = []\n",
    "\n",
    "    # 2) has subdirectory component (e.g. \"aic/images/aic_4939.jpg\")\n",
    "    if p.parent != Path(\".\"):\n",
    "        candidates.append(ROOT_DIR / p)\n",
    "        candidates.append(ROOT_DIR / \"data\" / p)\n",
    "\n",
    "    # 3) search by basename across all museum image roots\n",
    "    basename = p.name\n",
    "    for root in MUSEUM_IMAGE_DIRS:\n",
    "        candidates.append(root / basename)\n",
    "\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return c\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _load_font(size: int = 40):\n",
    "    \"\"\"Try Courier â†’ Arial â†’ default.\"\"\"\n",
    "    candidates = [\n",
    "        \"/Library/Fonts/Courier New.ttf\",\n",
    "        \"/System/Library/Fonts/Courier.dfont\",\n",
    "        \"/System/Library/Fonts/Supplemental/Courier New.ttf\",\n",
    "        \"/Library/Fonts/Arial.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                return ImageFont.truetype(p, size=size)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def draw_tiny_metrics_top_right(\n",
    "    im: Image.Image, lines: List[str], size: int = 16, margin: int = 10\n",
    ") -> Image.Image:\n",
    "    \"\"\"Small pink pose metrics in top-right corner of the image.\"\"\"\n",
    "    if not lines:\n",
    "        return im\n",
    "    img = im.copy()\n",
    "    d = ImageDraw.Draw(img)\n",
    "    font = _load_font(size)\n",
    "\n",
    "    widths = []\n",
    "    for s in lines:\n",
    "        l, t, r, b = d.textbbox((0, 0), s, font=font)\n",
    "        widths.append(r - l)\n",
    "    wmax = max(widths) if widths else 0\n",
    "\n",
    "    x = img.width - margin - wmax\n",
    "    y = margin\n",
    "    for s in lines:\n",
    "        d.text((x, y), s, fill=HOT_PINK, font=font)\n",
    "        _, _, _, b = d.textbbox((0, 0), s, font=font)\n",
    "        y += int(b * 0.95)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _angle_deg(p1, p2):\n",
    "    if p1 is None or p2 is None:\n",
    "        return None\n",
    "    vx, vy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "    return float(np.degrees(np.arctan2(vy, vx)))\n",
    "\n",
    "\n",
    "def _elbow_angle(shoulder, elbow, wrist):\n",
    "    if None in (shoulder, elbow, wrist):\n",
    "        return None\n",
    "    v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]], float)\n",
    "    v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]], float)\n",
    "    n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if n1 < 1e-5 or n2 < 1e-5:\n",
    "        return None\n",
    "    cosv = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cosv)))\n",
    "\n",
    "\n",
    "def format_metrics(kps: Dict[int, Tuple[float, float] | None]) -> List[str]:\n",
    "    le, re = kps.get(1), kps.get(2)\n",
    "    lsh, rsh = kps.get(5), kps.get(6)\n",
    "    lel, rel = kps.get(7), kps.get(8)\n",
    "    lwr, rwr = kps.get(9), kps.get(10)\n",
    "\n",
    "    fdeg = lambda v: \"â€”\" if v is None else f\"{v:+.1f}Â°\"\n",
    "    fpt = lambda p: \"(0, 0)\" if p is None else f\"({int(p[0])}, {int(p[1])})\"\n",
    "\n",
    "    return [\n",
    "        f\"Head tilt: {fdeg(_angle_deg(re, le))}\",\n",
    "        f\"Shoulder:  {fdeg(_angle_deg(rsh, lsh))}\",\n",
    "        f\"L elbow:   {fdeg(_elbow_angle(lsh, lel, lwr))}\",\n",
    "        f\"R elbow:   {fdeg(_elbow_angle(rsh, rel, rwr))}\",\n",
    "        f\"L wrist:   {fpt(lwr)}\",\n",
    "        f\"R wrist:   {fpt(rwr)}\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def overlay_right_labels(painting: Image.Image, meta: Dict) -> Image.Image:\n",
    "    \"\"\"Yellow label stack: price, year, artist.\"\"\"\n",
    "    im = painting.convert(\"RGB\").copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    font_big = _load_font(44)\n",
    "    font_small = _load_font(36)\n",
    "\n",
    "    price = (\n",
    "        meta.get(\"price_text\")\n",
    "        or meta.get(\"auction_price_usd\")\n",
    "        or meta.get(\"price\")\n",
    "        or \"â€”\"\n",
    "    )\n",
    "    year = str(meta.get(\"year\") or \"â€”\")\n",
    "    artist = meta.get(\"artist\") or \"artist name\"\n",
    "\n",
    "    lines = [price, year, artist]\n",
    "    fonts = [font_big, font_small, font_big]\n",
    "\n",
    "    margin_x = 24\n",
    "    y = int(im.height * 0.50)\n",
    "\n",
    "    for text, font in zip(lines, fonts):\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font)\n",
    "        w, h = r - l, b - t\n",
    "\n",
    "        pad_x, pad_y = 16, 10\n",
    "        box_w, box_h = w + 2 * pad_x, h + 2 * pad_y\n",
    "\n",
    "        x = margin_x\n",
    "        draw.rectangle([x, y, x + box_w, y + box_h], fill=YELLOW)\n",
    "        draw.text((x + pad_x, y + pad_y), text, fill=BLACK, font=font)\n",
    "\n",
    "        y += box_h + 10\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def force_rerun():\n",
    "    \"\"\"Compatible with new/old Streamlit.\"\"\"\n",
    "    try:\n",
    "        st.rerun()\n",
    "    except Exception:\n",
    "        try:\n",
    "            st.experimental_rerun()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =================== YOLO VIDEO PROCESSOR ===================\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "    HAS_YOLO = True\n",
    "except Exception:\n",
    "    HAS_YOLO = False\n",
    "\n",
    "\n",
    "class CuratorialProcessor(VideoProcessorBase):\n",
    "    \"\"\"\n",
    "    YOLOv8-Pose:\n",
    "    - blue bbox\n",
    "    - green skeleton\n",
    "    - pink tiny pose metrics\n",
    "    - stillness-based auto capture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        if HAS_YOLO:\n",
    "            try:\n",
    "                local = YOLO_MODEL_PATH\n",
    "                self.model = YOLO(str(local if local.exists() else \"yolov8n-pose.pt\"))\n",
    "            except Exception:\n",
    "                self.model = None\n",
    "\n",
    "        maxlen = max(6, int(MAX_BUF_SEC * FPS_ASSUMED))\n",
    "        self.cx_buf = deque(maxlen=maxlen)\n",
    "        self.cy_buf = deque(maxlen=maxlen)\n",
    "        self.area_buf = deque(maxlen=maxlen)\n",
    "        self.last_stable_ts: Optional[float] = None\n",
    "\n",
    "        self.captured_img: Optional[Image.Image] = None\n",
    "        self.captured_ts: float = 0.0\n",
    "        self.captured_metrics: List[str] = []\n",
    "\n",
    "        self.latest_rgb: Optional[np.ndarray] = None\n",
    "        self.last_metrics_lines: List[str] = []\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _detect_bbox(self, rgb: np.ndarray) -> Optional[Tuple[int, int, int, int]]:\n",
    "        if not self.model:\n",
    "            return None\n",
    "        res = self.model.predict(rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "        if len(res) == 0 or res[0].boxes is None:\n",
    "            return None\n",
    "        b = res[0].boxes.xyxy\n",
    "        if b is None or len(b) == 0:\n",
    "            return None\n",
    "        b = b.cpu().numpy()\n",
    "        areas = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "        i = int(np.argmax(areas))\n",
    "        x1, y1, x2, y2 = b[i].astype(int).tolist()\n",
    "        return x1, y1, x2, y2\n",
    "\n",
    "    def _extract_keypoints(self, res) -> Dict[int, Tuple[float, float] | None]:\n",
    "        kps: Dict[int, Tuple[float, float] | None] = {}\n",
    "        try:\n",
    "            if res and res[0].keypoints is not None and len(res[0].keypoints) > 0:\n",
    "                xy = res[0].keypoints.xy[0].cpu().numpy()\n",
    "                for i in range(xy.shape[0]):\n",
    "                    kps[i] = (float(xy[i, 0]), float(xy[i, 1]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return kps\n",
    "\n",
    "    def _update_stillness(self, bbox, w, h) -> bool:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cx = 0.5 * (x1 + x2)\n",
    "        cy = 0.5 * (y1 + y2)\n",
    "        area = max(1.0, (x2 - x1) * (y2 - y1))\n",
    "        rel_area = area / float(w * h)\n",
    "\n",
    "        if rel_area < MIN_FACE_AREA:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        self.cx_buf.append(cx)\n",
    "        self.cy_buf.append(cy)\n",
    "        self.area_buf.append(rel_area)\n",
    "\n",
    "        need_len = int(STILLNESS_SEC * FPS_ASSUMED * 0.6)\n",
    "        if len(self.cx_buf) < max(3, need_len):\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        stdx = float(np.std(self.cx_buf))\n",
    "        stdy = float(np.std(self.cy_buf))\n",
    "        stda = float(np.std(self.area_buf))\n",
    "        stable_now = (\n",
    "            stdx < MOTION_EPS_CXCY\n",
    "            and stdy < MOTION_EPS_CXCY\n",
    "            and stda < MOTION_EPS_AREA\n",
    "        )\n",
    "\n",
    "        now = time.time()\n",
    "        if stable_now:\n",
    "            if self.last_stable_ts is None:\n",
    "                self.last_stable_ts = now\n",
    "            return (now - self.last_stable_ts) >= STILLNESS_SEC\n",
    "        else:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "    def _stamp_capture(self):\n",
    "        self.captured_img = Image.fromarray(self.latest_rgb)\n",
    "        self.captured_ts = time.time()\n",
    "        self.captured_metrics = list(self.last_metrics_lines)\n",
    "        self.last_stable_ts = None\n",
    "\n",
    "    def capture_now(self) -> bool:\n",
    "        if self.latest_rgb is None:\n",
    "            return False\n",
    "        with self.lock:\n",
    "            self._stamp_capture()\n",
    "        return True\n",
    "\n",
    "    def recv(self, frame):\n",
    "        import av\n",
    "\n",
    "        img_bgr = frame.to_ndarray(format=\"bgr24\")\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        h, w, _ = img_rgb.shape\n",
    "        self.latest_rgb = img_rgb\n",
    "\n",
    "        if self.model:\n",
    "            res = self.model.predict(img_rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "            plotted = res[0].plot()[:, :, ::-1]\n",
    "            kps = self._extract_keypoints(res)\n",
    "            lines = format_metrics(kps)\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = lines\n",
    "            pil = Image.fromarray(plotted)\n",
    "            pil = draw_tiny_metrics_top_right(pil, lines, size=16, margin=10)\n",
    "            out_rgb = np.array(pil)\n",
    "        else:\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = [\"(pose model unavailable)\"]\n",
    "            out_rgb = img_rgb\n",
    "\n",
    "        bbox = self._detect_bbox(img_rgb) if self.model else None\n",
    "        if bbox and self._update_stillness(bbox, w, h):\n",
    "            with self.lock:\n",
    "                if time.time() - self.captured_ts > 0.35:\n",
    "                    self._stamp_capture()\n",
    "\n",
    "        out_bgr = out_rgb[:, :, ::-1]\n",
    "        return av.VideoFrame.from_ndarray(out_bgr, format=\"bgr24\")\n",
    "\n",
    "\n",
    "# =================== PAGE LAYOUT ===================\n",
    "\n",
    "st.set_page_config(page_title=APP_TITLE, layout=\"wide\")\n",
    "\n",
    "st.markdown(\n",
    "    f\"\"\"\n",
    "<style>\n",
    "section[data-testid=\"stSidebar\"] {{ display: none !important; }}\n",
    "header, footer, [data-testid=\"stToolbar\"] {{ visibility: hidden !important; }}\n",
    "\n",
    "body {{\n",
    "  background-color: #f5f5f7;\n",
    "}}\n",
    ".block-container {{\n",
    "  padding-top: 0.6rem;\n",
    "  padding-bottom: 0.6rem;\n",
    "  max-width: 1700px;\n",
    "}}\n",
    "\n",
    "h1 {{\n",
    "  font-family: -apple-system, BlinkMacSystemFont, \"SF Pro Display\", system-ui, sans-serif;\n",
    "  letter-spacing: 0.04em;\n",
    "  font-weight: 700;\n",
    "}}\n",
    "\n",
    ".left-col .cam-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;\n",
    "  width: 100%;\n",
    "  overflow: hidden;\n",
    "  border-radius: 18px;\n",
    "  background: #111;\n",
    "  box-shadow: 0 18px 45px rgba(0,0,0,0.3);\n",
    "}}\n",
    ".left-col .cam-wrap video {{\n",
    "  height: 100% !important;\n",
    "  width: auto !important;\n",
    "  object-fit: cover !important;\n",
    "  border-radius: 18px !important;\n",
    "}}\n",
    "\n",
    ".right-col .art-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;\n",
    "  max-width: {RIGHT_IMG_MAXW}px;\n",
    "  overflow: hidden;\n",
    "  margin: 0 auto;\n",
    "  border-radius: 18px;\n",
    "  background: #050505;\n",
    "  box-shadow: 0 18px 45px rgba(0,0,0,0.35);\n",
    "}}\n",
    ".right-col .art-wrap img {{\n",
    "  display: block;\n",
    "  width: 100% !important;\n",
    "  height: 100% !important;\n",
    "  object-fit: cover !important;\n",
    "}}\n",
    "\n",
    "button[kind=\"secondary\"] {{\n",
    "  border-radius: 999px !important;\n",
    "}}\n",
    "\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "st.title(APP_TITLE)\n",
    "st.caption(\n",
    "    \"Hold still for ~3â€“5 seconds to auto-capture, or press the button to capture manually. \"\n",
    "    \"Left: live video with pose. Right: matched artwork with tiny pink pose metrics.\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    st.autorefresh(interval=700, key=\"ear_auto\", limit=None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "left, right = st.columns([1, 1], gap=\"large\")\n",
    "\n",
    "if \"countdown_target\" not in st.session_state:\n",
    "    st.session_state[\"countdown_target\"] = None\n",
    "if \"last_match\" not in st.session_state:\n",
    "    st.session_state[\"last_match\"] = None\n",
    "if \"last_metrics\" not in st.session_state:\n",
    "    st.session_state[\"last_metrics\"] = []\n",
    "if \"last_ts\" not in st.session_state:\n",
    "    st.session_state[\"last_ts\"] = 0.0\n",
    "if \"recent_files\" not in st.session_state:\n",
    "    st.session_state[\"recent_files\"] = []  # type: List[str]\n",
    "\n",
    "API_URL = DEFAULT_API_URL\n",
    "\n",
    "\n",
    "# =================== LEFT PANEL ===================\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"Live\")\n",
    "    st.markdown('<div class=\"left-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"cam-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ctx = webrtc_streamer(\n",
    "        key=\"ear-curatorial-final\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "        video_processor_factory=CuratorialProcessor,\n",
    "        async_processing=True,\n",
    "    )\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"<div style='height:10px'></div>\", unsafe_allow_html=True)\n",
    "\n",
    "    c1, c2 = st.columns(2)\n",
    "\n",
    "    with c1:\n",
    "        if st.button(\"ğŸ“¸ Capture (wait 3s)\", use_container_width=True):\n",
    "            st.session_state[\"countdown_target\"] = time.time() + 3.0\n",
    "\n",
    "    with c2:\n",
    "        if st.button(\"âš¡ Instant Capture\", use_container_width=True):\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"Try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "\n",
    "    if st.session_state[\"countdown_target\"]:\n",
    "        remain = st.session_state[\"countdown_target\"] - time.time()\n",
    "        if remain > 0:\n",
    "            st.info(f\"Capturing in {remain:.1f}sâ€¦ Hold still.\")\n",
    "        else:\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"Try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "            st.session_state[\"countdown_target\"] = None\n",
    "            force_rerun()\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# =================== RIGHT PANEL ===================\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"Matched artwork\")\n",
    "\n",
    "    # Match mode control\n",
    "    match_mode = st.selectbox(\n",
    "        \"Match mode\",\n",
    "        [\"default\", \"portrait_only\", \"high_value_only\", \"portrait_high_value\"],\n",
    "        index=0,\n",
    "        help=\"Use 'portrait_only' to bias towards clear portraits; \"\n",
    "             \"'high_value_only' for masterpieces.\",\n",
    "    )\n",
    "\n",
    "    st.markdown('<div class=\"right-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"art-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ph = st.empty()\n",
    "\n",
    "    proc: Optional[CuratorialProcessor] = None\n",
    "    if ctx and ctx.video_processor:\n",
    "        proc = ctx.video_processor\n",
    "\n",
    "    if not proc:\n",
    "        ph.info(\"Initializing cameraâ€¦\")\n",
    "    else:\n",
    "        with proc.lock:\n",
    "            cap_img = proc.captured_img\n",
    "            cap_ts = getattr(proc, \"captured_ts\", 0.0)\n",
    "            cap_metrics = list(getattr(proc, \"captured_metrics\", []))\n",
    "\n",
    "        if cap_img is not None and cap_ts > st.session_state[\"last_ts\"]:\n",
    "            st.session_state[\"last_ts\"] = cap_ts\n",
    "            st.session_state[\"last_metrics\"] = cap_metrics\n",
    "\n",
    "            buf = io.BytesIO()\n",
    "            cap_img.save(buf, format=\"JPEG\")\n",
    "            buf.seek(0)\n",
    "\n",
    "            files = {\"image\": (\"frame.jpg\", buf.getvalue(), \"image/jpeg\")}\n",
    "            # Ask backend for Top-10, we'll do diversity selection here\n",
    "            data = {\n",
    "                \"museum\": \"mixed\",\n",
    "                \"topk\": TAIL_TOP_K,\n",
    "                \"mode\": match_mode,\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                resp = requests.post(API_URL, files=files, data=data, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                st.session_state[\"last_match\"] = resp.json()\n",
    "            except Exception as exc:\n",
    "                st.session_state[\"last_match\"] = {\"error\": str(exc)}\n",
    "\n",
    "            force_rerun()\n",
    "\n",
    "        payload = st.session_state.get(\"last_match\")\n",
    "\n",
    "        if not payload:\n",
    "            ph.info(\"Hold still or press capture to matchâ€¦\")\n",
    "        elif \"error\" in payload:\n",
    "            ph.error(f\"Backend error: {payload['error']}\")\n",
    "        else:\n",
    "            results = payload.get(\"results\") or []\n",
    "            if not results:\n",
    "                ph.warning(\"No matches returned.\")\n",
    "            else:\n",
    "                recent = st.session_state.get(\"recent_files\", [])\n",
    "\n",
    "                # ---------- diversity + mild randomness ----------\n",
    "                # Decide whether to start looking from the tail (ranks 4â€“10)\n",
    "                use_tail_first = (\n",
    "                    random.random() < TAIL_RANDOM_PROB\n",
    "                    and len(results) > PRIMARY_TOP_K\n",
    "                )\n",
    "                primary_slice = results[:PRIMARY_TOP_K]\n",
    "                tail_slice = results[PRIMARY_TOP_K:TAIL_TOP_K]\n",
    "\n",
    "                ordered_candidates = (\n",
    "                    (tail_slice + primary_slice)\n",
    "                    if use_tail_first\n",
    "                    else (primary_slice + tail_slice)\n",
    "                )\n",
    "\n",
    "                chosen = None\n",
    "                chosen_fname = None\n",
    "\n",
    "                for r in ordered_candidates:\n",
    "                    fname = (\n",
    "                        r.get(\"filename\")\n",
    "                        or r.get(\"file\")\n",
    "                        or r.get(\"image_path\")\n",
    "                        or r.get(\"path\")\n",
    "                    )\n",
    "                    if not fname:\n",
    "                        continue\n",
    "                    if fname not in recent:\n",
    "                        chosen = r\n",
    "                        chosen_fname = fname\n",
    "                        break\n",
    "\n",
    "                # Fallback: if all candidates are \"recent\", use strict Top-1\n",
    "                if chosen is None:\n",
    "                    chosen = results[0]\n",
    "                    chosen_fname = (\n",
    "                        chosen.get(\"filename\")\n",
    "                        or chosen.get(\"file\")\n",
    "                        or chosen.get(\"image_path\")\n",
    "                        or chosen.get(\"path\")\n",
    "                    )\n",
    "\n",
    "                # Update recent memory\n",
    "                if chosen_fname:\n",
    "                    if chosen_fname in recent:\n",
    "                        recent.remove(chosen_fname)\n",
    "                    recent.insert(0, chosen_fname)\n",
    "                    del recent[MAX_RECENT:]\n",
    "                    st.session_state[\"recent_files\"] = recent\n",
    "\n",
    "                filename = chosen_fname\n",
    "\n",
    "                img_path = ensure_image_path(filename or \"\")\n",
    "                if not img_path:\n",
    "                    ph.error(f\"Image not found: {filename}\")\n",
    "                else:\n",
    "                    painting = safe_open_image(img_path)\n",
    "                    if painting is None:\n",
    "                        ph.error(f\"Failed to open: {img_path}\")\n",
    "                    else:\n",
    "                        meta_row = lookup_meta(str(filename))\n",
    "                        meta = {\n",
    "                            \"artist\": meta_row.get(\"artist\")\n",
    "                            or chosen.get(\"artist\")\n",
    "                            or \"artist name\",\n",
    "                            \"year\": meta_row.get(\"year\") or chosen.get(\"year\") or \"\",\n",
    "                            \"price_text\": meta_row.get(\"price_text\")\n",
    "                            or meta_row.get(\"auction_price_usd\")\n",
    "                            or \"\",\n",
    "                        }\n",
    "\n",
    "                        painted = overlay_right_labels(painting, meta)\n",
    "                        metrics = st.session_state.get(\"last_metrics\") or []\n",
    "                        painted = draw_tiny_metrics_top_right(\n",
    "                            painted, metrics, size=16, margin=12\n",
    "                        )\n",
    "\n",
    "                        w = min(RIGHT_IMG_MAXW, painted.width)\n",
    "                        caption = f\"{meta_row.get('title','')} â€” {meta.get('artist','')}\"\n",
    "                        ph.image(painted, caption=caption, width=w)\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf1bb5",
   "metadata": {},
   "source": [
    "tools/build_pose_embeddings.py(clip_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795550c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "build_pose_embeddings.py\n",
    "-------------------------\n",
    "Generate pose-based embeddings for all artworks using YOLOv8-Pose.\n",
    "\n",
    "Output:\n",
    "    data/mixed/pose_embeddings.npy\n",
    "    (same row ordering as embeddings_meta.csv)\n",
    "\n",
    "Usage:\n",
    "    conda activate ear-mvp\n",
    "    python backend/tools/build_pose_embeddings.py --data-root data/mixed\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# YOLOv8-Pose\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "except Exception as exc:\n",
    "    raise RuntimeError(\"Ultralytics YOLO is required. `pip install ultralytics`.\") from exc\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Pose vector utilities\n",
    "# -------------------------\n",
    "\n",
    "def encode_pose_keypoints(kps: np.ndarray | None, img_w: int, img_h: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert raw keypoints (N x 3: x,y,confidence) into a fixed pose vector.\n",
    "\n",
    "    We encode:\n",
    "      - normalized coordinates (x/img_w, y/img_h)\n",
    "      - per-joint existence mask\n",
    "      - joint angle features\n",
    "    \"\"\"\n",
    "\n",
    "    num_joints = 17  # YOLOv8 Pose has 17 keypoints (0..16)\n",
    "    if kps is None:\n",
    "        # No detection â†’ zero vector\n",
    "        return np.zeros(num_joints * 4, dtype=np.float32)\n",
    "\n",
    "    # If YOLO returns more than one person, we take the one with largest area\n",
    "    # but build_embeddings will only run on artwork images (static), so 1 person is expected.\n",
    "    if len(kps.shape) == 3:  # shape [num_people, 17, 3]\n",
    "        kps = kps[0]\n",
    "\n",
    "    # Prepare vector components\n",
    "    coord_vec = np.zeros((num_joints, 2), dtype=np.float32)\n",
    "    mask_vec  = np.zeros((num_joints, 1), dtype=np.float32)\n",
    "    angle_vec = np.zeros((num_joints, 1), dtype=np.float32)\n",
    "\n",
    "    # Normalize\n",
    "    for i in range(num_joints):\n",
    "        x, y, conf = kps[i]\n",
    "        if conf > 0.1:\n",
    "            coord_vec[i] = np.array([x / img_w, y / img_h])\n",
    "            mask_vec[i] = 1.0\n",
    "        else:\n",
    "            coord_vec[i] = 0.0\n",
    "            mask_vec[i] = 0.0\n",
    "\n",
    "    # Compute simple limb angles (example: shoulders/elbows/wrists)\n",
    "    # We encode angle at elbow and knee joints for stability\n",
    "    def safe_angle(a, b, c):\n",
    "        \"\"\"Return angle ABC in radians.\"\"\"\n",
    "        if (a is None) or (b is None) or (c is None):\n",
    "            return 0.0\n",
    "        ax, ay = a\n",
    "        bx, by = b\n",
    "        cx, cy = c\n",
    "        v1 = np.array([ax - bx, ay - by])\n",
    "        v2 = np.array([cx - bx, cy - by])\n",
    "        n1 = np.linalg.norm(v1)\n",
    "        n2 = np.linalg.norm(v2)\n",
    "        if n1 < 1e-6 or n2 < 1e-6:\n",
    "            return 0.0\n",
    "        cosv = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "        return float(np.arccos(cosv))  # radians\n",
    "\n",
    "    # Joint indices follow COCO:\n",
    "    # 5: left shoulder, 6: right shoulder\n",
    "    # 7: left elbow,    8: right elbow\n",
    "    # 9: left wrist,   10: right wrist\n",
    "    # 11: left hip,    12: right hip\n",
    "    # 13: left knee,   14: right knee\n",
    "    # 15: left ankle,  16: right ankle\n",
    "\n",
    "    joints = kps[:, :2]\n",
    "    def jp(idx):\n",
    "        x, y, conf = kps[idx]\n",
    "        return (x, y) if conf > 0.1 else None\n",
    "\n",
    "    # elbow angles\n",
    "    angle_vec[7]  = safe_angle(jp(5), jp(7), jp(9))\n",
    "    angle_vec[8]  = safe_angle(jp(6), jp(8), jp(10))\n",
    "\n",
    "    # knee angles\n",
    "    angle_vec[13] = safe_angle(jp(11), jp(13), jp(15))\n",
    "    angle_vec[14] = safe_angle(jp(12), jp(14), jp(16))\n",
    "\n",
    "    # Flatten\n",
    "    return np.concatenate([\n",
    "        coord_vec.reshape(-1),\n",
    "        mask_vec.reshape(-1),\n",
    "        angle_vec.reshape(-1),\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main build function\n",
    "# -------------------------\n",
    "\n",
    "def build_pose_embeddings(data_root: Path, model_path: Path):\n",
    "    meta_path = data_root / \"embeddings_meta.csv\"\n",
    "    images_dir = data_root / \"images\"\n",
    "\n",
    "    if not meta_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {meta_path}\")\n",
    "\n",
    "    # Load metadata rows in order\n",
    "    import csv\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        rows = list(csv.DictReader(f))\n",
    "\n",
    "    print(f\"[INFO] Loaded {len(rows)} metadata rows.\")\n",
    "\n",
    "    # Init YOLO pose model\n",
    "    print(f\"[INFO] Loading YOLOv8-Pose: {model_path}\")\n",
    "    model = YOLO(str(model_path))\n",
    "\n",
    "    pose_vectors = []\n",
    "\n",
    "    for row in tqdm(rows, desc=\"Building pose vectors\"):\n",
    "        fname = (\n",
    "            row.get(\"filename\")\n",
    "            or row.get(\"image_path\")\n",
    "            or row.get(\"path\")\n",
    "            or row.get(\"file\")\n",
    "        )\n",
    "        if not fname:\n",
    "            pose_vectors.append(np.zeros(17 * 4, dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        img_path = images_dir / fname\n",
    "        if not img_path.exists():\n",
    "            pose_vectors.append(np.zeros(17 * 4, dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            pose_vectors.append(np.zeros(17 * 4, dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        w, h = img.size\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        # YOLO pose inference\n",
    "        res = model.predict(img_np, imgsz=640, device=\"cpu\", verbose=False)\n",
    "        if len(res) == 0 or res[0].keypoints is None:\n",
    "            pose_vectors.append(np.zeros(17 * 4, dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        kps = res[0].keypoints.cpu().numpy()  # shape: [1, 17, 3]\n",
    "        vec = encode_pose_keypoints(kps, w, h)\n",
    "        pose_vectors.append(vec)\n",
    "\n",
    "    pose_vectors = np.stack(pose_vectors, axis=0)\n",
    "    out_path = data_root / \"pose_embeddings.npy\"\n",
    "\n",
    "    np.save(out_path, pose_vectors)\n",
    "    print(f\"[OK] Saved pose embeddings â†’ {out_path}\")\n",
    "    print(f\"[INFO] shape = {pose_vectors.shape} (rows match embeddings_meta.csv)\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CLI\n",
    "# -------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data-root\", type=str, default=\"data/mixed\",\n",
    "                        help=\"path to dataset folder that contains images/ and embeddings_meta.csv\")\n",
    "    parser.add_argument(\"--yolo-model\", type=str, default=\"frontend/yolov8n-pose.pt\",\n",
    "                        help=\"path to YOLOv8-Pose .pt file\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    build_pose_embeddings(\n",
    "        Path(args.data_root),\n",
    "        Path(args.yolo_model),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f92ec0",
   "metadata": {},
   "source": [
    "backend/utils_pose.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b8ddd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "utils_pose.py\n",
    "-------------\n",
    "Unified utilities to convert YOLOv8-Pose keypoints â†’ pose embedding vectors.\n",
    "\n",
    "This MUST match build_pose_embeddings.py so that query vectors\n",
    "are compatible with stored pose_embeddings.npy.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def encode_keypoints_to_pose_vector(\n",
    "    keypoints_xy: np.ndarray,   # shape: (17, 2)\n",
    "    visibility: np.ndarray,     # shape: (17,)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert YOLOv8 keypoints â†’ a normalized pose embedding vector.\n",
    "\n",
    "    Steps:\n",
    "      1. Normalize XY to [0,1] by image width/height (caller must pre-normalize)\n",
    "      2. Use visibility as mask\n",
    "      3. Compute limb angles\n",
    "      4. Flatten into 1D feature vector\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- 1) Normalize XY (expected by build_pose_embeddings)\n",
    "    xy = keypoints_xy.astype(\"float32\")\n",
    "    vis = visibility.astype(\"float32\")\n",
    "\n",
    "    # Flatten XY\n",
    "    xy_flat = xy.reshape(-1)  # (34,)\n",
    "\n",
    "    # ----- 2) visibility mask\n",
    "    mask = vis.reshape(-1)  # (17,)\n",
    "\n",
    "    # ----- 3) Compute simple limb angles\n",
    "    def angle(p1, p2):\n",
    "        if p1 is None or p2 is None:\n",
    "            return 0.0\n",
    "        vx, vy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "        return np.degrees(np.arctan2(vy, vx))\n",
    "\n",
    "    # Example pairs (shoulder-left â†’ elbow-left, etc.)\n",
    "    k = xy\n",
    "    angle_pairs = [\n",
    "        (5, 7),\n",
    "        (7, 9),\n",
    "        (6, 8),\n",
    "        (8, 10),\n",
    "    ]\n",
    "    angs = []\n",
    "    for a, b in angle_pairs:\n",
    "        p1 = k[a] if vis[a] > 0.1 else None\n",
    "        p2 = k[b] if vis[b] > 0.1 else None\n",
    "        angs.append(angle(p1, p2))\n",
    "    angs = np.array(angs, dtype=\"float32\")  # (4,)\n",
    "\n",
    "    # ----- 4) Concatenate all\n",
    "    feat = np.concatenate([\n",
    "        xy_flat,      # 34\n",
    "        mask,         # 17\n",
    "        angs,         # 4\n",
    "    ]).astype(\"float32\")  # total = 34+17+4 = 55 dims\n",
    "\n",
    "    return feat  # (55,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ca39b",
   "metadata": {},
   "source": [
    "app_frontend.py(736)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0064e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_webrtc import (\n",
    "    WebRtcMode,\n",
    "    RTCConfiguration,\n",
    "    VideoProcessorBase,\n",
    "    webrtc_streamer,\n",
    ")\n",
    "\n",
    "# =================== PATHS & CONSTANTS ===================\n",
    "\n",
    "FRONTEND_DIR = Path(__file__).resolve().parent\n",
    "ROOT_DIR = FRONTEND_DIR.parent\n",
    "\n",
    "# Use GLOBAL MIXED INDEX (local + met + aic merged)\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"mixed\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "# All museum image roots (search order)\n",
    "MUSEUM_IMAGE_DIRS = [\n",
    "    DATA_DIR / \"images\",                    # mixed\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"images\",\n",
    "    ROOT_DIR / \"data\" / \"met\" / \"images\",\n",
    "    ROOT_DIR / \"data\" / \"aic\" / \"images\",\n",
    "]\n",
    "\n",
    "# Meta CSV candidates (fallback to local if needed)\n",
    "META_CSV_CANDIDATES = [\n",
    "    DATA_DIR / \"embeddings_meta.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works_enhanced_english.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works.csv\",\n",
    "]\n",
    "\n",
    "DEFAULT_API_URL = \"http://127.0.0.1:8000/match\"\n",
    "APP_TITLE = \"Embodied Aesthetic Reconstruction\"\n",
    "\n",
    "YOLO_MODEL_PATH = FRONTEND_DIR / \"yolov8n-pose.pt\"\n",
    "\n",
    "RTC_CONFIGURATION = RTCConfiguration(\n",
    "    {\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]}\n",
    ")\n",
    "\n",
    "STILLNESS_SEC = 3.5\n",
    "MAX_BUF_SEC = 5.0\n",
    "FPS_ASSUMED = 12\n",
    "MOTION_EPS_CXCY = 4.0\n",
    "MOTION_EPS_AREA = 0.03\n",
    "MIN_FACE_AREA = 0.06\n",
    "\n",
    "YELLOW = (255, 235, 59)\n",
    "BLACK = (0, 0, 0)\n",
    "HOT_PINK = (255, 30, 180)\n",
    "\n",
    "RIGHT_IMG_MAXW = 900\n",
    "\n",
    "# Diversity / randomness control\n",
    "MAX_RECENT = 6          # how many artworks to remember\n",
    "PRIMARY_TOP_K = 3       # \"very best\" range\n",
    "TAIL_TOP_K = 10         # search in top-10 in total\n",
    "TAIL_RANDOM_PROB = 0.3  # 30% chance to look into 4â€“10 first\n",
    "\n",
    "\n",
    "# =================== UTILITIES ===================\n",
    "\n",
    "_META_CACHE: Optional[Dict[str, Dict]] = None\n",
    "\n",
    "\n",
    "def load_meta_mapping() -> Dict[str, Dict]:\n",
    "    \"\"\"Load CSV â†’ filename â†’ metadata mapping.\"\"\"\n",
    "    global _META_CACHE\n",
    "    if _META_CACHE is not None:\n",
    "        return _META_CACHE\n",
    "\n",
    "    import csv\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    for p in META_CSV_CANDIDATES:\n",
    "        if p.exists():\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                rows = list(csv.DictReader(f))\n",
    "            break\n",
    "\n",
    "    mapping: Dict[str, Dict] = {}\n",
    "    for r in rows:\n",
    "        fname = (\n",
    "            r.get(\"filename\")\n",
    "            or r.get(\"image_path\")\n",
    "            or r.get(\"path\")\n",
    "            or r.get(\"file\")\n",
    "        )\n",
    "        if fname:\n",
    "            mapping[str(fname)] = r\n",
    "\n",
    "    _META_CACHE = mapping\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def lookup_meta(filename: str) -> Dict:\n",
    "    mapping = load_meta_mapping()\n",
    "    return mapping.get(filename, {})\n",
    "\n",
    "\n",
    "def safe_open_image(p: Path) -> Optional[Image.Image]:\n",
    "    try:\n",
    "        return Image.open(p).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_image_path(filename: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Resolve an artwork filename to a real image path.\n",
    "\n",
    "    Strategy:\n",
    "      1) If it's an absolute existing path â†’ return.\n",
    "      2) If it has subdirectories, try relative to project root / data.\n",
    "      3) Otherwise, search by basename in all known museum image dirs.\n",
    "    \"\"\"\n",
    "    if not filename:\n",
    "        return None\n",
    "\n",
    "    p = Path(filename)\n",
    "\n",
    "    # 1) absolute path already\n",
    "    if p.is_absolute() and p.exists():\n",
    "        return p\n",
    "\n",
    "    candidates: List[Path] = []\n",
    "\n",
    "    # 2) has subdirectory component (e.g. \"aic/images/aic_4939.jpg\")\n",
    "    if p.parent != Path(\".\"):\n",
    "        candidates.append(ROOT_DIR / p)\n",
    "        candidates.append(ROOT_DIR / \"data\" / p)\n",
    "\n",
    "    # 3) search by basename across all museum image roots\n",
    "    basename = p.name\n",
    "    for root in MUSEUM_IMAGE_DIRS:\n",
    "        candidates.append(root / basename)\n",
    "\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return c\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _load_font(size: int = 40):\n",
    "    \"\"\"Try Courier â†’ Arial â†’ default.\"\"\"\n",
    "    candidates = [\n",
    "        \"/Library/Fonts/Courier New.ttf\",\n",
    "        \"/System/Library/Fonts/Courier.dfont\",\n",
    "        \"/System/Library/Fonts/Supplemental/Courier New.ttf\",\n",
    "        \"/Library/Fonts/Arial.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                return ImageFont.truetype(p, size=size)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def draw_tiny_metrics_top_right(\n",
    "    im: Image.Image, lines: List[str], size: int = 16, margin: int = 10\n",
    ") -> Image.Image:\n",
    "    \"\"\"Small pink pose metrics in top-right corner of the image.\"\"\"\n",
    "    if not lines:\n",
    "        return im\n",
    "    img = im.copy()\n",
    "    d = ImageDraw.Draw(img)\n",
    "    font = _load_font(size)\n",
    "\n",
    "    widths = []\n",
    "    for s in lines:\n",
    "        l, t, r, b = d.textbbox((0, 0), s, font=font)\n",
    "        widths.append(r - l)\n",
    "    wmax = max(widths) if widths else 0\n",
    "\n",
    "    x = img.width - margin - wmax\n",
    "    y = margin\n",
    "    for s in lines:\n",
    "        d.text((x, y), s, fill=HOT_PINK, font=font)\n",
    "        _, _, _, b = d.textbbox((0, 0), s, font=font)\n",
    "        y += int(b * 0.95)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _angle_deg(p1, p2):\n",
    "    if p1 is None or p2 is None:\n",
    "        return None\n",
    "    vx, vy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "    return float(np.degrees(np.arctan2(vy, vx)))\n",
    "\n",
    "\n",
    "def _elbow_angle(shoulder, elbow, wrist):\n",
    "    if None in (shoulder, elbow, wrist):\n",
    "        return None\n",
    "    v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]], float)\n",
    "    v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]], float)\n",
    "    n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if n1 < 1e-5 or n2 < 1e-5:\n",
    "        return None\n",
    "    cosv = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cosv)))\n",
    "\n",
    "\n",
    "def format_metrics(kps: Dict[int, Tuple[float, float] | None]) -> List[str]:\n",
    "    le, re = kps.get(1), kps.get(2)\n",
    "    lsh, rsh = kps.get(5), kps.get(6)\n",
    "    lel, rel = kps.get(7), kps.get(8)\n",
    "    lwr, rwr = kps.get(9), kps.get(10)\n",
    "\n",
    "    fdeg = lambda v: \"â€”\" if v is None else f\"{v:+.1f}Â°\"\n",
    "    fpt = lambda p: \"(0, 0)\" if p is None else f\"({int(p[0])}, {int(p[1])})\"\n",
    "\n",
    "    return [\n",
    "        f\"Head tilt: {fdeg(_angle_deg(re, le))}\",\n",
    "        f\"Shoulder:  {fdeg(_angle_deg(rsh, lsh))}\",\n",
    "        f\"L elbow:   {fdeg(_elbow_angle(lsh, lel, lwr))}\",\n",
    "        f\"R elbow:   {fdeg(_elbow_angle(rsh, rel, rwr))}\",\n",
    "        f\"L wrist:   {fpt(lwr)}\",\n",
    "        f\"R wrist:   {fpt(rwr)}\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def overlay_right_labels(painting: Image.Image, meta: Dict) -> Image.Image:\n",
    "    \"\"\"Yellow label stack: price, year, artist.\"\"\"\n",
    "    im = painting.convert(\"RGB\").copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    font_big = _load_font(44)\n",
    "    font_small = _load_font(36)\n",
    "\n",
    "    price = (\n",
    "        meta.get(\"price_text\")\n",
    "        or meta.get(\"auction_price_usd\")\n",
    "        or meta.get(\"price\")\n",
    "        or \"â€”\"\n",
    "    )\n",
    "    year = str(meta.get(\"year\") or \"â€”\")\n",
    "    artist = meta.get(\"artist\") or \"artist name\"\n",
    "\n",
    "    lines = [price, year, artist]\n",
    "    fonts = [font_big, font_small, font_big]\n",
    "\n",
    "    margin_x = 24\n",
    "    y = int(im.height * 0.50)\n",
    "\n",
    "    for text, font in zip(lines, fonts):\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font)\n",
    "        w, h = r - l, b - t\n",
    "\n",
    "        pad_x, pad_y = 16, 10\n",
    "        box_w, box_h = w + 2 * pad_x, h + 2 * pad_y\n",
    "\n",
    "        x = margin_x\n",
    "        draw.rectangle([x, y, x + box_w, y + box_h], fill=YELLOW)\n",
    "        draw.text((x + pad_x, y + pad_y), text, fill=BLACK, font=font)\n",
    "\n",
    "        y += box_h + 10\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def force_rerun():\n",
    "    \"\"\"Compatible with new/old Streamlit.\"\"\"\n",
    "    try:\n",
    "        st.rerun()\n",
    "    except Exception:\n",
    "        try:\n",
    "            st.experimental_rerun()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =================== YOLO VIDEO PROCESSOR ===================\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "    HAS_YOLO = True\n",
    "except Exception:\n",
    "    HAS_YOLO = False\n",
    "\n",
    "\n",
    "class CuratorialProcessor(VideoProcessorBase):\n",
    "    \"\"\"\n",
    "    YOLOv8-Pose:\n",
    "    - blue bbox\n",
    "    - green skeleton\n",
    "    - pink tiny pose metrics\n",
    "    - stillness-based auto capture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        if HAS_YOLO:\n",
    "            try:\n",
    "                local = YOLO_MODEL_PATH\n",
    "                self.model = YOLO(str(local if local.exists() else \"yolov8n-pose.pt\"))\n",
    "            except Exception:\n",
    "                self.model = None\n",
    "\n",
    "        maxlen = max(6, int(MAX_BUF_SEC * FPS_ASSUMED))\n",
    "        self.cx_buf = deque(maxlen=maxlen)\n",
    "        self.cy_buf = deque(maxlen=maxlen)\n",
    "        self.area_buf = deque(maxlen=maxlen)\n",
    "        self.last_stable_ts: Optional[float] = None\n",
    "\n",
    "        self.captured_img: Optional[Image.Image] = None\n",
    "        self.captured_ts: float = 0.0\n",
    "        self.captured_metrics: List[str] = []\n",
    "\n",
    "        self.latest_rgb: Optional[np.ndarray] = None\n",
    "        self.last_metrics_lines: List[str] = []\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _detect_bbox(self, rgb: np.ndarray) -> Optional[Tuple[int, int, int, int]]:\n",
    "        if not self.model:\n",
    "            return None\n",
    "        res = self.model.predict(rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "        if len(res) == 0 or res[0].boxes is None:\n",
    "            return None\n",
    "        b = res[0].boxes.xyxy\n",
    "        if b is None or len(b) == 0:\n",
    "            return None\n",
    "        b = b.cpu().numpy()\n",
    "        areas = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "        i = int(np.argmax(areas))\n",
    "        x1, y1, x2, y2 = b[i].astype(int).tolist()\n",
    "        return x1, y1, x2, y2\n",
    "\n",
    "    def _extract_keypoints(self, res) -> Dict[int, Tuple[float, float] | None]:\n",
    "        kps: Dict[int, Tuple[float, float] | None] = {}\n",
    "        try:\n",
    "            if res and res[0].keypoints is not None and len(res[0].keypoints) > 0:\n",
    "                xy = res[0].keypoints.xy[0].cpu().numpy()\n",
    "                for i in range(xy.shape[0]):\n",
    "                    kps[i] = (float(xy[i, 0]), float(xy[i, 1]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return kps\n",
    "\n",
    "    def _update_stillness(self, bbox, w, h) -> bool:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cx = 0.5 * (x1 + x2)\n",
    "        cy = 0.5 * (y1 + y2)\n",
    "        area = max(1.0, (x2 - x1) * (y2 - y1))\n",
    "        rel_area = area / float(w * h)\n",
    "\n",
    "        if rel_area < MIN_FACE_AREA:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        self.cx_buf.append(cx)\n",
    "        self.cy_buf.append(cy)\n",
    "        self.area_buf.append(rel_area)\n",
    "\n",
    "        need_len = int(STILLNESS_SEC * FPS_ASSUMED * 0.6)\n",
    "        if len(self.cx_buf) < max(3, need_len):\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        stdx = float(np.std(self.cx_buf))\n",
    "        stdy = float(np.std(self.cy_buf))\n",
    "        stda = float(np.std(self.area_buf))\n",
    "        stable_now = (\n",
    "            stdx < MOTION_EPS_CXCY\n",
    "            and stdy < MOTION_EPS_CXCY\n",
    "            and stda < MOTION_EPS_AREA\n",
    "        )\n",
    "\n",
    "        now = time.time()\n",
    "        if stable_now:\n",
    "            if self.last_stable_ts is None:\n",
    "                self.last_stable_ts = now\n",
    "            return (now - self.last_stable_ts) >= STILLNESS_SEC\n",
    "        else:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "    def _stamp_capture(self):\n",
    "        self.captured_img = Image.fromarray(self.latest_rgb)\n",
    "        self.captured_ts = time.time()\n",
    "        self.captured_metrics = list(self.last_metrics_lines)\n",
    "        self.last_stable_ts = None\n",
    "\n",
    "    def capture_now(self) -> bool:\n",
    "        if self.latest_rgb is None:\n",
    "            return False\n",
    "        with self.lock:\n",
    "            self._stamp_capture()\n",
    "        return True\n",
    "\n",
    "    def recv(self, frame):\n",
    "        import av\n",
    "\n",
    "        img_bgr = frame.to_ndarray(format=\"bgr24\")\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        h, w, _ = img_rgb.shape\n",
    "        self.latest_rgb = img_rgb\n",
    "\n",
    "        if self.model:\n",
    "            res = self.model.predict(img_rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "            plotted = res[0].plot()[:, :, ::-1]\n",
    "            kps = self._extract_keypoints(res)\n",
    "            lines = format_metrics(kps)\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = lines\n",
    "            pil = Image.fromarray(plotted)\n",
    "            pil = draw_tiny_metrics_top_right(pil, lines, size=16, margin=10)\n",
    "            out_rgb = np.array(pil)\n",
    "        else:\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = [\"(pose model unavailable)\"]\n",
    "            out_rgb = img_rgb\n",
    "\n",
    "        bbox = self._detect_bbox(img_rgb) if self.model else None\n",
    "        if bbox and self._update_stillness(bbox, w, h):\n",
    "            with self.lock:\n",
    "                if time.time() - self.captured_ts > 0.35:\n",
    "                    self._stamp_capture()\n",
    "\n",
    "        out_bgr = out_rgb[:, :, ::-1]\n",
    "        return av.VideoFrame.from_ndarray(out_bgr, format=\"bgr24\")\n",
    "\n",
    "\n",
    "# =================== PAGE LAYOUT ===================\n",
    "\n",
    "st.set_page_config(page_title=APP_TITLE, layout=\"wide\")\n",
    "\n",
    "st.markdown(\n",
    "    f\"\"\"\n",
    "<style>\n",
    "section[data-testid=\"stSidebar\"] {{ display: none !important; }}\n",
    "header, footer, [data-testid=\"stToolbar\"] {{ visibility: hidden !important; }}\n",
    "\n",
    "body {{\n",
    "  background-color: #f5f5f7;\n",
    "}}\n",
    ".block-container {{\n",
    "  padding-top: 0.6rem;\n",
    "  padding-bottom: 0.6rem;\n",
    "  max-width: 1700px;\n",
    "}}\n",
    "\n",
    "h1 {{\n",
    "  font-family: -apple-system, BlinkMacSystemFont, \"SF Pro Display\", system-ui, sans-serif;\n",
    "  letter-spacing: 0.04em;\n",
    "  font-weight: 700;\n",
    "}}\n",
    "\n",
    ".left-col .cam-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;\n",
    "  width: 100%;\n",
    "  overflow: hidden;\n",
    "  border-radius: 18px;\n",
    "  background: #111;\n",
    "  box-shadow: 0 18px 45px rgba(0,0,0,0.3);\n",
    "}}\n",
    ".left-col .cam-wrap video {{\n",
    "  height: 100% !important;\n",
    "  width: auto !important;\n",
    "  object-fit: cover !important;\n",
    "  border-radius: 18px !important;\n",
    "}}\n",
    "\n",
    ".right-col .art-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;\n",
    "  max-width: {RIGHT_IMG_MAXW}px;\n",
    "  overflow: hidden;\n",
    "  margin: 0 auto;\n",
    "  border-radius: 18px;\n",
    "  background: #050505;\n",
    "  box-shadow: 0 18px 45px rgba(0,0,0,0.35);\n",
    "}}\n",
    ".right-col .art-wrap img {{\n",
    "  display: block;\n",
    "  width: 100% !important;\n",
    "  height: 100% !important;\n",
    "  object-fit: cover !important;\n",
    "}}\n",
    "\n",
    "button[kind=\"secondary\"] {{\n",
    "  border-radius: 999px !important;\n",
    "}}\n",
    "\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "st.title(APP_TITLE)\n",
    "st.caption(\n",
    "    \"Hold still for ~3â€“5 seconds to auto-capture, or press the button to capture manually. \"\n",
    "    \"Left: live video with pose. Right: matched artwork with tiny pink pose metrics.\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    st.autorefresh(interval=700, key=\"ear_auto\", limit=None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "left, right = st.columns([1, 1], gap=\"large\")\n",
    "\n",
    "if \"countdown_target\" not in st.session_state:\n",
    "    st.session_state[\"countdown_target\"] = None\n",
    "if \"last_match\" not in st.session_state:\n",
    "    st.session_state[\"last_match\"] = None\n",
    "if \"last_metrics\" not in st.session_state:\n",
    "    st.session_state[\"last_metrics\"] = []\n",
    "if \"last_ts\" not in st.session_state:\n",
    "    st.session_state[\"last_ts\"] = 0.0\n",
    "if \"recent_files\" not in st.session_state:\n",
    "    st.session_state[\"recent_files\"] = []  # type: List[str]\n",
    "\n",
    "API_URL = DEFAULT_API_URL\n",
    "\n",
    "\n",
    "# =================== LEFT PANEL ===================\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"Live\")\n",
    "    st.markdown('<div class=\"left-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"cam-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ctx = webrtc_streamer(\n",
    "        key=\"ear-curatorial-final\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "        video_processor_factory=CuratorialProcessor,\n",
    "        async_processing=True,\n",
    "    )\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"<div style='height:10px'></div>\", unsafe_allow_html=True)\n",
    "\n",
    "    c1, c2 = st.columns(2)\n",
    "\n",
    "    with c1:\n",
    "        if st.button(\"ğŸ“¸ Capture (wait 3s)\", use_container_width=True):\n",
    "            st.session_state[\"countdown_target\"] = time.time() + 3.0\n",
    "\n",
    "    with c2:\n",
    "        if st.button(\"âš¡ Instant Capture\", use_container_width=True):\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"Try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "\n",
    "    if st.session_state[\"countdown_target\"]:\n",
    "        remain = st.session_state[\"countdown_target\"] - time.time()\n",
    "        if remain > 0:\n",
    "            st.info(f\"Capturing in {remain:.1f}sâ€¦ Hold still.\")\n",
    "        else:\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"Try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "            st.session_state[\"countdown_target\"] = None\n",
    "            force_rerun()\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# =================== RIGHT PANEL ===================\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"Matched artwork\")\n",
    "\n",
    "    # Match mode control\n",
    "    match_mode = st.selectbox(\n",
    "        \"Match mode\",\n",
    "        [\"default\", \"portrait_only\", \"high_value_only\", \"portrait_high_value\"],\n",
    "        index=0,\n",
    "        help=\"Use 'portrait_only' to bias towards clear portraits; \"\n",
    "             \"'high_value_only' for masterpieces.\",\n",
    "    )\n",
    "\n",
    "    st.markdown('<div class=\"right-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"art-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ph = st.empty()\n",
    "\n",
    "    proc: Optional[CuratorialProcessor] = None\n",
    "    if ctx and ctx.video_processor:\n",
    "        proc = ctx.video_processor\n",
    "\n",
    "    if not proc:\n",
    "        ph.info(\"Initializing cameraâ€¦\")\n",
    "    else:\n",
    "        with proc.lock:\n",
    "            cap_img = proc.captured_img\n",
    "            cap_ts = getattr(proc, \"captured_ts\", 0.0)\n",
    "            cap_metrics = list(getattr(proc, \"captured_metrics\", []))\n",
    "\n",
    "        if cap_img is not None and cap_ts > st.session_state[\"last_ts\"]:\n",
    "            st.session_state[\"last_ts\"] = cap_ts\n",
    "            st.session_state[\"last_metrics\"] = cap_metrics\n",
    "\n",
    "            buf = io.BytesIO()\n",
    "            cap_img.save(buf, format=\"JPEG\")\n",
    "            buf.seek(0)\n",
    "\n",
    "            files = {\"image\": (\"frame.jpg\", buf.getvalue(), \"image/jpeg\")}\n",
    "            # Ask backend for Top-10, we'll do diversity selection here\n",
    "            data = {\n",
    "                \"museum\": \"mixed\",\n",
    "                \"topk\": TAIL_TOP_K,\n",
    "                \"mode\": match_mode,\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                resp = requests.post(API_URL, files=files, data=data, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                st.session_state[\"last_match\"] = resp.json()\n",
    "            except Exception as exc:\n",
    "                st.session_state[\"last_match\"] = {\"error\": str(exc)}\n",
    "\n",
    "            force_rerun()\n",
    "\n",
    "        payload = st.session_state.get(\"last_match\")\n",
    "\n",
    "        if not payload:\n",
    "            ph.info(\"Hold still or press capture to matchâ€¦\")\n",
    "        elif \"error\" in payload:\n",
    "            ph.error(f\"Backend error: {payload['error']}\")\n",
    "        else:\n",
    "            results = payload.get(\"results\") or []\n",
    "            if not results:\n",
    "                ph.warning(\"No matches returned.\")\n",
    "            else:\n",
    "                recent = st.session_state.get(\"recent_files\", [])\n",
    "\n",
    "                # ---------- diversity + mild randomness ----------\n",
    "                # Decide whether to start looking from the tail (ranks 4â€“10)\n",
    "                use_tail_first = (\n",
    "                    random.random() < TAIL_RANDOM_PROB\n",
    "                    and len(results) > PRIMARY_TOP_K\n",
    "                )\n",
    "                primary_slice = results[:PRIMARY_TOP_K]\n",
    "                tail_slice = results[PRIMARY_TOP_K:TAIL_TOP_K]\n",
    "\n",
    "                ordered_candidates = (\n",
    "                    (tail_slice + primary_slice)\n",
    "                    if use_tail_first\n",
    "                    else (primary_slice + tail_slice)\n",
    "                )\n",
    "\n",
    "                chosen = None\n",
    "                chosen_fname = None\n",
    "\n",
    "                for r in ordered_candidates:\n",
    "                    fname = (\n",
    "                        r.get(\"filename\")\n",
    "                        or r.get(\"file\")\n",
    "                        or r.get(\"image_path\")\n",
    "                        or r.get(\"path\")\n",
    "                    )\n",
    "                    if not fname:\n",
    "                        continue\n",
    "                    if fname not in recent:\n",
    "                        chosen = r\n",
    "                        chosen_fname = fname\n",
    "                        break\n",
    "\n",
    "                # Fallback: if all candidates are \"recent\", use strict Top-1\n",
    "                if chosen is None:\n",
    "                    chosen = results[0]\n",
    "                    chosen_fname = (\n",
    "                        chosen.get(\"filename\")\n",
    "                        or chosen.get(\"file\")\n",
    "                        or chosen.get(\"image_path\")\n",
    "                        or chosen.get(\"path\")\n",
    "                    )\n",
    "\n",
    "                # Update recent memory\n",
    "                if chosen_fname:\n",
    "                    if chosen_fname in recent:\n",
    "                        recent.remove(chosen_fname)\n",
    "                    recent.insert(0, chosen_fname)\n",
    "                    del recent[MAX_RECENT:]\n",
    "                    st.session_state[\"recent_files\"] = recent\n",
    "\n",
    "                filename = chosen_fname\n",
    "\n",
    "                img_path = ensure_image_path(filename or \"\")\n",
    "                if not img_path:\n",
    "                    ph.error(f\"Image not found: {filename}\")\n",
    "                else:\n",
    "                    painting = safe_open_image(img_path)\n",
    "                    if painting is None:\n",
    "                        ph.error(f\"Failed to open: {img_path}\")\n",
    "                    else:\n",
    "                        meta_row = lookup_meta(str(filename))\n",
    "                        meta = {\n",
    "                            \"artist\": meta_row.get(\"artist\")\n",
    "                            or chosen.get(\"artist\")\n",
    "                            or \"artist name\",\n",
    "                            \"year\": meta_row.get(\"year\") or chosen.get(\"year\") or \"\",\n",
    "                            \"price_text\": meta_row.get(\"price_text\")\n",
    "                            or meta_row.get(\"auction_price_usd\")\n",
    "                            or \"\",\n",
    "                        }\n",
    "\n",
    "                        painted = overlay_right_labels(painting, meta)\n",
    "                        metrics = st.session_state.get(\"last_metrics\") or []\n",
    "                        painted = draw_tiny_metrics_top_right(\n",
    "                            painted, metrics, size=16, margin=12\n",
    "                        )\n",
    "\n",
    "                        w = min(RIGHT_IMG_MAXW, painted.width)\n",
    "                        caption = f\"{meta_row.get('title','')} â€” {meta.get('artist','')}\"\n",
    "                        ph.image(painted, caption=caption, width=w)\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c90957",
   "metadata": {},
   "source": [
    "app_frontend.py(737)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc3b57",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_webrtc import (\n",
    "    WebRtcMode,\n",
    "    RTCConfiguration,\n",
    "    VideoProcessorBase,\n",
    "    webrtc_streamer,\n",
    ")\n",
    "\n",
    "# =================== PATHS & CONSTANTS ===================\n",
    "\n",
    "FRONTEND_DIR = Path(__file__).resolve().parent\n",
    "ROOT_DIR = FRONTEND_DIR.parent\n",
    "\n",
    "# Use GLOBAL MIXED INDEX (local + met + aic merged)\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"mixed\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "# All museum image roots (search order)\n",
    "MUSEUM_IMAGE_DIRS = [\n",
    "    DATA_DIR / \"images\",                    # mixed\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"images\",\n",
    "    ROOT_DIR / \"data\" / \"met\" / \"images\",\n",
    "    ROOT_DIR / \"data\" / \"aic\" / \"images\",\n",
    "]\n",
    "\n",
    "# Meta CSV candidates (fallback to local if needed)\n",
    "META_CSV_CANDIDATES = [\n",
    "    DATA_DIR / \"embeddings_meta.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works_enhanced_english.csv\",\n",
    "    ROOT_DIR / \"data\" / \"local\" / \"portrait_works.csv\",\n",
    "]\n",
    "\n",
    "DEFAULT_API_URL = \"http://127.0.0.1:8000/match\"\n",
    "APP_TITLE = \"Embodied Aesthetic Reconstruction\"\n",
    "\n",
    "YOLO_MODEL_PATH = FRONTEND_DIR / \"yolov8n-pose.pt\"\n",
    "\n",
    "RTC_CONFIGURATION = RTCConfiguration(\n",
    "    {\"iceServers\": [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]}\n",
    ")\n",
    "\n",
    "# Stillness / motion parameters\n",
    "STILLNESS_SEC = 3.5\n",
    "MAX_BUF_SEC = 5.0\n",
    "FPS_ASSUMED = 12\n",
    "MOTION_EPS_CXCY = 4.0\n",
    "MOTION_EPS_AREA = 0.03\n",
    "MIN_FACE_AREA = 0.06\n",
    "\n",
    "# Colors\n",
    "YELLOW = (255, 235, 59)\n",
    "BLACK = (0, 0, 0)\n",
    "HOT_PINK = (255, 30, 180)\n",
    "\n",
    "RIGHT_IMG_MAXW = 900\n",
    "\n",
    "# Diversity / randomness control\n",
    "MAX_RECENT = 6          # how many artworks to remember\n",
    "PRIMARY_TOP_K = 3       # \"very best\" range\n",
    "TAIL_TOP_K = 10         # search in top-10 in total\n",
    "TAIL_RANDOM_PROB = 0.3  # 30% chance to look into 4â€“10 first\n",
    "\n",
    "\n",
    "# =================== UTILITIES ===================\n",
    "\n",
    "_META_CACHE: Optional[Dict[str, Dict]] = None\n",
    "\n",
    "\n",
    "def load_meta_mapping() -> Dict[str, Dict]:\n",
    "    \"\"\"Load CSV â†’ filename â†’ metadata mapping.\"\"\"\n",
    "    global _META_CACHE\n",
    "    if _META_CACHE is not None:\n",
    "        return _META_CACHE\n",
    "\n",
    "    import csv\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    for p in META_CSV_CANDIDATES:\n",
    "        if p.exists():\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                rows = list(csv.DictReader(f))\n",
    "            break\n",
    "\n",
    "    mapping: Dict[str, Dict] = {}\n",
    "    for r in rows:\n",
    "        fname = (\n",
    "            r.get(\"filename\")\n",
    "            or r.get(\"image_path\")\n",
    "            or r.get(\"path\")\n",
    "            or r.get(\"file\")\n",
    "        )\n",
    "        if fname:\n",
    "            mapping[str(fname)] = r\n",
    "\n",
    "    _META_CACHE = mapping\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def lookup_meta(filename: str) -> Dict:\n",
    "    mapping = load_meta_mapping()\n",
    "    return mapping.get(filename, {})\n",
    "\n",
    "\n",
    "def safe_open_image(p: Path) -> Optional[Image.Image]:\n",
    "    try:\n",
    "        return Image.open(p).convert(\"RGB\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_image_path(filename: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Resolve an artwork filename to a real image path.\n",
    "\n",
    "    Strategy:\n",
    "      1) If it's an absolute existing path â†’ return.\n",
    "      2) If it has subdirectories, try relative to project root / data.\n",
    "      3) Otherwise, search by basename in all known museum image dirs.\n",
    "    \"\"\"\n",
    "    if not filename:\n",
    "        return None\n",
    "\n",
    "    p = Path(filename)\n",
    "\n",
    "    # 1) absolute path already\n",
    "    if p.is_absolute() and p.exists():\n",
    "        return p\n",
    "\n",
    "    candidates: List[Path] = []\n",
    "\n",
    "    # 2) has subdirectory component (e.g. \"aic/images/aic_4939.jpg\")\n",
    "    if p.parent != Path(\".\"):\n",
    "        candidates.append(ROOT_DIR / p)\n",
    "        candidates.append(ROOT_DIR / \"data\" / p)\n",
    "\n",
    "    # 3) search by basename across all museum image roots\n",
    "    basename = p.name\n",
    "    for root in MUSEUM_IMAGE_DIRS:\n",
    "        candidates.append(root / basename)\n",
    "\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return c\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _load_font(size: int = 40):\n",
    "    \"\"\"Try Courier â†’ Arial â†’ default.\"\"\"\n",
    "    candidates = [\n",
    "        \"/Library/Fonts/Courier New.ttf\",\n",
    "        \"/System/Library/Fonts/Courier.dfont\",\n",
    "        \"/System/Library/Fonts/Supplemental/Courier New.ttf\",\n",
    "        \"/Library/Fonts/Arial.ttf\",\n",
    "        \"/System/Library/Fonts/Supplemental/Arial.ttf\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                return ImageFont.truetype(p, size=size)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def draw_tiny_metrics_top_right(\n",
    "    im: Image.Image, lines: List[str], size: int = 16, margin: int = 10\n",
    ") -> Image.Image:\n",
    "    \"\"\"Small pink pose metrics in top-right corner of the image.\"\"\"\n",
    "    if not lines:\n",
    "        return im\n",
    "    img = im.copy()\n",
    "    d = ImageDraw.Draw(img)\n",
    "    font = _load_font(size)\n",
    "\n",
    "    widths = []\n",
    "    for s in lines:\n",
    "        l, t, r, b = d.textbbox((0, 0), s, font=font)\n",
    "        widths.append(r - l)\n",
    "    wmax = max(widths) if widths else 0\n",
    "\n",
    "    x = img.width - margin - wmax\n",
    "    y = margin\n",
    "    for s in lines:\n",
    "        d.text((x, y), s, fill=HOT_PINK, font=font)\n",
    "        _, _, _, b = d.textbbox((0, 0), s, font=font)\n",
    "        y += int(b * 0.95)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _angle_deg(p1, p2):\n",
    "    if p1 is None or p2 is None:\n",
    "        return None\n",
    "    vx, vy = p2[0] - p1[0], p2[1] - p1[1]\n",
    "    return float(np.degrees(np.arctan2(vy, vx)))\n",
    "\n",
    "\n",
    "def _elbow_angle(shoulder, elbow, wrist):\n",
    "    if None in (shoulder, elbow, wrist):\n",
    "        return None\n",
    "    v1 = np.array([shoulder[0] - elbow[0], shoulder[1] - elbow[1]], float)\n",
    "    v2 = np.array([wrist[0] - elbow[0], wrist[1] - elbow[1]], float)\n",
    "    n1, n2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if n1 < 1e-5 or n2 < 1e-5:\n",
    "        return None\n",
    "    cosv = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return float(np.degrees(np.arccos(cosv)))\n",
    "\n",
    "\n",
    "def format_metrics(kps: Dict[int, Tuple[float, float] | None]) -> List[str]:\n",
    "    le, re = kps.get(1), kps.get(2)\n",
    "    lsh, rsh = kps.get(5), kps.get(6)\n",
    "    lel, rel = kps.get(7), kps.get(8)\n",
    "    lwr, rwr = kps.get(9), kps.get(10)\n",
    "\n",
    "    fdeg = lambda v: \"â€”\" if v is None else f\"{v:+.1f}Â°\"\n",
    "    fpt = lambda p: \"(0, 0)\" if p is None else f\"({int(p[0])}, {int(p[1])})\"\n",
    "\n",
    "    return [\n",
    "        f\"Head tilt: {fdeg(_angle_deg(re, le))}\",\n",
    "        f\"Shoulder:  {fdeg(_angle_deg(rsh, lsh))}\",\n",
    "        f\"L elbow:   {fdeg(_elbow_angle(lsh, lel, lwr))}\",\n",
    "        f\"R elbow:   {fdeg(_elbow_angle(rsh, rel, rwr))}\",\n",
    "        f\"L wrist:   {fpt(lwr)}\",\n",
    "        f\"R wrist:   {fpt(rwr)}\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def overlay_right_labels(painting: Image.Image, meta: Dict) -> Image.Image:\n",
    "    \"\"\"Yellow label stack: price, year, artist.\"\"\"\n",
    "    im = painting.convert(\"RGB\").copy()\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    font_big = _load_font(44)\n",
    "    font_small = _load_font(36)\n",
    "\n",
    "    price = (\n",
    "        meta.get(\"price_text\")\n",
    "        or meta.get(\"auction_price_usd\")\n",
    "        or meta.get(\"price\")\n",
    "        or \"â€”\"\n",
    "    )\n",
    "    year = str(meta.get(\"year\") or \"â€”\")\n",
    "    artist = meta.get(\"artist\") or \"artist name\"\n",
    "\n",
    "    lines = [price, year, artist]\n",
    "    fonts = [font_big, font_small, font_big]\n",
    "\n",
    "    margin_x = 24\n",
    "    y = int(im.height * 0.50)\n",
    "\n",
    "    for text, font in zip(lines, fonts):\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font)\n",
    "        w, h = r - l, b - t\n",
    "\n",
    "        pad_x, pad_y = 16, 10\n",
    "        box_w, box_h = w + 2 * pad_x, h + 2 * pad_y\n",
    "\n",
    "        x = margin_x\n",
    "        draw.rectangle([x, y, x + box_w, y + box_h], fill=YELLOW)\n",
    "        draw.text((x + pad_x, y + pad_y), text, fill=BLACK, font=font)\n",
    "\n",
    "        y += box_h + 10\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def force_rerun():\n",
    "    \"\"\"Compatible with new/old Streamlit.\"\"\"\n",
    "    try:\n",
    "        st.rerun()\n",
    "    except Exception:\n",
    "        try:\n",
    "            st.experimental_rerun()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =================== YOLO VIDEO PROCESSOR ===================\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "    HAS_YOLO = True\n",
    "except Exception:\n",
    "    HAS_YOLO = False\n",
    "\n",
    "\n",
    "class CuratorialProcessor(VideoProcessorBase):\n",
    "    \"\"\"\n",
    "    YOLOv8-Pose:\n",
    "    - blue bbox\n",
    "    - green skeleton\n",
    "    - pink tiny pose metrics\n",
    "    - stillness-based auto capture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        if HAS_YOLO:\n",
    "            try:\n",
    "                local = YOLO_MODEL_PATH\n",
    "                self.model = YOLO(str(local if local.exists() else \"yolov8n-pose.pt\"))\n",
    "            except Exception:\n",
    "                self.model = None\n",
    "\n",
    "        maxlen = max(6, int(MAX_BUF_SEC * FPS_ASSUMED))\n",
    "        self.cx_buf = deque(maxlen=maxlen)\n",
    "        self.cy_buf = deque(maxlen=maxlen)\n",
    "        self.area_buf = deque(maxlen=maxlen)\n",
    "        self.last_stable_ts: Optional[float] = None\n",
    "\n",
    "        self.captured_img: Optional[Image.Image] = None\n",
    "        self.captured_ts: float = 0.0\n",
    "        self.captured_metrics: List[str] = []\n",
    "\n",
    "        self.latest_rgb: Optional[np.ndarray] = None\n",
    "        self.last_metrics_lines: List[str] = []\n",
    "\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def _detect_bbox(self, rgb: np.ndarray) -> Optional[Tuple[int, int, int, int]]:\n",
    "        if not self.model:\n",
    "            return None\n",
    "        res = self.model.predict(rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "        if len(res) == 0 or res[0].boxes is None:\n",
    "            return None\n",
    "        b = res[0].boxes.xyxy\n",
    "        if b is None or len(b) == 0:\n",
    "            return None\n",
    "        b = b.cpu().numpy()\n",
    "        areas = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "        i = int(np.argmax(areas))\n",
    "        x1, y1, x2, y2 = b[i].astype(int).tolist()\n",
    "        return x1, y1, x2, y2\n",
    "\n",
    "    def _extract_keypoints(self, res) -> Dict[int, Tuple[float, float] | None]:\n",
    "        kps: Dict[int, Tuple[float, float] | None] = {}\n",
    "        try:\n",
    "            if res and res[0].keypoints is not None and len(res[0].keypoints) > 0:\n",
    "                xy = res[0].keypoints.xy[0].cpu().numpy()\n",
    "                for i in range(xy.shape[0]):\n",
    "                    kps[i] = (float(xy[i, 0]), float(xy[i, 1]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return kps\n",
    "\n",
    "    def _update_stillness(self, bbox, w, h) -> bool:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cx = 0.5 * (x1 + x2)\n",
    "        cy = 0.5 * (y1 + y2)\n",
    "        area = max(1.0, (x2 - x1) * (y2 - y1))\n",
    "        rel_area = area / float(w * h)\n",
    "\n",
    "        if rel_area < MIN_FACE_AREA:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        self.cx_buf.append(cx)\n",
    "        self.cy_buf.append(cy)\n",
    "        self.area_buf.append(rel_area)\n",
    "\n",
    "        need_len = int(STILLNESS_SEC * FPS_ASSUMED * 0.6)\n",
    "        if len(self.cx_buf) < max(3, need_len):\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "        stdx = float(np.std(self.cx_buf))\n",
    "        stdy = float(np.std(self.cy_buf))\n",
    "        stda = float(np.std(self.area_buf))\n",
    "        stable_now = (\n",
    "            stdx < MOTION_EPS_CXCY\n",
    "            and stdy < MOTION_EPS_CXCY\n",
    "            and stda < MOTION_EPS_AREA\n",
    "        )\n",
    "\n",
    "        now = time.time()\n",
    "        if stable_now:\n",
    "            if self.last_stable_ts is None:\n",
    "                self.last_stable_ts = now\n",
    "            return (now - self.last_stable_ts) >= STILLNESS_SEC\n",
    "        else:\n",
    "            self.last_stable_ts = None\n",
    "            return False\n",
    "\n",
    "    def _stamp_capture(self):\n",
    "        self.captured_img = Image.fromarray(self.latest_rgb)\n",
    "        self.captured_ts = time.time()\n",
    "        self.captured_metrics = list(self.last_metrics_lines)\n",
    "        self.last_stable_ts = None\n",
    "\n",
    "    def capture_now(self) -> bool:\n",
    "        if self.latest_rgb is None:\n",
    "            return False\n",
    "        with self.lock:\n",
    "            self._stamp_capture()\n",
    "        return True\n",
    "\n",
    "    def recv(self, frame):\n",
    "        import av\n",
    "\n",
    "        img_bgr = frame.to_ndarray(format=\"bgr24\")\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        h, w, _ = img_rgb.shape\n",
    "        self.latest_rgb = img_rgb\n",
    "\n",
    "        if self.model:\n",
    "            res = self.model.predict(img_rgb, imgsz=640, device=\"cpu\", verbose=False)\n",
    "            plotted = res[0].plot()[:, :, ::-1]\n",
    "            kps = self._extract_keypoints(res)\n",
    "            lines = format_metrics(kps)\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = lines\n",
    "            pil = Image.fromarray(plotted)\n",
    "            pil = draw_tiny_metrics_top_right(pil, lines, size=16, margin=10)\n",
    "            out_rgb = np.array(pil)\n",
    "        else:\n",
    "            with self.lock:\n",
    "                self.last_metrics_lines = [\"(pose model unavailable)\"]\n",
    "            out_rgb = img_rgb\n",
    "\n",
    "        bbox = self._detect_bbox(img_rgb) if self.model else None\n",
    "        if bbox and self._update_stillness(bbox, w, h):\n",
    "            with self.lock:\n",
    "                if time.time() - self.captured_ts > 0.35:\n",
    "                    self._stamp_capture()\n",
    "\n",
    "        out_bgr = out_rgb[:, :, ::-1]\n",
    "        return av.VideoFrame.from_ndarray(out_bgr, format=\"bgr24\")\n",
    "\n",
    "\n",
    "# =================== PAGE LAYOUT ===================\n",
    "\n",
    "st.set_page_config(page_title=APP_TITLE, layout=\"wide\")\n",
    "\n",
    "st.markdown(\n",
    "    f\"\"\"\n",
    "<style>\n",
    "section[data-testid=\"stSidebar\"] {{ display: none !important; }}\n",
    "header, footer, [data-testid=\"stToolbar\"] {{ visibility: hidden !important; }}\n",
    "\n",
    "body {{\n",
    "  background-color: #f5f5f7;\n",
    "}}\n",
    ".block-container {{\n",
    "  padding-top: 0.6rem;\n",
    "  padding-bottom: 0.6rem;\n",
    "  max-width: 1700px;\n",
    "}}\n",
    "\n",
    "h1 {{\n",
    "  font-family: -apple-system, BlinkMacSystemFont, \"SF Pro Display\", system-ui, sans-serif;\n",
    "  letter-spacing: 0.04em;\n",
    "  font-weight: 700;\n",
    "}}\n",
    "\n",
    ".left-col .cam-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;\n",
    "  width: 100%;\n",
    "  overflow: hidden;\n",
    "  border-radius: 18px;\n",
    "  background: #111;\n",
    "  box-shadow: 0 18px 45px rgba(0,0,0,0.3);\n",
    "}}\n",
    ".left-col .cam-wrap video {{\n",
    "  height: 100% !important;\n",
    "  width: auto !important;\n",
    "  object-fit: cover !important;\n",
    "  border-radius: 18px !important;\n",
    "}}\n",
    "\n",
    ".right-col .art-wrap {{\n",
    "  position: relative;\n",
    "  height: 80vh;\n",
    "  max-width: {RIGHT_IMG_MAXW}px;\n",
    "  overflow: hidden;\n",
    "  margin: 0 auto;\n",
    "  border-radius: 18px;\n",
    "  background: #050505;\n",
    "  box-shadow: 0 18px 45px rgba(0,0,0,0.35);\n",
    "}}\n",
    ".right-col .art-wrap img {{\n",
    "  display: block;\n",
    "  width: 100% !important;\n",
    "  height: 100% !important;\n",
    "  object-fit: cover !important;\n",
    "}}\n",
    "\n",
    "button[kind=\"secondary\"] {{\n",
    "  border-radius: 999px !important;\n",
    "}}\n",
    "\n",
    "</style>\n",
    "\"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "st.title(APP_TITLE)\n",
    "st.caption(\n",
    "    \"Hold still for ~3â€“5 seconds to auto-capture, or press the button to capture manually. \"\n",
    "    \"Left: live video with pose. Right: matched artwork with tiny pink pose metrics.\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    st.autorefresh(interval=700, key=\"ear_auto\", limit=None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "left, right = st.columns([1, 1], gap=\"large\")\n",
    "\n",
    "if \"countdown_target\" not in st.session_state:\n",
    "    st.session_state[\"countdown_target\"] = None\n",
    "if \"last_match\" not in st.session_state:\n",
    "    st.session_state[\"last_match\"] = None\n",
    "if \"last_metrics\" not in st.session_state:\n",
    "    st.session_state[\"last_metrics\"] = []\n",
    "if \"last_ts\" not in st.session_state:\n",
    "    st.session_state[\"last_ts\"] = 0.0\n",
    "if \"recent_files\" not in st.session_state:\n",
    "    st.session_state[\"recent_files\"] = []  # type: List[str]\n",
    "\n",
    "API_URL = DEFAULT_API_URL\n",
    "\n",
    "\n",
    "# =================== LEFT PANEL ===================\n",
    "\n",
    "with left:\n",
    "    st.subheader(\"Live\")\n",
    "    st.markdown('<div class=\"left-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"cam-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ctx = webrtc_streamer(\n",
    "        key=\"ear-curatorial-final\",\n",
    "        mode=WebRtcMode.SENDRECV,\n",
    "        rtc_configuration=RTC_CONFIGURATION,\n",
    "        media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "        video_processor_factory=CuratorialProcessor,\n",
    "        async_processing=True,\n",
    "    )\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"<div style='height:10px'></div>\", unsafe_allow_html=True)\n",
    "\n",
    "    c1, c2 = st.columns(2)\n",
    "\n",
    "    with c1:\n",
    "        if st.button(\"ğŸ“¸ Capture (wait 3s)\", use_container_width=True):\n",
    "            st.session_state[\"countdown_target\"] = time.time() + 3.0\n",
    "\n",
    "    with c2:\n",
    "        if st.button(\"âš¡ Instant Capture\", use_container_width=True):\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"Try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "\n",
    "    if st.session_state[\"countdown_target\"]:\n",
    "        remain = st.session_state[\"countdown_target\"] - time.time()\n",
    "        if remain > 0:\n",
    "            st.info(f\"Capturing in {remain:.1f}sâ€¦ Hold still.\")\n",
    "        else:\n",
    "            if ctx and ctx.video_processor:\n",
    "                ok = ctx.video_processor.capture_now()\n",
    "                st.toast(\"Captured.\" if ok else \"Try again.\", icon=\"âœ…\" if ok else \"âš ï¸\")\n",
    "            st.session_state[\"countdown_target\"] = None\n",
    "            force_rerun()\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# =================== RIGHT PANEL ===================\n",
    "\n",
    "with right:\n",
    "    st.subheader(\"Matched artwork\")\n",
    "\n",
    "    # Match mode control\n",
    "    match_mode = st.selectbox(\n",
    "        \"Match mode\",\n",
    "        [\"default\", \"portrait_only\", \"high_value_only\", \"portrait_high_value\"],\n",
    "        index=0,\n",
    "        help=\"Use 'portrait_only' to bias towards clear portraits; \"\n",
    "             \"'high_value_only' for masterpieces.\",\n",
    "    )\n",
    "\n",
    "    st.markdown('<div class=\"right-col\">', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"art-wrap\">', unsafe_allow_html=True)\n",
    "\n",
    "    ph = st.empty()\n",
    "\n",
    "    proc: Optional[CuratorialProcessor] = None\n",
    "    if ctx and ctx.video_processor:\n",
    "        proc = ctx.video_processor\n",
    "\n",
    "    if not proc:\n",
    "        ph.info(\"Initializing cameraâ€¦\")\n",
    "    else:\n",
    "        with proc.lock:\n",
    "            cap_img = proc.captured_img\n",
    "            cap_ts = getattr(proc, \"captured_ts\", 0.0)\n",
    "            cap_metrics = list(getattr(proc, \"captured_metrics\", []))\n",
    "\n",
    "        if cap_img is not None and cap_ts > st.session_state[\"last_ts\"]:\n",
    "            st.session_state[\"last_ts\"] = cap_ts\n",
    "            st.session_state[\"last_metrics\"] = cap_metrics\n",
    "\n",
    "            buf = io.BytesIO()\n",
    "            cap_img.save(buf, format=\"JPEG\")\n",
    "            buf.seek(0)\n",
    "\n",
    "            files = {\"image\": (\"frame.jpg\", buf.getvalue(), \"image/jpeg\")}\n",
    "            # Ask backend for Top-10, we'll do diversity selection here\n",
    "            data = {\n",
    "                \"museum\": \"mixed\",\n",
    "                \"topk\": TAIL_TOP_K,\n",
    "                \"mode\": match_mode,\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                resp = requests.post(API_URL, files=files, data=data, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                st.session_state[\"last_match\"] = resp.json()\n",
    "            except Exception as exc:\n",
    "                st.session_state[\"last_match\"] = {\"error\": str(exc)}\n",
    "\n",
    "            force_rerun()\n",
    "\n",
    "        payload = st.session_state.get(\"last_match\")\n",
    "\n",
    "        if not payload:\n",
    "            ph.info(\"Hold still or press capture to matchâ€¦\")\n",
    "        elif \"error\" in payload:\n",
    "            ph.error(f\"Backend error: {payload['error']}\")\n",
    "        else:\n",
    "            results = payload.get(\"results\") or []\n",
    "            if not results:\n",
    "                ph.warning(\"No matches returned.\")\n",
    "            else:\n",
    "                recent = st.session_state.get(\"recent_files\", [])\n",
    "\n",
    "                # ---------- diversity + mild randomness ----------\n",
    "                # Decide whether to start looking from the tail (ranks 4â€“10)\n",
    "                use_tail_first = (\n",
    "                    random.random() < TAIL_RANDOM_PROB\n",
    "                    and len(results) > PRIMARY_TOP_K\n",
    "                )\n",
    "                primary_slice = results[:PRIMARY_TOP_K]\n",
    "                tail_slice = results[PRIMARY_TOP_K:TAIL_TOP_K]\n",
    "\n",
    "                ordered_candidates = (\n",
    "                    (tail_slice + primary_slice)\n",
    "                    if use_tail_first\n",
    "                    else (primary_slice + tail_slice)\n",
    "                )\n",
    "\n",
    "                chosen = None\n",
    "                chosen_fname = None\n",
    "\n",
    "                for r in ordered_candidates:\n",
    "                    fname = (\n",
    "                        r.get(\"filename\")\n",
    "                        or r.get(\"file\")\n",
    "                        or r.get(\"image_path\")\n",
    "                        or r.get(\"path\")\n",
    "                    )\n",
    "                    if not fname:\n",
    "                        continue\n",
    "                    if fname not in recent:\n",
    "                        chosen = r\n",
    "                        chosen_fname = fname\n",
    "                        break\n",
    "\n",
    "                # Fallback: if all candidates are \"recent\", use strict Top-1\n",
    "                if chosen is None:\n",
    "                    chosen = results[0]\n",
    "                    chosen_fname = (\n",
    "                        chosen.get(\"filename\")\n",
    "                        or chosen.get(\"file\")\n",
    "                        or chosen.get(\"image_path\")\n",
    "                        or chosen.get(\"path\")\n",
    "                    )\n",
    "\n",
    "                # Update recent memory\n",
    "                if chosen_fname:\n",
    "                    if chosen_fname in recent:\n",
    "                        recent.remove(chosen_fname)\n",
    "                    recent.insert(0, chosen_fname)\n",
    "                    del recent[MAX_RECENT:]\n",
    "                    st.session_state[\"recent_files\"] = recent\n",
    "\n",
    "                filename = chosen_fname\n",
    "\n",
    "                img_path = ensure_image_path(filename or \"\")\n",
    "                if not img_path:\n",
    "                    ph.error(f\"Image not found: {filename}\")\n",
    "                else:\n",
    "                    painting = safe_open_image(img_path)\n",
    "                    if painting is None:\n",
    "                        ph.error(f\"Failed to open: {img_path}\")\n",
    "                    else:\n",
    "                        meta_row = lookup_meta(str(filename))\n",
    "                        meta = {\n",
    "                            \"artist\": meta_row.get(\"artist\")\n",
    "                            or chosen.get(\"artist\")\n",
    "                            or \"artist name\",\n",
    "                            \"year\": meta_row.get(\"year\") or chosen.get(\"year\") or \"\",\n",
    "                            \"price_text\": meta_row.get(\"price_text\")\n",
    "                            or meta_row.get(\"auction_price_usd\")\n",
    "                            or \"\",\n",
    "                        }\n",
    "\n",
    "                        painted = overlay_right_labels(painting, meta)\n",
    "                        metrics = st.session_state.get(\"last_metrics\") or []\n",
    "                        painted = draw_tiny_metrics_top_right(\n",
    "                            painted, metrics, size=16, margin=12\n",
    "                        )\n",
    "\n",
    "                        w = min(RIGHT_IMG_MAXW, painted.width)\n",
    "                        caption = f\"{meta_row.get('title','')} â€” {meta.get('artist','')}\"\n",
    "                        ph.image(painted, caption=caption, width=w)\n",
    "\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21783ed2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fetch a small curated portrait dataset from The Met Museum Open Access API.\n",
    "\n",
    "This script:\n",
    "1. Searches for portraits / female portrait / male portrait\n",
    "2. Downloads the first ~80 high-quality, CC0 images\n",
    "3. Saves them into data/met/images/\n",
    "4. Writes a metadata CSV: data/met/portrait_works.csv\n",
    "\n",
    "You only need to run this once.\n",
    "\n",
    "Usage:\n",
    "    python scripts/fetch_met_portraits.py\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import time\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "OUT_DIR = ROOT / \"data\" / \"met\"\n",
    "IMG_DIR = OUT_DIR / \"images\"\n",
    "OUT_CSV = OUT_DIR / \"portrait_works.csv\"\n",
    "\n",
    "SEARCH_TERMS = [\n",
    "    \"portrait\",\n",
    "    \"woman portrait\",\n",
    "    \"man portrait\",\n",
    "    \"lady portrait\",\n",
    "    \"self-portrait\",\n",
    "]\n",
    "\n",
    "MAX_RESULTS = 80  # total images you want\n",
    "\n",
    "# Ensure folders exist\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def met_search_ids(query: str):\n",
    "    \"\"\"Search Met API and return list of objectIDs.\"\"\"\n",
    "    url = \"https://collectionapi.metmuseum.org/public/collection/v1/search\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"hasImages\": \"true\",\n",
    "        \"isOnView\": \"true\"\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=10)\n",
    "    if r.status_code != 200:\n",
    "        return []\n",
    "    data = r.json()\n",
    "    ids = data.get(\"objectIDs\") or []\n",
    "    return ids[:100]  # get up to 100 per keyword\n",
    "\n",
    "\n",
    "def fetch_object(object_id: int):\n",
    "    \"\"\"Get object details + primaryImage.\"\"\"\n",
    "    url = f\"https://collectionapi.metmuseum.org/public/collection/v1/objects/{object_id}\"\n",
    "    r = requests.get(url, timeout=10)\n",
    "    if r.status_code != 200:\n",
    "        return None\n",
    "    data = r.json()\n",
    "\n",
    "    img = data.get(\"primaryImage\") or data.get(\"primaryImageSmall\")\n",
    "    if not img:\n",
    "        return None\n",
    "\n",
    "    # Only accept CC0\n",
    "    if data.get(\"isPublicDomain\") != True:\n",
    "        return None\n",
    "\n",
    "    title = data.get(\"title\") or \"\"\n",
    "    artist = data.get(\"artistDisplayName\") or \"Unknown\"\n",
    "    year = data.get(\"objectDate\") or \"\"\n",
    "    museum = \"Metropolitan Museum of Art, New York\"\n",
    "    permalink = data.get(\"objectURL\") or \"\"\n",
    "\n",
    "    return {\n",
    "        \"object_id\": object_id,\n",
    "        \"title\": title,\n",
    "        \"artist\": artist,\n",
    "        \"year\": year,\n",
    "        \"image_url\": img,\n",
    "        \"museum\": museum,\n",
    "        \"permalink\": permalink,\n",
    "    }\n",
    "\n",
    "\n",
    "def download_image(url: str, out_path: Path):\n",
    "    \"\"\"Save image to disk.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=== Fetching Met Museum portraits ===\")\n",
    "\n",
    "    all_ids = []\n",
    "    for q in SEARCH_TERMS:\n",
    "        ids = met_search_ids(q)\n",
    "        print(f\"{q} â†’ {len(ids)} ids\")\n",
    "        all_ids.extend(ids)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    all_ids = list(dict.fromkeys(all_ids))  # remove duplicates\n",
    "    print(f\"Total unique IDs: {len(all_ids)}\")\n",
    "\n",
    "    rows = []\n",
    "    count = 0\n",
    "\n",
    "    for oid in all_ids:\n",
    "        if count >= MAX_RESULTS:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            item = fetch_object(oid)\n",
    "        except Exception:\n",
    "            item = None\n",
    "\n",
    "        if not item:\n",
    "            continue\n",
    "\n",
    "        filename = f\"{item['object_id']}.jpg\"\n",
    "        out_img = IMG_DIR / filename\n",
    "\n",
    "        ok = download_image(item[\"image_url\"], out_img)\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        row = {\n",
    "            \"filename\": filename,\n",
    "            \"title\": item[\"title\"],\n",
    "            \"artist\": item[\"artist\"],\n",
    "            \"year\": item[\"year\"],\n",
    "            \"museum\": item[\"museum\"],\n",
    "            \"license\": \"CC0\",\n",
    "            \"price_text\": \"\",\n",
    "            \"auction_price_usd\": \"\",\n",
    "            \"permalink\": item[\"permalink\"],\n",
    "        }\n",
    "        rows.append(row)\n",
    "        count += 1\n",
    "        print(f\"[{count}] Saved {filename}\")\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    # Write metadata CSV\n",
    "    with open(OUT_CSV, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(\n",
    "            f,\n",
    "            fieldnames=[\n",
    "                \"filename\",\n",
    "                \"title\",\n",
    "                \"artist\",\n",
    "                \"year\",\n",
    "                \"museum\",\n",
    "                \"license\",\n",
    "                \"price_text\",\n",
    "                \"auction_price_usd\",\n",
    "                \"permalink\",\n",
    "            ],\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"\\nDone! Saved {count} images.\")\n",
    "    print(f\"Images â†’ {IMG_DIR}\")\n",
    "    print(f\"CSV    â†’ {OUT_CSV}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
